[{"content":" # data analysis and wrangling import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # import random as rnd # visualization import matplotlib.pyplot as plt # %matplotlib inline # machine learning from sklearn.model_selection import train_test_split import tensorflow as tf from tensorflow import keras from keras.models import Sequential from keras.layers.core import Dense import keras.metrics as metrics import os for dirname, _, filenames in os.walk(\u0026#39;/kaggle/input\u0026#39;): for filename in filenames: print(os.path.join(dirname, filename)) /kaggle/input/californiahousingdataset/train.csv /kaggle/input/californiahousingdataset/test.csv 1. Browse the Keras library (tutorial and documentation cited in the slides) 2. Load the California housing dataset Some info about the California_housing_dataset # #samples-istances: 20640\nvariables: 8 numeric predictors, 1 target\nPredictors: MedInc (mi): median income in block HouseAge (ha): median house age in block AveRooms (ar): average number of rooms AveBedrms (ab): average number of bedrooms Population (p): block population AveOccup (ao): average house occupancy Latitude (lt): house block latitude Longitude (lg): house block longitude Response: Target (v): median house value for California districts Missing values: none\nData Acquisition ## Load the California Housing dataset df_train = pd.read_csv(\u0026#39;../input/californiahousingdataset/train.csv\u0026#39;,sep=\u0026#39;,\u0026#39;) df_test = pd.read_csv(\u0026#39;../input/californiahousingdataset/test.csv\u0026#39;,sep=\u0026#39;,\u0026#39;) # Some stats print(f\u0026#34;We have {df_train.shape[0] + df_test.shape[0]} observation, splitted into:\\n\\ * {df_train.shape[0]} training observations;\\n\\ * {df_test.shape[0]} test observations.\\n\\ There are {df_train.isna().sum().sum() + df_test.isna().sum().sum()} missing values in the dataset.\u0026#34;) We have 20640 observation, splitted into: * 16385 training observations; * 4255 test observations. There are 0 missing values in the dataset. Data pre-processing ## Drop an useless feature df_train = df_train.drop(columns=\u0026#39;Unnamed: 0\u0026#39;); df_test = df_test.drop(columns=\u0026#39;Unnamed: 0\u0026#39;); df_train mi ha ar ab p ao lt lg v 0 5.8735 35.0 5.811639 1.056662 1521.0 2.329250 34.11 -118.63 4.48100 1 1.4688 8.0 10.000000 1.916667 63.0 2.625000 33.32 -115.98 0.53800 2 2.1603 28.0 4.808173 0.995460 2008.0 2.279228 38.74 -120.78 1.11300 3 4.7404 43.0 5.855140 1.009346 967.0 2.259346 37.58 -122.37 5.00001 4 3.2617 10.0 3.929142 1.051896 2032.0 2.027944 37.45 -121.92 2.52200 ... ... ... ... ... ... ... ... ... ... 16380 5.0427 22.0 6.405405 1.009828 1216.0 2.987715 38.55 -121.35 1.26900 16381 4.7396 25.0 5.453390 0.949153 727.0 3.080508 38.73 -121.44 1.35500 16382 5.0839 25.0 6.039216 1.150980 1558.0 3.054902 34.73 -118.61 1.56700 16383 5.5292 16.0 6.875000 1.015086 1414.0 3.047414 34.11 -117.68 2.08600 16384 5.1514 19.0 6.204918 1.001639 2198.0 3.603279 37.11 -121.66 4.36700 16385 rows × 9 columns\nSplit the dataset into Training and Test sets ## Training set predictorsTrain = df_train.loc[:, df_train.columns != \u0026#39;v\u0026#39;] responseTrain = df_train[\u0026#39;v\u0026#39;] # Test set predictorsTest = df_test.loc[:, df_train.columns != \u0026#39;v\u0026#39;] responseTest = df_test[\u0026#39;v\u0026#39;] Standardization ## Standardize \u0026#34;predictorsTrain\u0026#34; predictorsTrainMeans = predictorsTrain.mean() predictorsTrainStds = predictorsTrain.std() predictorsTrain_std = (predictorsTrain - predictorsTrainMeans)/predictorsTrainStds # standardized variables of predictorTrain # Standardize \u0026#34;predictorsTest\u0026#34; (using the mean and std of predictorsTrain, it\u0026#39;s better!) predictorsTest_std = (predictorsTest - predictorsTrainMeans)/predictorsTrainStds # standardized variables of predictorTest Split the training set into Train and Validation sets #Splitting the dataset is essential for an unbiased evaluation of prediction performance. In most cases, it’s enough to split your dataset randomly into three subsets:\nThe training set is applied to train, or fit, your model. For example, you use the training set to find the optimal coefficients for linear regression.\nThe validation set is used for unbiased model evaluation during hyperparameter tuning. For example, when you want to find the optimal number of neurons in a neural network, you experiment with different values. For each considered setting of hyperparameters, you fit the model with the training set and assess its performance with the validation set.\nThe test set is needed for an unbiased evaluation of the final model. Don\u0026rsquo;t use it for fitting or validation.\nI choosed to split the train set in two parts: a small fraction (20%) became the validation set which the model is evaluated and the rest (80%) is used to train the model.\n# Set the random seed random_seed = 3 # a random_state parameter may be provided to control the random number generator used # Split the train and the validation set for the fitting X_train, X_val, y_train, y_val = train_test_split(predictorsTrain_std, responseTrain, test_size = 0.2, random_state = random_seed) X_train.shape, y_train.shape, X_val.shape, y_val.shape ((13108, 8), (13108,), (3277, 8), (3277,)) Rename the data # Rename our data ## Training set - already done it above when I created the validation set # X_train = X_train # X_val = X_val # y_train = y_train # y_val = y_val ## Test set X_test = predictorsTest_std y_test = responseTest # # Since Keras models are trained on Numpy arrays of input data and labels: # # Training set # # X_train = X_train.values # # X_val = X_val.values # # y_train = y_train.values # # y_val = y_val.values # # Test set # X_test = X_test.values # y_test = y_test.values Warning: Converting data into Numpy arrays makes the fitting process very slower! Generating the first Artificial Neural Network #3. Generate the artificial neural network model analyzed in this slides and compare the results obtained by structures defined below** Create the ANN #Lets create a simple model from Keras Sequential layer:\nDense is fully connected layer that means all neurons in previous layers will be connected to all neurons in fully connected layer. model = Sequential() # Input Layer model.add(Dense(10, input_dim=X_train.shape[1], activation=\u0026#39;relu\u0026#39;)) # Hidden Layers model.add(Dense(30, activation=\u0026#39;relu\u0026#39;)) model.add(Dense(40, activation=\u0026#39;relu\u0026#39;)) # Output Layer model.add(Dense(1)) 2022-06-24 16:09:21.913542: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. Compile network ## Compile the model model.compile(optimizer =\u0026#39;adam\u0026#39;, # Optimizer: an algorithm for first-order stochastic gradiend descent loss = \u0026#39;mean_squared_error\u0026#39;, # Loss function: the objective that the model will try to minimize metrics=[metrics.mae]) # A list of metrics: used to judge the performance of your model Fitting procedure #EPOCHS = 150 # 150 are too much (using np.arrays) print(f\u0026#34;Train on {X_train.shape[0]} samples, validate on {X_val.shape[0]} samples.\u0026#34;) # train model on full train set, with 80/20 CV split history = model.fit(X_train, y_train, validation_data=(X_val, y_val), # validation_data: data on which to evaluate the loss and any model metrics at the end of each epoch epochs=EPOCHS, # epochs: number of iterations of the training phase batch_size=32) # batch_size: number of samples per gradient update (default: 32) Train on 13108 samples, validate on 3277 samples. Epoch 1/150 2022-06-24 16:09:22.231798: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2) 410/410 [==============================] - 2s 3ms/step - loss: 1.2253 - mean_absolute_error: 0.7533 - val_loss: 0.5908 - val_mean_absolute_error: 0.5529 Epoch 2/150 410/410 [==============================] - 1s 2ms/step - loss: 0.5182 - mean_absolute_error: 0.5102 - val_loss: 0.4717 - val_mean_absolute_error: 0.4867 Epoch 3/150 410/410 [==============================] - 1s 2ms/step - loss: 0.4256 - mean_absolute_error: 0.4662 - val_loss: 0.4313 - val_mean_absolute_error: 0.4772 Epoch 4/150 410/410 [==============================] - 1s 2ms/step - loss: 0.3909 - mean_absolute_error: 0.4474 - val_loss: 0.4028 - val_mean_absolute_error: 0.4475 Epoch 5/150 410/410 [==============================] - 1s 2ms/step - loss: 0.3738 - mean_absolute_error: 0.4374 - val_loss: 0.4008 - val_mean_absolute_error: 0.4341 Epoch 6/150 410/410 [==============================] - 1s 2ms/step - loss: 0.3637 - mean_absolute_error: 0.4284 - val_loss: 0.3835 - val_mean_absolute_error: 0.4412 Epoch 7/150 410/410 [==============================] - 1s 2ms/step - loss: 0.3578 - mean_absolute_error: 0.4246 - val_loss: 0.3987 - val_mean_absolute_error: 0.4290 Epoch 8/150 410/410 [==============================] - 1s 2ms/step - loss: 0.3471 - mean_absolute_error: 0.4180 - val_loss: 0.3797 - val_mean_absolute_error: 0.4406 Epoch 9/150 410/410 [==============================] - 1s 2ms/step - loss: 0.3449 - mean_absolute_error: 0.4167 - val_loss: 0.3776 - val_mean_absolute_error: 0.4168 Epoch 10/150 410/410 [==============================] - 1s 2ms/step - loss: 0.3375 - mean_absolute_error: 0.4114 - val_loss: 0.3777 - val_mean_absolute_error: 0.4179 Epoch 11/150 410/410 [==============================] - 1s 2ms/step - loss: 0.3341 - mean_absolute_error: 0.4091 - val_loss: 0.3913 - val_mean_absolute_error: 0.4518 Epoch 12/150 410/410 [==============================] - 1s 2ms/step - loss: 0.3310 - mean_absolute_error: 0.4078 - val_loss: 0.3635 - val_mean_absolute_error: 0.4131 Epoch 13/150 410/410 [==============================] - 1s 2ms/step - loss: 0.3272 - mean_absolute_error: 0.4033 - val_loss: 0.3650 - val_mean_absolute_error: 0.4186 Epoch 14/150 410/410 [==============================] - 1s 2ms/step - loss: 0.3244 - mean_absolute_error: 0.4007 - val_loss: 0.3560 - val_mean_absolute_error: 0.4127 Epoch 15/150 410/410 [==============================] - 1s 3ms/step - loss: 0.3186 - mean_absolute_error: 0.3977 - val_loss: 0.3796 - val_mean_absolute_error: 0.4072 Epoch 16/150 410/410 [==============================] - 1s 2ms/step - loss: 0.3209 - mean_absolute_error: 0.3966 - val_loss: 0.3632 - val_mean_absolute_error: 0.4107 Epoch 17/150 410/410 [==============================] - 1s 2ms/step - loss: 0.3142 - mean_absolute_error: 0.3922 - val_loss: 0.3457 - val_mean_absolute_error: 0.4125 Epoch 18/150 410/410 [==============================] - 1s 2ms/step - loss: 0.3075 - mean_absolute_error: 0.3894 - val_loss: 0.3431 - val_mean_absolute_error: 0.4050 Epoch 19/150 410/410 [==============================] - 1s 2ms/step - loss: 0.3045 - mean_absolute_error: 0.3875 - val_loss: 0.3506 - val_mean_absolute_error: 0.3960 Epoch 20/150 410/410 [==============================] - 1s 2ms/step - loss: 0.3029 - mean_absolute_error: 0.3854 - val_loss: 0.3421 - val_mean_absolute_error: 0.3993 Epoch 21/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2997 - mean_absolute_error: 0.3831 - val_loss: 0.3396 - val_mean_absolute_error: 0.4012 Epoch 22/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2966 - mean_absolute_error: 0.3803 - val_loss: 0.3392 - val_mean_absolute_error: 0.3958 Epoch 23/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2966 - mean_absolute_error: 0.3804 - val_loss: 0.3374 - val_mean_absolute_error: 0.3996 Epoch 24/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2963 - mean_absolute_error: 0.3801 - val_loss: 0.3317 - val_mean_absolute_error: 0.3929 Epoch 25/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2920 - mean_absolute_error: 0.3767 - val_loss: 0.3299 - val_mean_absolute_error: 0.3868 Epoch 26/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2890 - mean_absolute_error: 0.3742 - val_loss: 0.3314 - val_mean_absolute_error: 0.3980 Epoch 27/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2916 - mean_absolute_error: 0.3773 - val_loss: 0.3312 - val_mean_absolute_error: 0.3837 Epoch 28/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2870 - mean_absolute_error: 0.3721 - val_loss: 0.3285 - val_mean_absolute_error: 0.3937 Epoch 29/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2873 - mean_absolute_error: 0.3722 - val_loss: 0.3345 - val_mean_absolute_error: 0.3871 Epoch 30/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2849 - mean_absolute_error: 0.3716 - val_loss: 0.3309 - val_mean_absolute_error: 0.3861 Epoch 31/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2825 - mean_absolute_error: 0.3690 - val_loss: 0.3465 - val_mean_absolute_error: 0.3935 Epoch 32/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2835 - mean_absolute_error: 0.3703 - val_loss: 0.3266 - val_mean_absolute_error: 0.3992 Epoch 33/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2816 - mean_absolute_error: 0.3679 - val_loss: 0.3259 - val_mean_absolute_error: 0.3915 Epoch 34/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2785 - mean_absolute_error: 0.3664 - val_loss: 0.3205 - val_mean_absolute_error: 0.3787 Epoch 35/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2790 - mean_absolute_error: 0.3660 - val_loss: 0.3246 - val_mean_absolute_error: 0.3951 Epoch 36/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2759 - mean_absolute_error: 0.3640 - val_loss: 0.3227 - val_mean_absolute_error: 0.3902 Epoch 37/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2759 - mean_absolute_error: 0.3632 - val_loss: 0.3226 - val_mean_absolute_error: 0.3959 Epoch 38/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2734 - mean_absolute_error: 0.3623 - val_loss: 0.3232 - val_mean_absolute_error: 0.3744 Epoch 39/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2699 - mean_absolute_error: 0.3584 - val_loss: 0.3181 - val_mean_absolute_error: 0.3905 Epoch 40/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2707 - mean_absolute_error: 0.3593 - val_loss: 0.3274 - val_mean_absolute_error: 0.3990 Epoch 41/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2704 - mean_absolute_error: 0.3592 - val_loss: 0.3200 - val_mean_absolute_error: 0.3748 Epoch 42/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2685 - mean_absolute_error: 0.3592 - val_loss: 0.3129 - val_mean_absolute_error: 0.3802 Epoch 43/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2676 - mean_absolute_error: 0.3571 - val_loss: 0.3128 - val_mean_absolute_error: 0.3720 Epoch 44/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2670 - mean_absolute_error: 0.3563 - val_loss: 0.3225 - val_mean_absolute_error: 0.3992 Epoch 45/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2645 - mean_absolute_error: 0.3547 - val_loss: 0.3103 - val_mean_absolute_error: 0.3783 Epoch 46/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2649 - mean_absolute_error: 0.3549 - val_loss: 0.3098 - val_mean_absolute_error: 0.3795 Epoch 47/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2635 - mean_absolute_error: 0.3545 - val_loss: 0.3124 - val_mean_absolute_error: 0.3750 Epoch 48/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2651 - mean_absolute_error: 0.3544 - val_loss: 0.3114 - val_mean_absolute_error: 0.3776 Epoch 49/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2622 - mean_absolute_error: 0.3524 - val_loss: 0.3091 - val_mean_absolute_error: 0.3770 Epoch 50/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2624 - mean_absolute_error: 0.3526 - val_loss: 0.3110 - val_mean_absolute_error: 0.3730 Epoch 51/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2588 - mean_absolute_error: 0.3497 - val_loss: 0.3087 - val_mean_absolute_error: 0.3704 Epoch 52/150 410/410 [==============================] - 1s 3ms/step - loss: 0.2628 - mean_absolute_error: 0.3516 - val_loss: 0.3232 - val_mean_absolute_error: 0.3949 Epoch 53/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2598 - mean_absolute_error: 0.3493 - val_loss: 0.3068 - val_mean_absolute_error: 0.3712 Epoch 54/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2577 - mean_absolute_error: 0.3476 - val_loss: 0.3082 - val_mean_absolute_error: 0.3790 Epoch 55/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2569 - mean_absolute_error: 0.3483 - val_loss: 0.3132 - val_mean_absolute_error: 0.3695 Epoch 56/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2563 - mean_absolute_error: 0.3479 - val_loss: 0.3023 - val_mean_absolute_error: 0.3664 Epoch 57/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2538 - mean_absolute_error: 0.3458 - val_loss: 0.3114 - val_mean_absolute_error: 0.3833 Epoch 58/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2532 - mean_absolute_error: 0.3450 - val_loss: 0.3077 - val_mean_absolute_error: 0.3813 Epoch 59/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2536 - mean_absolute_error: 0.3451 - val_loss: 0.3078 - val_mean_absolute_error: 0.3684 Epoch 60/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2531 - mean_absolute_error: 0.3446 - val_loss: 0.3080 - val_mean_absolute_error: 0.3695 Epoch 61/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2529 - mean_absolute_error: 0.3440 - val_loss: 0.3052 - val_mean_absolute_error: 0.3708 Epoch 62/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2518 - mean_absolute_error: 0.3435 - val_loss: 0.3037 - val_mean_absolute_error: 0.3703 Epoch 63/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2515 - mean_absolute_error: 0.3437 - val_loss: 0.3066 - val_mean_absolute_error: 0.3735 Epoch 64/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2524 - mean_absolute_error: 0.3442 - val_loss: 0.3064 - val_mean_absolute_error: 0.3790 Epoch 65/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2499 - mean_absolute_error: 0.3423 - val_loss: 0.3079 - val_mean_absolute_error: 0.3698 Epoch 66/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2503 - mean_absolute_error: 0.3431 - val_loss: 0.3095 - val_mean_absolute_error: 0.3662 Epoch 67/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2479 - mean_absolute_error: 0.3408 - val_loss: 0.3021 - val_mean_absolute_error: 0.3635 Epoch 68/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2493 - mean_absolute_error: 0.3419 - val_loss: 0.3027 - val_mean_absolute_error: 0.3623 Epoch 69/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2488 - mean_absolute_error: 0.3414 - val_loss: 0.3055 - val_mean_absolute_error: 0.3688 Epoch 70/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2480 - mean_absolute_error: 0.3406 - val_loss: 0.3127 - val_mean_absolute_error: 0.3661 Epoch 71/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2481 - mean_absolute_error: 0.3394 - val_loss: 0.3071 - val_mean_absolute_error: 0.3783 Epoch 72/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2502 - mean_absolute_error: 0.3414 - val_loss: 0.2998 - val_mean_absolute_error: 0.3706 Epoch 73/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2475 - mean_absolute_error: 0.3403 - val_loss: 0.2979 - val_mean_absolute_error: 0.3608 Epoch 74/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2443 - mean_absolute_error: 0.3377 - val_loss: 0.3053 - val_mean_absolute_error: 0.3644 Epoch 75/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2479 - mean_absolute_error: 0.3394 - val_loss: 0.3181 - val_mean_absolute_error: 0.3924 Epoch 76/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2457 - mean_absolute_error: 0.3392 - val_loss: 0.3049 - val_mean_absolute_error: 0.3735 Epoch 77/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2435 - mean_absolute_error: 0.3374 - val_loss: 0.3039 - val_mean_absolute_error: 0.3625 Epoch 78/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2449 - mean_absolute_error: 0.3389 - val_loss: 0.2987 - val_mean_absolute_error: 0.3651 Epoch 79/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2439 - mean_absolute_error: 0.3386 - val_loss: 0.3043 - val_mean_absolute_error: 0.3621 Epoch 80/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2427 - mean_absolute_error: 0.3368 - val_loss: 0.3064 - val_mean_absolute_error: 0.3756 Epoch 81/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2428 - mean_absolute_error: 0.3360 - val_loss: 0.2989 - val_mean_absolute_error: 0.3723 Epoch 82/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2414 - mean_absolute_error: 0.3351 - val_loss: 0.3052 - val_mean_absolute_error: 0.3797 Epoch 83/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2424 - mean_absolute_error: 0.3364 - val_loss: 0.2976 - val_mean_absolute_error: 0.3585 Epoch 84/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2410 - mean_absolute_error: 0.3353 - val_loss: 0.2986 - val_mean_absolute_error: 0.3675 Epoch 85/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2416 - mean_absolute_error: 0.3352 - val_loss: 0.3049 - val_mean_absolute_error: 0.3712 Epoch 86/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2417 - mean_absolute_error: 0.3344 - val_loss: 0.3122 - val_mean_absolute_error: 0.3806 Epoch 87/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2417 - mean_absolute_error: 0.3353 - val_loss: 0.2922 - val_mean_absolute_error: 0.3573 Epoch 88/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2402 - mean_absolute_error: 0.3347 - val_loss: 0.2990 - val_mean_absolute_error: 0.3648 Epoch 89/150 410/410 [==============================] - 1s 3ms/step - loss: 0.2383 - mean_absolute_error: 0.3324 - val_loss: 0.2984 - val_mean_absolute_error: 0.3653 Epoch 90/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2397 - mean_absolute_error: 0.3338 - val_loss: 0.2947 - val_mean_absolute_error: 0.3648 Epoch 91/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2402 - mean_absolute_error: 0.3341 - val_loss: 0.3147 - val_mean_absolute_error: 0.3704 Epoch 92/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2391 - mean_absolute_error: 0.3340 - val_loss: 0.2971 - val_mean_absolute_error: 0.3636 Epoch 93/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2379 - mean_absolute_error: 0.3323 - val_loss: 0.2973 - val_mean_absolute_error: 0.3563 Epoch 94/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2369 - mean_absolute_error: 0.3320 - val_loss: 0.3061 - val_mean_absolute_error: 0.3600 Epoch 95/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2367 - mean_absolute_error: 0.3320 - val_loss: 0.2945 - val_mean_absolute_error: 0.3626 Epoch 96/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2374 - mean_absolute_error: 0.3332 - val_loss: 0.2952 - val_mean_absolute_error: 0.3661 Epoch 97/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2360 - mean_absolute_error: 0.3314 - val_loss: 0.3027 - val_mean_absolute_error: 0.3579 Epoch 98/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2370 - mean_absolute_error: 0.3329 - val_loss: 0.2946 - val_mean_absolute_error: 0.3585 Epoch 99/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2401 - mean_absolute_error: 0.3325 - val_loss: 0.2981 - val_mean_absolute_error: 0.3572 Epoch 100/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2350 - mean_absolute_error: 0.3313 - val_loss: 0.2998 - val_mean_absolute_error: 0.3569 Epoch 101/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2348 - mean_absolute_error: 0.3308 - val_loss: 0.2981 - val_mean_absolute_error: 0.3718 Epoch 102/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2332 - mean_absolute_error: 0.3292 - val_loss: 0.3151 - val_mean_absolute_error: 0.3881 Epoch 103/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2338 - mean_absolute_error: 0.3298 - val_loss: 0.2954 - val_mean_absolute_error: 0.3628 Epoch 104/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2336 - mean_absolute_error: 0.3298 - val_loss: 0.3009 - val_mean_absolute_error: 0.3679 Epoch 105/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2329 - mean_absolute_error: 0.3289 - val_loss: 0.2938 - val_mean_absolute_error: 0.3587 Epoch 106/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2316 - mean_absolute_error: 0.3283 - val_loss: 0.2990 - val_mean_absolute_error: 0.3556 Epoch 107/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2329 - mean_absolute_error: 0.3291 - val_loss: 0.2904 - val_mean_absolute_error: 0.3553 Epoch 108/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2321 - mean_absolute_error: 0.3277 - val_loss: 0.2956 - val_mean_absolute_error: 0.3588 Epoch 109/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2353 - mean_absolute_error: 0.3298 - val_loss: 0.2900 - val_mean_absolute_error: 0.3645 Epoch 110/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2321 - mean_absolute_error: 0.3289 - val_loss: 0.2951 - val_mean_absolute_error: 0.3708 Epoch 111/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2292 - mean_absolute_error: 0.3267 - val_loss: 0.3024 - val_mean_absolute_error: 0.3787 Epoch 112/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2297 - mean_absolute_error: 0.3265 - val_loss: 0.2942 - val_mean_absolute_error: 0.3593 Epoch 113/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2305 - mean_absolute_error: 0.3275 - val_loss: 0.3016 - val_mean_absolute_error: 0.3767 Epoch 114/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2281 - mean_absolute_error: 0.3264 - val_loss: 0.2936 - val_mean_absolute_error: 0.3552 Epoch 115/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2295 - mean_absolute_error: 0.3273 - val_loss: 0.2995 - val_mean_absolute_error: 0.3701 Epoch 116/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2286 - mean_absolute_error: 0.3263 - val_loss: 0.2912 - val_mean_absolute_error: 0.3616 Epoch 117/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2287 - mean_absolute_error: 0.3257 - val_loss: 0.2982 - val_mean_absolute_error: 0.3655 Epoch 118/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2297 - mean_absolute_error: 0.3258 - val_loss: 0.2932 - val_mean_absolute_error: 0.3663 Epoch 119/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2282 - mean_absolute_error: 0.3254 - val_loss: 0.2913 - val_mean_absolute_error: 0.3551 Epoch 120/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2289 - mean_absolute_error: 0.3261 - val_loss: 0.2969 - val_mean_absolute_error: 0.3529 Epoch 121/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2273 - mean_absolute_error: 0.3247 - val_loss: 0.2998 - val_mean_absolute_error: 0.3777 Epoch 122/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2274 - mean_absolute_error: 0.3261 - val_loss: 0.2891 - val_mean_absolute_error: 0.3607 Epoch 123/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2295 - mean_absolute_error: 0.3272 - val_loss: 0.2906 - val_mean_absolute_error: 0.3588 Epoch 124/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2290 - mean_absolute_error: 0.3273 - val_loss: 0.2894 - val_mean_absolute_error: 0.3602 Epoch 125/150 410/410 [==============================] - 1s 3ms/step - loss: 0.2278 - mean_absolute_error: 0.3256 - val_loss: 0.2883 - val_mean_absolute_error: 0.3590 Epoch 126/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2269 - mean_absolute_error: 0.3252 - val_loss: 0.2937 - val_mean_absolute_error: 0.3556 Epoch 127/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2271 - mean_absolute_error: 0.3246 - val_loss: 0.2919 - val_mean_absolute_error: 0.3541 Epoch 128/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2256 - mean_absolute_error: 0.3242 - val_loss: 0.2894 - val_mean_absolute_error: 0.3544 Epoch 129/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2248 - mean_absolute_error: 0.3242 - val_loss: 0.2890 - val_mean_absolute_error: 0.3536 Epoch 130/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2258 - mean_absolute_error: 0.3251 - val_loss: 0.2888 - val_mean_absolute_error: 0.3498 Epoch 131/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2267 - mean_absolute_error: 0.3249 - val_loss: 0.2890 - val_mean_absolute_error: 0.3571 Epoch 132/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2255 - mean_absolute_error: 0.3225 - val_loss: 0.2959 - val_mean_absolute_error: 0.3639 Epoch 133/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2247 - mean_absolute_error: 0.3238 - val_loss: 0.3008 - val_mean_absolute_error: 0.3538 Epoch 134/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2256 - mean_absolute_error: 0.3236 - val_loss: 0.2906 - val_mean_absolute_error: 0.3499 Epoch 135/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2233 - mean_absolute_error: 0.3225 - val_loss: 0.2922 - val_mean_absolute_error: 0.3584 Epoch 136/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2223 - mean_absolute_error: 0.3218 - val_loss: 0.2916 - val_mean_absolute_error: 0.3576 Epoch 137/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2224 - mean_absolute_error: 0.3210 - val_loss: 0.3019 - val_mean_absolute_error: 0.3587 Epoch 138/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2238 - mean_absolute_error: 0.3229 - val_loss: 0.3015 - val_mean_absolute_error: 0.3580 Epoch 139/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2238 - mean_absolute_error: 0.3229 - val_loss: 0.2901 - val_mean_absolute_error: 0.3607 Epoch 140/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2248 - mean_absolute_error: 0.3239 - val_loss: 0.2936 - val_mean_absolute_error: 0.3643 Epoch 141/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2237 - mean_absolute_error: 0.3225 - val_loss: 0.2952 - val_mean_absolute_error: 0.3576 Epoch 142/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2227 - mean_absolute_error: 0.3218 - val_loss: 0.2827 - val_mean_absolute_error: 0.3479 Epoch 143/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2221 - mean_absolute_error: 0.3203 - val_loss: 0.2915 - val_mean_absolute_error: 0.3605 Epoch 144/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2232 - mean_absolute_error: 0.3222 - val_loss: 0.2929 - val_mean_absolute_error: 0.3605 Epoch 145/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2223 - mean_absolute_error: 0.3216 - val_loss: 0.2914 - val_mean_absolute_error: 0.3639 Epoch 146/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2225 - mean_absolute_error: 0.3235 - val_loss: 0.2891 - val_mean_absolute_error: 0.3572 Epoch 147/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2214 - mean_absolute_error: 0.3210 - val_loss: 0.2862 - val_mean_absolute_error: 0.3535 Epoch 148/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2221 - mean_absolute_error: 0.3216 - val_loss: 0.2944 - val_mean_absolute_error: 0.3589 Epoch 149/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2235 - mean_absolute_error: 0.3226 - val_loss: 0.2892 - val_mean_absolute_error: 0.3554 Epoch 150/150 410/410 [==============================] - 1s 2ms/step - loss: 0.2205 - mean_absolute_error: 0.3197 - val_loss: 0.2908 - val_mean_absolute_error: 0.3615 print( \u0026#39;- Stats on Training set:\u0026#39;, \u0026#39;\\n\\t* Loss:\\t\\t\u0026#39;, history.history[\u0026#39;loss\u0026#39;][-1], \u0026#39;\\n\\t* MAE:\\t\\t\u0026#39;, history.history[\u0026#39;mean_absolute_error\u0026#39;][-1], \u0026#39;\\n- Stats on Validation set:\u0026#39;, \u0026#39;\\n\\t* loss:\\t\\t\u0026#39;, history.history[\u0026#39;val_loss\u0026#39;][-1], \u0026#39;\\n\\t* MAE:\\t\\t\u0026#39;, history.history[\u0026#39;val_mean_absolute_error\u0026#39;][-1], ) - Stats on Training set: * Loss:\t0.2204761952161789 * MAE:\t0.3197171688079834 - Stats on Validation set: * loss:\t0.29083022475242615 * MAE:\t0.3615073263645172 model.summary() Model: \u0026quot;sequential\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 10) 90 _________________________________________________________________ dense_1 (Dense) (None, 30) 330 _________________________________________________________________ dense_2 (Dense) (None, 40) 1240 _________________________________________________________________ dense_3 (Dense) (None, 1) 41 ================================================================= Total params: 1,701 Trainable params: 1,701 Non-trainable params: 0 _________________________________________________________________ Warning: In the first Dense layer I was expecting 90 parameters, but I have only 80 parameters. From this observation, the formula seems to be: $$\\#param = \\#inp \\cdot \\#neurons_{layer_1};$$ but the true formula is: $$ \\#param = (\\#inp + 1) \\cdot \\#neurons_{layer_1}. $$ Maybe in the slides we have that #inputs = 7. 1\nEvaluate the model #On the Training set ## Initialize the figure width, height = 10, 5 nfig = 2 fig = plt.figure(figsize = (width*nfig,height)) # SBP 1: LOSS ax1 = fig.add_subplot(1, nfig, 1); ax1.plot(range(1,EPOCHS+1),history.history[\u0026#39;loss\u0026#39;], color=\u0026#39;darkblue\u0026#39;, label=\u0026#34;Training loss\u0026#34;) ax1.plot(range(1,EPOCHS+1),history.history[\u0026#39;val_loss\u0026#39;], color=\u0026#39;darkorange\u0026#39;, label=\u0026#34;validation loss\u0026#34;,axes =ax1) ax1.legend(loc=\u0026#39;best\u0026#39;, shadow=True) ax1.set_xlabel(\u0026#39;Epoch\u0026#39;,fontsize=14); ax1.set_ylabel(\u0026#39;Loss\u0026#39;,fontsize=14); ax1.set_title(\u0026#39;Loss\u0026#39;,fontsize=18); ax1.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); # SBP 1: MAE ax2 = fig.add_subplot(1, nfig, 2); ax2.plot(range(1,EPOCHS+1),history.history[\u0026#39;mean_absolute_error\u0026#39;], color=\u0026#39;darkblue\u0026#39;, label=\u0026#34;Training accuracy\u0026#34;) ax2.plot(range(1,EPOCHS+1),history.history[\u0026#39;val_mean_absolute_error\u0026#39;], color=\u0026#39;darkorange\u0026#39;,label=\u0026#34;Validation accuracy\u0026#34;) ax2.legend(loc=\u0026#39;best\u0026#39;, shadow=True) ax2.set_xlabel(\u0026#39;Epoch\u0026#39;,fontsize=14); ax2.set_ylabel(\u0026#39;Mean Absolute Error\u0026#39;,fontsize=14); ax2.set_title(\u0026#39;MAE\u0026#39;,fontsize=18); ax2.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); # plt.suptitle(\u0026#34;Stats on Training set\u0026#34;,fontsize=25) # plt.subplots_adjust(top=0.8) # change title position plt.show() On the Test set #def root_mean_squared_error(y_true, y_pred): return np.sqrt(np.mean((y_pred-y_true)**2)) # Compute LOSS and MAE on Test set loss, mae = model.evaluate(X_test, y_test, verbose = 0); # 133/133 because it\u0026#39;s the number of batches: # X_test.shape[0]/32 (default batch_size = 32) # Compute RMSE on Test set y_pred = model.predict(X_test) A = y_test.values # convert into a numpy array B = y_pred.flatten() # to get rid off the multiple brackets returned by predict method rmse = root_mean_squared_error(A, B) print(f\u0026#34;- Statistics on the Test set:\\n\\ \\t* Test Loss: {loss}\\n\\ \\t* Test MAE: {mae}\\n\\ \\t* Test RMSE: {rmse}\u0026#34; ) - Statistics on the Test set: * Test Loss: 0.2790662944316864 * Test MAE: 0.35706254839897156 * Test RMSE: 0.5282672929159882 Warning: we don\u0026rsquo;t store these results now, because we perform the same model below. Generating other ANN models #4. Test the following network structures and compare the results in terms of training/validation MAE/loss, RMSE on test set: 1 layer containing a single neuron 1 layer containing 3 neurons 1 layer containing 10 neurons 2 layers containing respectively 10 and 30 neurons 3 layers containing respectively 10, 30 and 40 neurons def create_model(network): num_layers = len(network) model = Sequential() # Input Layer model.add(Dense(network[0], input_dim=X_train.shape[1], activation=\u0026#39;relu\u0026#39;)) # Hidden Layers if num_layers \u0026gt; 1: for i in range(1,num_layers): model.add(Dense(network[i], activation=\u0026#39;relu\u0026#39;)) # Output Layer model.add(Dense(1)) return model def get_test_stats(model, xtest, ytest, verbose_flag): # Compute LOSS and MAE on Test set loss, mae = model.evaluate(xtest, ytest, verbose = verbose_flag); # Compute RMSE on Test set y_pred = model.predict(xtest) rmse = root_mean_squared_error(y_test.values, y_pred.flatten()) return loss, mae, rmse, y_pred DOE = [[1], [3], [10], [10,30], [10,30,40]] #Design of experiment from time import time # Store the info in order to compare the results with the following models. training_loss, training_MAE = [], [] val_loss, val_MAE = [], [] test_loss, test_MAE, test_RMSE = [], [], [] net_struct, net_epochs, pred_list = [], [], [] #info about the network setting print(f\u0026#34;Now we preform {len(DOE)} ANN models.\\n\u0026#34;) for network in DOE: idx = DOE.index(network) # we consider as \u0026#34;MODEL #0\u0026#34; the one shown above! print(150*\u0026#34;=\u0026#34;) print(f\u0026#34;[INFO] MODEL #{idx+1} using {DOE[idx]} neurons. [{idx+1}/{len(DOE)}]\\n\u0026#34;) custom_model = create_model(network) ## Compile the model custom_model.compile(optimizer =\u0026#39;adam\u0026#39;,loss = \u0026#39;mean_squared_error\u0026#39;, metrics=[metrics.mae]) ## Train model on full train set, with 80/20 CV split print(f\u0026#34;[INFO] Fitting using {EPOCHS} epochs...\u0026#34;) print(f\u0026#34;Train on {X_train.shape[0]} samples, validate on {X_val.shape[0]} samples.\u0026#34;) tstart = time() custom_history = custom_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=EPOCHS, batch_size=32, verbose = 0) tend = time() - tstart print(f\u0026#34;\\n...OK, fitted the model in {tend}s.\u0026#34;) ## Summary print(\u0026#34;\\n[INFO] Summary:\u0026#34;) custom_model.summary() ## Test set statistics print(\u0026#34;\\n[INFO] Evaluate the model on Test set:\u0026#34;) loss, mae, rmse, y_pred = get_test_stats(custom_model, X_test, y_test, verbose_flag = 1) print(\u0026#39;\\n[INFO] Statistics:\\ \\n- Stats on Training set:\u0026#39;, \u0026#39;\\n\\t* Loss:\\t\\t\u0026#39;, custom_history.history[\u0026#39;loss\u0026#39;][-1], \u0026#39;\\n\\t* MAE:\\t\\t\u0026#39;, custom_history.history[\u0026#39;mean_absolute_error\u0026#39;][-1], \u0026#39;\\n- Stats on Validation set:\u0026#39;, \u0026#39;\\n\\t* loss:\\t\\t\u0026#39;, custom_history.history[\u0026#39;val_loss\u0026#39;][-1], \u0026#39;\\n\\t* MAE:\\t\\t\u0026#39;, custom_history.history[\u0026#39;val_mean_absolute_error\u0026#39;][-1], \u0026#39;\\n- Stats on Test set:\u0026#39;, \u0026#39;\\n\\t* loss:\\t\\t\u0026#39;, loss, \u0026#39;\\n\\t* MAE:\\t\\t\u0026#39;, mae, \u0026#39;\\n\\t* RMSE:\\t\\t\u0026#39;, rmse, ) ## Store all the statistics # store training info training_loss.append(custom_history.history[\u0026#39;loss\u0026#39;]) training_MAE.append(custom_history.history[\u0026#39;mean_absolute_error\u0026#39;]) # store val info val_loss.append(custom_history.history[\u0026#39;val_loss\u0026#39;]) val_MAE.append(custom_history.history[\u0026#39;val_mean_absolute_error\u0026#39;]) # store test info test_loss.append(loss) test_MAE.append(mae) test_RMSE.append(rmse) #structure of the network net_struct.append(DOE[idx]) net_epochs.append(EPOCHS) pred_list.append(y_pred) print(150*\u0026#34;=\u0026#34;) print(\u0026#34;\\n\u0026#34;) print(f\u0026#34;Performed all the {len(DOE)} models.\u0026#34;) Now we preform 5 ANN models. ====================================================================================================================================================== [INFO] MODEL #1 using [1] neurons. [1/5] [INFO] Fitting using 150 epochs... Train on 13108 samples, validate on 3277 samples. ...OK, fitted the model in 81.33795595169067s. [INFO] Summary: Model: \u0026quot;sequential_1\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_4 (Dense) (None, 1) 9 _________________________________________________________________ dense_5 (Dense) (None, 1) 2 ================================================================= Total params: 11 Trainable params: 11 Non-trainable params: 0 _________________________________________________________________ [INFO] Evaluate the model on Test set: 133/133 [==============================] - 0s 1ms/step - loss: 0.5043 - mean_absolute_error: 0.5226 [INFO] Statistics: - Stats on Training set: * Loss:\t0.5019071698188782 * MAE:\t0.5186200737953186 - Stats on Validation set: * loss:\t0.5074825882911682 * MAE:\t0.5189610123634338 - Stats on Test set: * loss:\t0.5042912364006042 * MAE:\t0.5225676894187927 * RMSE:\t0.7101348192496855 ====================================================================================================================================================== ====================================================================================================================================================== [INFO] MODEL #2 using [3] neurons. [2/5] [INFO] Fitting using 150 epochs... Train on 13108 samples, validate on 3277 samples. ...OK, fitted the model in 86.69740176200867s. [INFO] Summary: Model: \u0026quot;sequential_2\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_6 (Dense) (None, 3) 27 _________________________________________________________________ dense_7 (Dense) (None, 1) 4 ================================================================= Total params: 31 Trainable params: 31 Non-trainable params: 0 _________________________________________________________________ [INFO] Evaluate the model on Test set: 133/133 [==============================] - 0s 1ms/step - loss: 0.4569 - mean_absolute_error: 0.4864 [INFO] Statistics: - Stats on Training set: * Loss:\t0.45196104049682617 * MAE:\t0.4827933609485626 - Stats on Validation set: * loss:\t0.4575823247432709 * MAE:\t0.4799402058124542 - Stats on Test set: * loss:\t0.4569326639175415 * MAE:\t0.4863855540752411 * RMSE:\t0.6759679391200216 ====================================================================================================================================================== ====================================================================================================================================================== [INFO] MODEL #3 using [10] neurons. [3/5] [INFO] Fitting using 150 epochs... Train on 13108 samples, validate on 3277 samples. ...OK, fitted the model in 85.105064868927s. [INFO] Summary: Model: \u0026quot;sequential_3\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_8 (Dense) (None, 10) 90 _________________________________________________________________ dense_9 (Dense) (None, 1) 11 ================================================================= Total params: 101 Trainable params: 101 Non-trainable params: 0 _________________________________________________________________ [INFO] Evaluate the model on Test set: 133/133 [==============================] - 0s 1ms/step - loss: 0.3562 - mean_absolute_error: 0.4191 [INFO] Statistics: - Stats on Training set: * Loss:\t0.3352857232093811 * MAE:\t0.40571486949920654 - Stats on Validation set: * loss:\t0.36747753620147705 * MAE:\t0.41564399003982544 - Stats on Test set: * loss:\t0.35619547963142395 * MAE:\t0.4191289246082306 * RMSE:\t0.59682117017886 ====================================================================================================================================================== ====================================================================================================================================================== [INFO] MODEL #4 using [10, 30] neurons. [4/5] [INFO] Fitting using 150 epochs... Train on 13108 samples, validate on 3277 samples. ...OK, fitted the model in 106.76856350898743s. [INFO] Summary: Model: \u0026quot;sequential_4\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_10 (Dense) (None, 10) 90 _________________________________________________________________ dense_11 (Dense) (None, 30) 330 _________________________________________________________________ dense_12 (Dense) (None, 1) 31 ================================================================= Total params: 451 Trainable params: 451 Non-trainable params: 0 _________________________________________________________________ [INFO] Evaluate the model on Test set: 133/133 [==============================] - 0s 1ms/step - loss: 0.2938 - mean_absolute_error: 0.3772 [INFO] Statistics: - Stats on Training set: * Loss:\t0.25864139199256897 * MAE:\t0.3488193154335022 - Stats on Validation set: * loss:\t0.29905804991722107 * MAE:\t0.37346842885017395 - Stats on Test set: * loss:\t0.29382142424583435 * MAE:\t0.37718966603279114 * RMSE:\t0.5420530397809932 ====================================================================================================================================================== ====================================================================================================================================================== [INFO] MODEL #5 using [10, 30, 40] neurons. [5/5] [INFO] Fitting using 150 epochs... Train on 13108 samples, validate on 3277 samples. ...OK, fitted the model in 123.2047529220581s. [INFO] Summary: Model: \u0026quot;sequential_5\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_13 (Dense) (None, 10) 90 _________________________________________________________________ dense_14 (Dense) (None, 30) 330 _________________________________________________________________ dense_15 (Dense) (None, 40) 1240 _________________________________________________________________ dense_16 (Dense) (None, 1) 41 ================================================================= Total params: 1,701 Trainable params: 1,701 Non-trainable params: 0 _________________________________________________________________ [INFO] Evaluate the model on Test set: 133/133 [==============================] - 0s 2ms/step - loss: 0.2787 - mean_absolute_error: 0.3495 [INFO] Statistics: - Stats on Training set: * Loss:\t0.234622061252594 * MAE:\t0.33155113458633423 - Stats on Validation set: * loss:\t0.2967351973056793 * MAE:\t0.35640445351600647 - Stats on Test set: * loss:\t0.27867719531059265 * MAE:\t0.3495338559150696 * RMSE:\t0.5278987847617789 ====================================================================================================================================================== Performed all the 5 models. Warning: pay attention to running multiple times the above cell, because you append other results on the final statistics. # Collect all the most useful data into a DataFrame stats = pd.DataFrame({ \u0026#39;ANN_structure\u0026#39;: net_struct, \u0026#39;ANN_epochs\u0026#39;: net_epochs, \u0026#39;Training Loss\u0026#39;: [last for *_, last in training_loss], \u0026#39;Training MAE\u0026#39;: [last for *_, last in training_MAE], \u0026#39;Validation Loss\u0026#39;: [last for *_, 0.435833last in val_loss], \u0026#39;Validation MAE\u0026#39;: [last for *_, last in val_MAE], \u0026#39;Test Loss\u0026#39;: test_loss, \u0026#39;Test MAE\u0026#39;: test_MAE, \u0026#39;Test RMSE\u0026#39;: test_RMSE }) stats ANN_structure ANN_epochs Training Loss Training MAE Validation Loss Validation MAE Test Loss Test MAE Test RMSE 0 [1] 150 0.421746 0.471426 0.454049 0.483403 0.435833 0.480550 0.660177 1 [3] 150 0.372986 0.437562 0.405804 0.448418 0.384103 0.451114 0.619760 2 [10] 150 0.319872 0.398626 0.359499 0.417824 0.348028 0.418218 0.589939 3 [10, 30] 150 0.261056 0.350203 0.307047 0.368865 0.299178 0.371206 0.546971 4 [10, 30, 40] 150 0.239802 0.333529 0.308254 0.359592 0.298137 0.361128 0.546019 5. Generate a chart in which the performance of these models are displayed and compared Compare the results #On the Training set ## Initialize the figure width, height = 10, 5 nfig = 2 fig = plt.figure(figsize = (width*nfig,height)) # SBP 1: LOSS on Training set ax1 = fig.add_subplot(1, nfig, 1); for i in range(0,len(DOE)): ax1.plot(range(1,EPOCHS+1), training_loss[i], label=\u0026#34;training_nn: \u0026#34; + str(net_struct[i])) ax1.legend(loc=\u0026#39;best\u0026#39;, shadow=True) ax1.set_xlabel(\u0026#39;Epoch\u0026#39;,fontsize=14); ax1.set_ylabel(\u0026#39;Loss\u0026#39;,fontsize=14); ax1.set_title(\u0026#39;Loss on Training set\u0026#39;,fontsize=18); ax1.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); # SBP 2: LOSS on Validation set ax2 = fig.add_subplot(1, nfig, 2); for i in range(0,len(DOE)): ax2.plot(range(1,EPOCHS+1), val_loss[i], label=\u0026#34;val_nn: \u0026#34; + str(net_struct[i])) ax2.legend(loc=\u0026#39;best\u0026#39;, shadow=True) ax2.set_xlabel(\u0026#39;Epoch\u0026#39;,fontsize=14); ax2.set_ylabel(\u0026#39;Loss\u0026#39;,fontsize=14); ax2.set_title(\u0026#39;Loss on Validation set\u0026#39;,fontsize=18); ax2.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); plt.show() # Initialize the figure width, height = 10, 5 nfig = 2 fig = plt.figure(figsize = (width*nfig,height)) # SBP 1: LOSS on Training set ax1 = fig.add_subplot(1, nfig, 1); for i in range(0,len(DOE)): ax1.plot(range(1,EPOCHS+1), training_MAE[i], label=\u0026#34;training_nn: \u0026#34; + str(net_struct[i])) ax1.legend(loc=\u0026#39;best\u0026#39;, shadow=True) ax1.set_xlabel(\u0026#39;Epoch\u0026#39;,fontsize=14); ax1.set_ylabel(\u0026#39;Loss\u0026#39;,fontsize=14); ax1.set_title(\u0026#39;MAE on Training set\u0026#39;,fontsize=18); ax1.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); # SBP 2: LOSS on Validation set ax2 = fig.add_subplot(1, nfig, 2); for i in range(0,len(DOE)): ax2.plot(range(1,EPOCHS+1), val_MAE[i], label=\u0026#34;val_nn: \u0026#34; + str(net_struct[i])) ax2.legend(loc=\u0026#39;best\u0026#39;, shadow=True) ax2.set_xlabel(\u0026#39;Epoch\u0026#39;,fontsize=14); ax2.set_ylabel(\u0026#39;Loss\u0026#39;,fontsize=14); ax2.set_title(\u0026#39;MAE on Validation set\u0026#39;,fontsize=18); ax2.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); plt.show() On the Test set ## Initialize the figure width, height = 10, 5 fig = plt.figure(figsize = (width,height)) ax1 = fig.add_subplot(1, 1, 1); ax1.bar(range(1,len(DOE)+1), test_RMSE,width=0.4) ax1.set_xlabel(\u0026#39;model\u0026#39;,fontsize=14); ax1.set_ylabel(\u0026#39;RMSE\u0026#39;,fontsize=14); ax1.set_title(\u0026#39;RMSE on Test set: comparison\u0026#39;,fontsize=18); ax1.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); # change the x-axis xrange = [1,2,3,4,5] squad = net_struct ax1.set_xticks(xrange) ax1.set_xticklabels(squad, minor=False, rotation=45) for xx,yy in zip(xrange,test_RMSE): ax1.text(xx -0.15, yy + .005, str(test_RMSE[xx-1].round(3)), color=\u0026#39;darkblue\u0026#39;, fontweight=\u0026#39;bold\u0026#39;) plt.show() The first and the last models #ns = 50 # number of samples to visualize id_one, id_two = 0,-1 # index of the two models we want to compare # Initialize the figure width, height = 7, 10 # single pic rows, columns = 3, 2 fig = plt.figure(figsize = (width*rows,height*columns)) idx_model = id_one y_pred = pred_list[idx_model] # prediction of a certain model ## SBP 1 ax1 = fig.add_subplot(rows, columns, 1); for i in range(0,ns+1): ax1.plot(i,y_pred[i], \u0026#39;darkorange\u0026#39;,marker=\u0026#39;o\u0026#39;) ax1.plot(i,y_test[i], \u0026#39;b\u0026#39;,marker=\u0026#39;o\u0026#39;) ax1.plot([i, i], [y_pred[i], y_test[i]], color=\u0026#39;grey\u0026#39;) # distance btw y_test and y_pred ax1.legend([\u0026#39;Prediction\u0026#39;,\u0026#39;Real\u0026#39;]) ax1.set_xlabel(\u0026#39;samples\u0026#39;,fontsize=14); ax1.set_ylabel(\u0026#39;median house value\u0026#39;,fontsize=14); ax1.set_title(f\u0026#39;Houses prices prediction:\\nANN structure: {net_struct[idx_model]}\u0026#39;,fontsize=18); ax1.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); ## SBP 3 ax3 = fig.add_subplot(rows, columns, 3); ax3.plot(range(1,EPOCHS+1), training_loss[idx_model], label=\u0026#34;training loss\u0026#34;) ax3.plot(range(1,EPOCHS+1), val_loss[idx_model], label=\u0026#34;val loss\u0026#34;) ax3.legend() ax3.set_xlabel(\u0026#39;Epoch\u0026#39;,fontsize=14); ax3.set_ylabel(\u0026#39;loss\u0026#39;,fontsize=14); ax3.set_title(f\u0026#39;Loss:\\nANN structure: {net_struct[idx_model]}\u0026#39;,fontsize=18); ax3.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); ## SBP 5 ax5 = fig.add_subplot(rows, columns, 5); ax5.plot(range(1,EPOCHS+1), training_MAE[idx_model], label=\u0026#34;training MAE\u0026#34;) ax5.plot(range(1,EPOCHS+1), val_MAE[idx_model], label=\u0026#34;val MAE\u0026#34;) ax5.legend() ax5.set_xlabel(\u0026#39;Epoch\u0026#39;,fontsize=14); ax5.set_ylabel(\u0026#39;MAE\u0026#39;,fontsize=14); ax5.set_title(f\u0026#39;MAE:\\nANN structure: {net_struct[idx_model]}\u0026#39;,fontsize=18); ax5.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); idx_model = id_two y_pred = pred_list[idx_model] # prediction of a certain model ## SBP 2 ax2 = fig.add_subplot(rows, columns, 2); for i in range(0,ns+1): ax2.plot(i,y_pred[i], \u0026#39;darkorange\u0026#39;,marker=\u0026#39;o\u0026#39;) ax2.plot(i,y_test[i], \u0026#39;b\u0026#39;,marker=\u0026#39;o\u0026#39;) ax2.plot([i, i], [y_pred[i], y_test[i]], color=\u0026#39;grey\u0026#39;) # distance btw y_test and y_pred ax2.legend([\u0026#39;prediction\u0026#39;,\u0026#39;actual value\u0026#39;]) ax2.set_xlabel(\u0026#39;samples\u0026#39;,fontsize=14); ax2.set_ylabel(\u0026#39;median house value\u0026#39;,fontsize=14); ax2.set_title(f\u0026#39;House prices prediction\\nANN structure: {net_struct[idx_model]}\u0026#39;,fontsize=18); ax2.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); ## SBP 4 ax4 = fig.add_subplot(rows, columns, 4); ax4.plot(range(1,EPOCHS+1), training_loss[idx_model], label=\u0026#34;training loss\u0026#34;) ax4.plot(range(1,EPOCHS+1), val_loss[idx_model], label=\u0026#34;val loss\u0026#34;) ax4.legend() ax4.set_xlabel(\u0026#39;Epoch\u0026#39;,fontsize=14); ax4.set_ylabel(\u0026#39;loss\u0026#39;,fontsize=14); ax4.set_title(f\u0026#39;Loss:\\nANN structure: {net_struct[idx_model]}\u0026#39;,fontsize=18); ax4.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); ## SBP 6 ax6 = fig.add_subplot(rows, columns, 6); ax6.plot(range(1,EPOCHS+1), training_MAE[idx_model], label=\u0026#34;training MAE\u0026#34;) ax6.plot(range(1,EPOCHS+1), val_MAE[idx_model], label=\u0026#34;val MAE\u0026#34;) ax6.legend() ax6.set_xlabel(\u0026#39;Epoch\u0026#39;,fontsize=14); ax6.set_ylabel(\u0026#39;MAE\u0026#39;,fontsize=14); ax6.set_title(f\u0026#39;MAE:\\nANN structure: {net_struct[idx_model]}\u0026#39;,fontsize=18); ax6.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); # set the spacing between subplots plt.suptitle(f\u0026#34;Comparison ANN structure:\\n{net_struct[id_one]} vs. {net_struct[id_two]}\u0026#34;, fontsize=20) plt.subplots_adjust(top=0.9, wspace=0.25, hspace=0.35) plt.show() /opt/conda/lib/python3.7/site-packages/numpy/core/shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray. ary = asanyarray(ary) The last two models #ns = 50 # number of samples to visualize id_one, id_two = -2,-1 # index of the two models we want to compare # Initialize the figure width, height = 7, 10 # single pic rows, columns = 3, 2 fig = plt.figure(figsize = (width*rows,height*columns)) idx_model = id_one y_pred = pred_list[idx_model] # prediction of a certain model ## SBP 1 ax1 = fig.add_subplot(rows, columns, 1); for i in range(0,ns+1): ax1.plot(i,y_pred[i], \u0026#39;darkorange\u0026#39;,marker=\u0026#39;o\u0026#39;) ax1.plot(i,y_test[i], \u0026#39;b\u0026#39;,marker=\u0026#39;o\u0026#39;) ax1.plot([i, i], [y_pred[i], y_test[i]], color=\u0026#39;grey\u0026#39;) # distance btw y_test and y_pred ax1.legend([\u0026#39;Prediction\u0026#39;,\u0026#39;Real\u0026#39;]) ax1.set_xlabel(\u0026#39;samples\u0026#39;,fontsize=14); ax1.set_ylabel(\u0026#39;median house value\u0026#39;,fontsize=14); ax1.set_title(f\u0026#39;House prices prediction:\\nANN structure: {net_struct[idx_model]}\u0026#39;,fontsize=18); ax1.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); ## SBP 3 ax3 = fig.add_subplot(rows, columns, 3); ax3.plot(range(1,EPOCHS+1), training_loss[idx_model], label=\u0026#34;training loss\u0026#34;) ax3.plot(range(1,EPOCHS+1), val_loss[idx_model], label=\u0026#34;val loss\u0026#34;) ax3.legend() ax3.set_xlabel(\u0026#39;Epoch\u0026#39;,fontsize=14); ax3.set_ylabel(\u0026#39;loss\u0026#39;,fontsize=14); ax3.set_title(f\u0026#39;Loss:\\nANN structure: {net_struct[idx_model]}\u0026#39;,fontsize=18); ax3.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); ## SBP 5 ax5 = fig.add_subplot(rows, columns, 5); ax5.plot(range(1,EPOCHS+1), training_MAE[idx_model], label=\u0026#34;training MAE\u0026#34;) ax5.plot(range(1,EPOCHS+1), val_MAE[idx_model], label=\u0026#34;val MAE\u0026#34;) ax5.legend() ax5.set_xlabel(\u0026#39;Epoch\u0026#39;,fontsize=14); ax5.set_ylabel(\u0026#39;MAE\u0026#39;,fontsize=14); ax5.set_title(f\u0026#39;MAE:\\nANN structure: {net_struct[idx_model]}\u0026#39;,fontsize=18); ax5.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); idx_model = id_two y_pred = pred_list[idx_model] # prediction of a certain model ## SBP 2 ax2 = fig.add_subplot(rows, columns, 2); for i in range(0,ns+1): ax2.plot(i,y_pred[i], \u0026#39;darkorange\u0026#39;,marker=\u0026#39;o\u0026#39;) ax2.plot(i,y_test[i], \u0026#39;b\u0026#39;,marker=\u0026#39;o\u0026#39;) ax2.plot([i, i], [y_pred[i], y_test[i]], color=\u0026#39;grey\u0026#39;) # distance btw y_test and y_pred ax2.legend([\u0026#39;prediction\u0026#39;,\u0026#39;actual value\u0026#39;]) ax2.set_xlabel(\u0026#39;samples\u0026#39;,fontsize=14); ax2.set_ylabel(\u0026#39;median house value\u0026#39;,fontsize=14); ax2.set_title(f\u0026#39;Houses prices prediction\\nANN structure: {net_struct[idx_model]}\u0026#39;,fontsize=18); ax2.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); ## SBP 4 ax4 = fig.add_subplot(rows, columns, 4); ax4.plot(range(1,EPOCHS+1), training_loss[idx_model], label=\u0026#34;training loss\u0026#34;) ax4.plot(range(1,EPOCHS+1), val_loss[idx_model], label=\u0026#34;val loss\u0026#34;) ax4.legend() ax4.set_xlabel(\u0026#39;Epoch\u0026#39;,fontsize=14); ax4.set_ylabel(\u0026#39;loss\u0026#39;,fontsize=14); ax4.set_title(f\u0026#39;Loss:\\nANN structure: {net_struct[idx_model]}\u0026#39;,fontsize=18); ax4.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); ## SBP 6 ax6 = fig.add_subplot(rows, columns, 6); ax6.plot(range(1,EPOCHS+1), training_MAE[idx_model], label=\u0026#34;training MAE\u0026#34;) ax6.plot(range(1,EPOCHS+1), val_MAE[idx_model], label=\u0026#34;val MAE\u0026#34;) ax6.legend() ax6.set_xlabel(\u0026#39;Epoch\u0026#39;,fontsize=14); ax6.set_ylabel(\u0026#39;MAE\u0026#39;,fontsize=14); ax6.set_title(f\u0026#39;MAE:\\nANN structure: {net_struct[idx_model]}\u0026#39;,fontsize=18); ax6.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); # set the spacing between subplots plt.suptitle(f\u0026#34;Comparison ANN structure:\\n{net_struct[id_one]} vs. {net_struct[id_two]}\u0026#34;, fontsize=20) plt.subplots_adjust(top=0.9, wspace=0.25, hspace=0.35) plt.show() We can find the formula to count the parameter for the dense layer here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":null,"permalink":"/posts/some-exercise-about-statistical-learning/sl-ex6-californiahouseprices-ann/","section":"Posts","summary":"","title":"SL6: California housing dataset – regression with ANN"},{"content":" # data analysis and wrangling import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # import random as rnd # visualization import seaborn as sns import matplotlib.pyplot as plt # %matplotlib inline # machine learning from sklearn.cluster import KMeans from sklearn.cluster import MiniBatchKMeans from IPython.display import Image # to visualize images from tabulate import tabulate # to create tables import os for dirname, _, filenames in os.walk(\u0026#39;/kaggle/input\u0026#39;): for filename in filenames: print(os.path.join(dirname, filename)) /kaggle/input/14cancer/chart.png /kaggle/input/14cancer/14cancer.ytrain.txt /kaggle/input/14cancer/14cancer.xtrain.txt /kaggle/input/14cancer/chart2.png 1. Download the “14-cancer microarray data” from the book website Get information about the dataset in file 14cancer.info and in Chapter 1 (page 5) of the book (Hastie et al., 2009) Some info about 14cancer.xtrain.txt and14cancer.ytrain.txt.\nDNA microarrays measure the expression of genes in a cell\n14-cancer gene expression data set:\n16064 genes 144 training samples 54 test samples One gene per row, one sample per column.\nCancer classes are labelled as follows:\nbreast prostate lung collerectal lymphoma bladder melanoma uterus leukemia renal pancreas ovary meso cns 2. Generate a new Kernel and give it the name: “SL_EX5_HTM_Clustering_Surname” 3. Load the data in Kaggle Data acquisition ## Load the Cancer Microarray dataset (already splitted in train and test) xtrain = pd.read_csv(\u0026#39;/kaggle/input/14cancer/14cancer.xtrain.txt\u0026#39;, sep=\u0026#39;\\s+\u0026#39;,header=None) ytrain = pd.read_csv(\u0026#39;/kaggle/input/14cancer/14cancer.ytrain.txt\u0026#39;,sep=\u0026#39;\\s+\u0026#39;,header=None) Warning: the dataset is already splitted in training set and test set. Data pre-processing #xtrain = xtrain.transpose() # The columns represent the genes, and the rows are the different samples ytrain = ytrain.transpose() # for each sample I have a label (n_samples, n_genes), n_labels = xtrain.shape, np.unique(ytrain).size print(f\u0026#34;#genes: {n_genes}, #samples: {n_samples}, #labels {n_labels}\u0026#34;) #genes: 16063, #samples: 144, #labels 14 Warning: I don\u0026rsquo;t standardize the data before to perform clustering, in order to do not loose the natural properties of my dataset. xtrain 0 1 2 3 4 5 6 7 8 9 ... 16053 16054 16055 16056 16057 16058 16059 16060 16061 16062 0 -73.0 -69.0 -48.0 13.0 -86.0 -147.0 -65.0 -71.0 -32.0 100.0 ... -134.0 352.0 -67.0 121.0 -5.0 -11.0 -21.0 -41.0 -967.0 -120.0 1 -16.0 -63.0 -97.0 -42.0 -91.0 -164.0 -53.0 -77.0 -17.0 122.0 ... -51.0 244.0 -15.0 119.0 -32.0 4.0 -14.0 -28.0 -205.0 -31.0 2 4.0 -45.0 -112.0 -25.0 -85.0 -127.0 56.0 -110.0 81.0 41.0 ... 14.0 163.0 -14.0 7.0 15.0 -8.0 -104.0 -36.0 -245.0 34.0 3 -31.0 -110.0 -20.0 -50.0 -115.0 -113.0 -17.0 -40.0 -17.0 80.0 ... 26.0 625.0 18.0 59.0 -10.0 32.0 -2.0 10.0 -495.0 -37.0 4 -33.0 -39.0 -45.0 14.0 -56.0 -106.0 73.0 -34.0 18.0 64.0 ... -69.0 398.0 38.0 215.0 -2.0 44.0 3.0 68.0 -293.0 -34.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 139 -196.0 -369.0 -263.0 162.0 -277.0 -615.0 -397.0 -243.0 70.0 -167.0 ... -25.0 2674.0 171.0 1499.0 95.0 735.0 -12.0 647.0 -2414.0 -33.0 140 34.0 -81.0 -146.0 -151.0 -174.0 -121.0 -290.0 -106.0 43.0 240.0 ... -32.0 226.0 189.0 310.0 -13.0 210.0 -22.0 622.0 -889.0 -104.0 141 -56.0 -818.0 -1338.0 -57.0 -989.0 -796.0 -1466.0 -347.0 -413.0 103.0 ... -85.0 1827.0 581.0 1547.0 -72.0 999.0 -461.0 564.0 -3567.0 -192.0 142 -245.0 -235.0 -127.0 197.0 -562.0 -714.0 -1621.0 -226.0 -35.0 -243.0 ... -419.0 580.0 233.0 1065.0 -71.0 397.0 -28.0 114.0 -3086.0 -16.0 143 -26.0 -1595.0 -2085.0 -334.0 -455.0 -354.0 -482.0 196.0 114.0 45.0 ... -243.0 526.0 126.0 320.0 -30.0 308.0 -179.0 121.0 -1878.0 -357.0 144 rows × 16063 columns\nClustering Analysis #4. Use thesklearn.cluster module to perform clustering analysis on the dataset. In particular, repeat the analysis proposed in section 14.3.8 of the book (Hastie et al., 2009) Start using K-means and then test some other clustering algorithms at your choice Cluster the samples (i.e., columns). Each sample has a label (tumor type) Do not use the labels in the clustering phase but examine them posthoc to interpret the clusters Run k-means with K from 2 to 10 and compare the clusterings in terms of within-sum of squares Show the chart of the performance depending on K Select some K and analyze the clusters as done in the book K-Means #The KMeans algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares. 1\nThe k-means algorithm divides a set of samples into disjoint clusters, each described by the mean of the samples in the cluster. The means are commonly called the cluster “centroids”; note that they are not, in general, points from, although they live in the same space.\nThe K-means algorithm aims to choose centroids that minimise the inertia, or within-cluster sum-of-squares criterion: \\[\\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2)\\]\n# K-means with k from 2 to 10 n_clusters = range(2,11) alg = \u0026#39;k-means++\u0026#39; # Method for initialization niter = 10 # Number of time the k-means algorithm will be run with different centroid seeds. wc_km_sos=[] # within_cluster_km_sos print(\u0026#39;k\\tInertia\\t\\t\\tdecrease %\u0026#39;) print(50 * \u0026#39;-\u0026#39;) formatter_result = (\u0026#34;{:d}\\t{:f}\\t{:f}\u0026#34;) for k in n_clusters: results = [] results.append(k) km = KMeans(init=alg, n_clusters=k, n_init=niter).fit(xtrain) # inertia = Sum of squared distances of samples to their closest cluster center wcv = km.inertia_ wc_km_sos.append(wcv) results.append(wcv) # variations in % if len(wc_km_sos)\u0026gt;1: results.append( (wc_km_sos[k-2] - wc_km_sos[k-3])*100/wc_km_sos[k-2] ) else: results.append(0) print(formatter_result.format(*results)) k\tInertia\tdecrease % -------------------------------------------------- 2\t865755593329.079102 0.000000 3\t728028390816.590332\t-18.917834 4\t638452947540.124023\t-14.030077 5\t586449466492.984497\t-8.867513 6\t538128754493.843750 -8.979396 7\t516487727616.067871 -4.190037 8\t488855618252.548340 -5.652407 9\t454592949466.325562\t-7.537000 10\t440415484775.366333\t-3.219111 # fig width, height = 8, 4 fig, ax = plt.subplots(figsize=(width,height)) ax.plot(n_clusters, wc_km_sos, marker=\u0026#39;o\u0026#39;, color=\u0026#34;darkblue\u0026#34;) ax.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); #ax.yaxis.set_major_formatter(FormatStrFormatter(\u0026#39;%.f\u0026#39;)) ax.set_xlabel(\u0026#34;Number of Clusters K\u0026#34;, fontsize=12) ax.set_ylabel(\u0026#34;within-cluster sum of squares\u0026#34;, fontsize=12) plt.suptitle(\u0026#34;Total within-cluster sum of squares\\n for K-means clustering\u0026#34;,fontsize=20) plt.subplots_adjust(top=0.825) # change title position plt.show() # We can compare the above chart with the one in the book: scale = 80 Image(\u0026#34;../input/14cancer/chart.png\u0026#34;, width = width*scale, height = height*scale) This plot is taken from \u0026ldquo;The Elements of Statistical Learning\u0026rdquo; book. 2\nComparison between different methods of initialization: k-means++ vs random #n_clusters = range(2,11) niter = 10 # Number of time the k-means algorithm will be run with different centroid seeds. wc_kmpp, wc_rnd = [], [] print(\u0026#39;k\\tK-means\\t\\t\\trandom\u0026#39;) print(60 * \u0026#39;-\u0026#39;) formatter_result = (\u0026#34;{:d}\\t{:f}\\t{:f}\u0026#34;) for k in n_clusters: results = [] results.append(k) kmpp = KMeans(init=\u0026#34;k-means++\u0026#34;, n_clusters=k, n_init=niter).fit(xtrain) rnd = KMeans(init=\u0026#34;random\u0026#34;, n_clusters=k, n_init=niter).fit(xtrain) results.append(kmpp.inertia_) results.append(rnd.inertia_) wc_kmpp.append(kmpp.inertia_) wc_rnd.append(rnd.inertia_) print(formatter_result.format(*results)) k\tK-means\trandom ------------------------------------------------------------ 2\t865755593329.079102\t865755593329.078979 3\t728215342983.054443\t728215342983.054443 4\t638286863470.537109\t638452947540.124146 5\t586098738159.229004\t580943572411.067993 6\t541362331453.668213\t539591832514.661987 7\t501565429046.019531\t500472648214.279541 8\t481877683922.631714\t484882990782.917847 9\t461806611237.345337\t464195618439.327515 10\t448128965453.922974\t454970652718.346436 # fig width, height = 8, 4 fig, ax = plt.subplots(figsize=(width,height)) ax.plot(n_clusters, wc_kmpp, marker=\u0026#39;*\u0026#39;, color=\u0026#34;darkblue\u0026#34;, label = \u0026#34;k-means++\u0026#34;) ax.plot(n_clusters, wc_rnd, marker=\u0026#39;o\u0026#39;, color=\u0026#34;orange\u0026#34;, label = \u0026#34;random\u0026#34;) ax.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); #ax.yaxis.set_major_formatter(FormatStrFormatter(\u0026#39;%.f\u0026#39;)) ax.legend() ax.set_xlabel(\u0026#34;Number of Clusters K\u0026#34;, fontsize=12) ax.set_ylabel(\u0026#34;within-cluster sum of squares\u0026#34;, fontsize=12) plt.suptitle(\u0026#34;Comparison with different method of initialization\u0026#34;,fontsize=20) plt.subplots_adjust(top=0.825) # change title position plt.show() Comparison between different n_iter: #Number of time the k-means algorithm will be run with different centroid seeds.\nn_clusters = range(2,11) wc_ten_seeds, wc_twenty_seeds = [], [] print(\u0026#39;k\\tn_iter=10\\t\\tn_iter=20\u0026#39;) print(70 * \u0026#39;-\u0026#39;) formatter_result = (\u0026#34;{:d}\\t{:f}\\t{:f}\u0026#34;) for k in n_clusters: results = [] results.append(k) ten_seeds = KMeans(init=\u0026#34;k-means++\u0026#34;, n_clusters=k, n_init=10).fit(xtrain) twenty_seeds = KMeans(init=\u0026#34;k-means++\u0026#34;, n_clusters=k, n_init=20).fit(xtrain) results.append(ten_seeds.inertia_) results.append(twenty_seeds.inertia_) wc_ten_seeds.append(ten_seeds.inertia_) wc_twenty_seeds.append(twenty_seeds.inertia_) print(formatter_result.format(*results)) k\tn_iter=10\tn_iter=20 ---------------------------------------------------------------------- 2\t866070488704.476074\t865755593329.079102 3\t728028390816.590210\t727972625271.491211 4\t639723868185.660278 638286863470.537109 5\t579977474766.300903 580224574540.047607 6\t543140308602.200195 537625894944.809998 7\t499824352123.143555 499900728332.191284 8\t481177796841.305420 478729684111.517700 9\t463786737203.969238 455823165084.713989 10\t447920765947.759399 440614709199.603394 # fig width, height = 8, 4 fig, ax = plt.subplots(figsize=(width,height)) ax.plot(n_clusters, wc_ten_seeds, marker=\u0026#39;*\u0026#39;, color=\u0026#34;darkblue\u0026#34;, label = \u0026#34;n_iter=10\u0026#34;) ax.plot(n_clusters, wc_twenty_seeds, marker=\u0026#39;o\u0026#39;, color=\u0026#34;orange\u0026#34;, label = \u0026#34;n_iter=20\u0026#34;) ax.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); #ax.yaxis.set_major_formatter(FormatStrFormatter(\u0026#39;%.f\u0026#39;)) ax.legend() ax.set_xlabel(\u0026#34;Number of Clusters K\u0026#34;, fontsize=12) ax.set_ylabel(\u0026#34;within-cluster sum of squares\u0026#34;, fontsize=12) plt.suptitle(\u0026#34;Comparison between different n_iter\u0026#34;,fontsize=20) plt.subplots_adjust(top=0.825) # change title position plt.show() Mini-batch K-means #The MiniBatchKMeans is a variant of the KMeans algorithm which uses mini-batches to reduce the computation time, while still attempting to optimise the same objective function.\nMini-batches are subsets of the input data, randomly sampled in each training iteration. These mini-batches drastically reduce the amount of computation required to converge to a local solution.\nIn contrast to other algorithms that reduce the convergence time of k-means, mini-batch k-means produces results that are generally only slightly worse than the standard algorithm. 1\n# K-means with k from 2 to 10 n_clusters = range(2,11) alg = \u0026#39;k-means++\u0026#39; # Method for initialization niter = 10 # Number of time the k-means algorithm will be run with different centroid seeds. wc_mbkm_sos=[] print(\u0026#39;k\\tInertia\\t\\t\\tdecrease %\u0026#39;) print(50 * \u0026#39;-\u0026#39;) formatter_result = (\u0026#34;{:d}\\t{:f}\\t{:f}\u0026#34;) for k in n_clusters: results = [] results.append(k) mbkm = MiniBatchKMeans(init=alg, n_clusters=k, n_init=niter).fit(xtrain) # inertia = Sum of squared distances of samples to their closest cluster center wcv = mbkm.inertia_ wc_mbkm_sos.append(wcv) results.append(wcv) # variations in % if len(wc_mbkm_sos)\u0026gt;1: results.append( (wc_mbkm_sos[k-2] - wc_mbkm_sos[k-3])*100/wc_mbkm_sos[k-2] ) else: results.append(0) print(formatter_result.format(*results)) k\tInertia\tdecrease % -------------------------------------------------- 2\t870435631688.268311\t0.000000 3\t728979505913.590698\t-19.404678 4\t644499761950.796875\t-13.107801 5\t651489332987.863525\t1.072860 6\t554034243741.206177\t-17.590084 7\t547550479471.734985\t-1.184140 8\t526030833538.899902\t-4.090948 9\t489328464969.691284\t-7.500559 10\t452845818672.533325\t-8.056306 # fig width, height = 8, 4 fig, ax = plt.subplots(figsize=(width,height)) ax.plot(n_clusters, wc_mbkm_sos, marker=\u0026#39;o\u0026#39;, color=\u0026#34;darkblue\u0026#34;, label = \u0026#34;k-means\u0026#34;) ax.plot(n_clusters, wc_km_sos, marker=\u0026#39;o\u0026#39;, color=\u0026#34;orange\u0026#34;, label = \u0026#34;Mini batch k-means\u0026#34;) ax.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); #ax.yaxis.set_major_formatter(FormatStrFormatter(\u0026#39;%.f\u0026#39;)) ax.legend() ax.set_xlabel(\u0026#34;Number of Clusters K\u0026#34;, fontsize=12) ax.set_ylabel(\u0026#34;within-cluster sum of squares\u0026#34;, fontsize=12) plt.suptitle(\u0026#34;Total within-cluster sum of squares\\ncomparison\u0026#34;,fontsize=20) plt.subplots_adjust(top=0.825) # change title position plt.show() Analysis for K=3 #Number of cancer cases of each type in each of the 3 clusters #rows = KMeans(init=\u0026#34;k-means++\u0026#34;, n_clusters=3).fit(xtrain).labels_ # labels of each sample after clustering columns = ytrain.to_numpy().flatten() # make the df into an iterable list # Collect info in a table tab = np.zeros(3*n_labels).reshape(3,n_labels) # rows: clusters, columns: cancer labels # Update table for i in range(n_samples): tab[rows[i],columns[i]-1]+=1 # column-1 because we range over 14 clusters (0,13) # Better formatting of the table into a DataFrame table = pd.DataFrame(tab.astype(int)) table.columns = [\u0026#34;breast\u0026#34;, \u0026#34;prostate\u0026#34;, \u0026#34;lung\u0026#34;, \u0026#34;collerectal\u0026#34;, \u0026#34;lymphoma\u0026#34;, \u0026#34;bladder\u0026#34;, \u0026#34;melanoma\u0026#34;, \u0026#34;uterus\u0026#34;, \u0026#34;leukemia\u0026#34;, \u0026#34;renal\u0026#34;, \u0026#34;pancreas\u0026#34;, \u0026#34;ovary\u0026#34;, \u0026#34;meso\u0026#34;, \u0026#34;cns\u0026#34;] table breast prostate lung collerectal lymphoma bladder melanoma uterus leukemia renal pancreas ovary meso cns 0 0 2 0 0 2 0 0 2 21 1 0 2 0 0 1 8 3 6 5 1 7 5 3 0 5 6 4 4 3 2 0 3 2 3 13 1 3 3 3 2 2 2 4 13 More info about K-Means method and other clustering methods can be found here\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe above chart can be found in the section 14.3.8 of the book The Elements of Statistical Learning: Data Mining, Inference, and Prediction.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":null,"permalink":"/posts/some-exercise-about-statistical-learning/sl-ex5-humantumormicro-clustering/","section":"Posts","summary":"","title":"SL5: Human Tumor Microarray dataset - clustering with k-means"},{"content":" # data analysis and wrangling import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # import random as rnd # visualization import seaborn as sns import matplotlib.pyplot as plt # %matplotlib inline # machine learning from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge from sklearn.linear_model import RidgeCV from sklearn.linear_model import Lasso from sklearn.linear_model import lasso_path from sklearn.linear_model import LassoCV from sklearn.metrics import r2_score from sklearn.metrics import mean_squared_error import statsmodels.api as sm from IPython.display import Image # to visualize images from tabulate import tabulate # to create tables import os for dirname, _, filenames in os.walk(\u0026#39;/kaggle/input\u0026#39;): for filename in filenames: print(os.path.join(dirname, filename)) /kaggle/input/prostate-data/tab4.png /kaggle/input/prostate-data/tab4-no-cap.png /kaggle/input/prostate-data/tab3.png /kaggle/input/prostate-data/prostate.data /kaggle/input/prostate-data/tab.png /kaggle/input/prostate-data/tab2.png 1. Open your kernel SL_EX2_ProstateCancer_Surname in Kaggle 2. Generate a copy called SL_EX4_Prostate_Shrinkage_Surname by the Fork button 3. Import Ridge from sklearn.linear_model Data acquisition ## Load the Prostate Cancer dataset data = pd.read_csv(\u0026#39;../input/prostate-data/prostate.data\u0026#39;,sep=\u0026#39;\\t\u0026#39;) # Save \u0026#34;train\u0026#34; and lpsa\u0026#34; columns into Pandas Series variables train = data[\u0026#39;train\u0026#39;] lpsa = data[\u0026#39;lpsa\u0026#39;] # Drop \u0026#34;train\u0026#34; and lpsa\u0026#34; variable from data data = data.drop(columns=[\u0026#39;Unnamed: 0\u0026#39;,\u0026#39;lpsa\u0026#39;,\u0026#39;train\u0026#39;],axis=1) Data pre-processing ### X VARIABLE: # Split the data in train and test sets dataTrain = data.loc[train == \u0026#39;T\u0026#39;] # Obviously, len(idx)==len(dataTrain) is True! dataTest = data.loc[train == \u0026#39;F\u0026#39;] # Rename these two variables as \u0026#34;predictorsTrain\u0026#34; and \u0026#34;predictorsTest\u0026#34; predictorsTrain = dataTrain predictorsTest = dataTest ## Y VARIABLE: # Split the \u0026#34;lpsa\u0026#34; in train and test sets lpsaTrain = lpsa.loc[train == \u0026#39;T\u0026#39;] lpsaTest = lpsa.loc[train == \u0026#39;F\u0026#39;] Standardization ## Standardize \u0026#34;predictorsTrain\u0026#34; predictorsTrainMeans = predictorsTrain.mean() predictorsTrainStds = predictorsTrain.std() predictorsTrain_std = (predictorsTrain - predictorsTrainMeans)/predictorsTrainStds # standardized variables of predictorTrain # Standardize \u0026#34;predictorsTest\u0026#34; (using the mean and std of predictorsTrain, it\u0026#39;s better!) predictorsTest_std = (predictorsTest - predictorsTrainMeans)/predictorsTrainStds # standardized variables of predictorTest # Standardize all the data together (necessary for CV) predictors = data predictors_std = (predictors - predictors.mean())/predictors.std() Split into Training and Test sets ### TRAINING SET X_train = predictorsTrain_std Y_train = lpsaTrain ## TEST SET X_test = predictorsTest_std Y_test = lpsaTest ## All the data standardized X = predictors_std Y = lpsa Ridge regression #4. Starting from standardized data, generate a ridge regression model with \\(\\alpha\\)=1.0 (called \\(\\lambda\\) in the slides) ## Ridge Regression model trained on training set ridge = Ridge(alpha=1, fit_intercept=True) ridge.fit(X_train,Y_train); # if alpha=0, then Ridge coincides with OLS 5. Show model’s coefficients, intercept and score (on both the training and the test set) ## For Ridge model # Stats on train R2_train_ridge = ridge.score(X_train,Y_train) # Stats on test R2_test_ridge = ridge.score(X_test,Y_test) round_coef = 3 print(f\u0026#34;\\ About the the Ridge regression model for alpha = 1:\\n\\ * intercept: {ridge.intercept_.round(round_coef)}\\n\\ * model coefficients:: {ridge.coef_.round(round_coef).tolist()}\\n\\ * R^2 on training set: {R2_train_ridge.round(round_coef)}\\n\\ * R^2 on test set: {R2_test_ridge.round(round_coef)}\\n\\ \u0026#34;) About the the Ridge regression model for alpha = 1: * intercept: 2.452 * model coefficients:: [0.69, 0.292, -0.135, 0.21, 0.304, -0.256, -0.011, 0.258] * R^2 on training set: 0.694 * R^2 on test set: 0.512 6. Plot Ridge coefficients as functions of the regularization parameter alpha # create a vector of alphas alphas = np.logspace(start = -1, stop = 4, num = 200, base = 10) # regularization params [base**start,base**end] coefs_r = [] # lists to store model coeffs for a in alphas: # Compute the Ridge model reg = Ridge(alpha=a, fit_intercept=True) reg.fit(X_train,Y_train) # Store coefficient coefs_r.append(reg.coef_) # fig width, height = 8, 8 fig, ax = plt.subplots(figsize=(width,height)) ax.plot(alphas, coefs_r) ax.axhline(0, linestyle=\u0026#34;--\u0026#34;, linewidth=1.25, color=\u0026#34;black\u0026#34;) # horizontal line ax.set_xscale(\u0026#39;log\u0026#39;) # ax.set_xlim(ax.get_xlim()[::-1]) # reverse the x-axis ax.set_xlabel(r\u0026#34;$\\alpha $\u0026#34;, fontsize=15) ax.set_ylabel(\u0026#34;Coefficients\u0026#34;, fontsize=15) ax.set_title(r\u0026#34;Ridge coefficients as a function of $\\alpha$\u0026#34;, fontsize=20) ax.legend(X_train.columns.tolist()) plt.show() Ridge coefficients as functions of the effective DoF #The effective degree of freedom of the Ridge regression is \\[\\text{df} (\\alpha ) = \\sum_{i=1}^{n} \\dfrac{d_i^2}{d_i^2+\\alpha}\\] where \\(d_i\\) are singular values of \\(X\\).\nIn a linear-regression fit with p variables, the degrees-of-freedom of the fit is \\(df(\\alpha)=p\\), the number of free parameters.\nThe idea is that although all p coefficients in a ridge fit will be non-zero, they are fit in a restricted fashion controlled by \\(\\alpha\\). Note that when we have no regularization \\(df(0)=p\\) .\n# Compute effective DoF df_r = [] s2 = np.linalg.svd(X_train, compute_uv=False)**2 # singular values squared for a in alphas: tmp = s2/(s2+a) df_r.append(tmp.sum()) # fig width, height = 8, 8 palette = \u0026#39;darkblue\u0026#39; fig, ax = plt.subplots(figsize=(width,height)) ax.plot(df_r, coefs_r, color=palette) #ax.axvline(5, linestyle=\u0026#34;:\u0026#34;, linewidth=1.25, color=\u0026#34;red\u0026#34;) # vertical line ax.axhline(0, linestyle=\u0026#34;--\u0026#34;, linewidth=1.25, color=\u0026#34;black\u0026#34;) # horizontal line ax.set_xlabel(r\u0026#34;df($\\alpha $)\u0026#34;, fontsize=15) ax.set_ylabel(\u0026#34;Coefficients\u0026#34;, fontsize=15) ax.set_title(\u0026#34;Ridge coefficients as functions of the effective DoF\u0026#34;, fontsize=20) ax.set_xlim(-0.5, 9) # Annotate the name of each variable at the last value coords = zip([8]*len(coefs_r[0]),coefs_r[0]) # last value where I want to annotate the corresponding label labels = X_train.columns.tolist() for coord,lab in zip(coords,labels): ax.annotate(xy=coord, # The point (x, y) to annotate. xytext=coord, # The position (x, y) to place the text at. textcoords=\u0026#39;offset points\u0026#39;, text=lab, verticalalignment=\u0026#39;center\u0026#39;) plt.show() # We can compare the above chart with the one in the book (figure 3.8): scale = 70 Image(\u0026#34;../input/prostate-data/tab4-no-cap.png\u0026#34;, width = width*scale, height = height*scale) This plot is taken from \u0026ldquo;The Elements of Statistical Learning\u0026rdquo; book. 1\nRidge regression LOOCV #7. Find the best regularization parameter by cross-validation using leave-one-out Use only the training set (97 observations). Use store_cv_values=True to get the leave-one-out errors. The cv_values_ in the result object provided by RidgeCV contains one row for each observation (97 rows) and one column for each alpha in alphas (100 columns). Cell i,j contains the leave-one-out mean squared error on the i-th observation given the j-th alpha.\nBy computing the mean by column of cv_values_ you obtain the average cross-validation error for each alpha. This is the value that has to be minimized (see next step). # Create and train the model using all the data alphas = np.logspace(start=-1,stop=4,num=200,base=10.0) # cv = None, to use the Leave-One-Out cross-validation ridgeLOO = RidgeCV(alphas = alphas, cv=None, fit_intercept = True, store_cv_values = True) ridgeLOO.fit(X,Y) cvVal = ridgeLOO.cv_values_ # cvVal[i,j] = LOO MSE on the i-th observation given the j-th alpha cvMeans = np.mean(cvVal,axis=0) cvStd = np.std(cvVal,axis=0) print(\u0026#34;The shape is: \u0026#34;,np.shape(cvVal)) # contains one row for each observation and one column for each alpha in alphas The shape is: (97, 200) ## Best regularization parameter: identify the minimum LOOCV MSE and the related alpha min_val = min(cvMeans) # min_val = abs(ridgeLOO.best_score_) # alternative way min_alpha = ridgeLOO.alpha_ # min_alpha = ridgeLOO.alphas[np.argmin(cvMeans)] # alternative way print(f\u0026#34;\\ The best regularization parameter is \\u03B1={min_alpha.round(3)}, with corresponding LOOCV MSE equal to {min_val.round(3)}.\\ \u0026#34;) The best regularization parameter is α=6.08, with corresponding LOOCV MSE equal to 0.536. 9. Identify the minimum leave-one-out cross-validation error and the related alpha, model coefficients and performance ## Perform the Ridge regression on the best alpha ridgeLOO_best = RidgeCV(alphas = min_alpha, fit_intercept = True) ridgeLOO_best.fit(X,Y) # Stats on train R2_train_ridgeLOO_best = ridgeLOO_best.score(X_train,Y_train) # Stats on test R2_test_ridgeLOO_best = ridgeLOO_best.score(X_test,Y_test) # print some info about Ridge CV round_coef = 3 print(f\u0026#34;\\ For this value of \\u03B1={min_alpha.round(round_coef)} we have the following parameters:\\n\\ * intercept: {ridgeLOO_best.intercept_.round(round_coef)}\\n\\ * model coefficients: {ridgeLOO_best.coef_.round(round_coef).tolist()}\\n\\ * R^2 on training set: {R2_train_ridgeLOO_best.round(round_coef)}\\n\\ * R^2 on test set: {R2_test_ridgeLOO_best.round(round_coef)}\\n\\ \u0026#34;) For this value of α=6.08 we have the following parameters: * intercept: 2.478 * model coefficients: [0.59, 0.26, -0.128, 0.126, 0.287, -0.065, 0.045, 0.099] * R^2 on training set: 0.67 * R^2 on test set: 0.609 8. Plot the leave-one-out cross-validation curve Hint: plot the alphas vector against the average cross-validation error for each alpha computed at the last point # fig width, height = 8, 8 palette = \u0026#39;darkblue\u0026#39; fig, ax = plt.subplots(figsize=(width,height)) # straight lines ax.plot(ridgeLOO.alphas, cvMeans, color=\u0026#39;darkblue\u0026#39;) ax.plot(min_alpha, min_val, marker= \u0026#39;o\u0026#39;, markersize = 8, color=\u0026#39;r\u0026#39;) ax.axvline(min_alpha, linestyle=\u0026#34;:\u0026#34;, linewidth=1.5, color=\u0026#34;red\u0026#34;) # vertical line ax.set_xlabel(r\u0026#34;$\\alpha$\u0026#34;, fontsize=15) ax.set_ylabel(\u0026#34;cv MSE\u0026#34;, fontsize=20) ax.set_title(fr\u0026#34;LOOCV curve ($\\alpha$: {ridgeLOO.alpha_.round(3)})\u0026#34;, fontsize=20) ax.set_xscale(\u0026#39;log\u0026#39;) ax.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); # ax.set_xlim(ax.get_xlim()[::-1]) # reverse the x-axis plt.show() Ridge regression with 10-Folds Cross Validation #10. Identify the minimum 10-folds cross-validation error and the related alpha, model coefficients and performance # store_cv_values = False otherwise it is incompatible with cv!=none ridge_10cv = RidgeCV(alphas = alphas, fit_intercept = True, store_cv_values = False, cv = 10); ridge_10cv.fit(X,Y); # Best regularization parameter: identify the minimum LOO CV error and the related alpha min_alpha_10cv = ridge_10cv.alpha_ min_val_10cv = abs(ridge_10cv.best_score_) # loocv of 10cv print(f\u0026#34;\\ The best regularization parameters is \\u03B1={min_alpha_10cv.round(3)}, with corresponding Leave-One-Out MSE equal to: {min_val_10cv.round(3)}\\n\\ \u0026#34;) The best regularization parameters is α=207.292, with corresponding Leave-One-Out MSE equal to: 52.447 ## Perform the Ridge regression with 10 folds on the best alpha (in order to perform on train and test sets) ridge_10cv_best = RidgeCV(alphas = min_alpha_10cv, fit_intercept = True) ridge_10cv_best.fit(X,Y) # Stats on train R2_train_ridge_10cv_best = ridge_10cv_best.score(X_train,Y_train) # Stats on test R2_test_ridge_10cv_best = ridge_10cv_best.score(X_test,Y_test) # print some info about Ridge CV round_coef = 3 print(f\u0026#34;\\ For this value of \\u03B1={min_alpha_10cv.round(round_coef)} we have the following parameters:\\n\\ * intercept: {ridge_10cv_best.intercept_.round(round_coef)}\\n\\ * model coefficients: {ridge_10cv_best.coef_.round(round_coef).tolist()}\\n\\ * R^2 on training set: {R2_train_ridge_10cv_best.round(round_coef)}\\n\\ * R^2 on test set: {R2_test_ridge_10cv_best.round(round_coef)}\\n\\ \u0026#34;) For this value of α=207.292 we have the following parameters: * intercept: 2.478 * model coefficients: [0.195, 0.118, 0.01, 0.047, 0.131, 0.101, 0.058, 0.068] * R^2 on training set: 0.468 * R^2 on test set: 0.459 Comparison between Ridge LOO CV and Ridge 10 Folds CV ## table round_coef = 3 headers = [\u0026#39;Term\u0026#39;,\u0026#39;Ridge LOOCV\u0026#39;, \u0026#39;Ridge 10-CV\u0026#39;] # labels (0-th column) intercept_label = np.array([\u0026#39;Intercept\u0026#39;]) alpha_label = np.array([\u0026#39;Best Alpha\u0026#39;]) coefs_label = X_train.columns.tolist() labels = np.concatenate((intercept_label,alpha_label,coefs_label),axis=0) # ridge LOOCV column values model = ridgeLOO_best model_intercept_val = np.array([model.intercept_]) model_alpha_val = np.array([model.alpha_]) model_coefs_val = model.coef_ ridgeLOO_values = np.concatenate((model_intercept_val,model_alpha_val,model_coefs_val), axis=0).round(round_coef) # ridge 10CV column values model = ridge_10cv_best model_intercept_val = np.array([model.intercept_]) model_alpha_val = np.array([model.alpha_]) model_coefs_val = model.coef_ ridge_10cv_values = np.concatenate((model_intercept_val,model_alpha_val,model_coefs_val), axis=0).round(round_coef) table = np.column_stack((labels, ridgeLOO_values, ridge_10cv_values)) print(tabulate(table, headers=headers, tablefmt=\u0026#39;fancy_grid\u0026#39;)) ╒════════════╤═══════════════╤═══════════════╕ │ Term │ Ridge LOOCV │ Ridge 10-CV │ ╞════════════╪═══════════════╪═══════════════╡ │ Intercept │ 2.478 │ 2.478 │ ├────────────┼───────────────┼───────────────┤ │ Best Alpha │ 6.08 │ 207.292 │ ├────────────┼───────────────┼───────────────┤ │ lcavol │ 0.59 │ 0.195 │ ├────────────┼───────────────┼───────────────┤ │ lweight │ 0.26 │ 0.118 │ ├────────────┼───────────────┼───────────────┤ │ age │ -0.128 │ 0.01 │ ├────────────┼───────────────┼───────────────┤ │ lbph │ 0.126 │ 0.047 │ ├────────────┼───────────────┼───────────────┤ │ svi │ 0.287 │ 0.131 │ ├────────────┼───────────────┼───────────────┤ │ lcp │ -0.065 │ 0.101 │ ├────────────┼───────────────┼───────────────┤ │ gleason │ 0.045 │ 0.058 │ ├────────────┼───────────────┼───────────────┤ │ pgg45 │ 0.099 │ 0.068 │ ╘════════════╧═══════════════╧═══════════════╛ Lasso regression #11. Import Lasso from sklearn.linear_model ✅ 12. Generate a lasso regression model with a specific regularization parameter alpha=0.1 # Create a Lasso regression model lasso = Lasso(alpha = 0.1) lasso.fit(X_train,Y_train); 14 Show model’s coefficients, intercept and score (on both the training and the test set) # Stats on train R2_train_lasso = lasso.score(X_train,Y_train) # Stats on test R2_test_lasso = lasso.score(X_test,Y_test) round_coef = 3 print(f\u0026#34;\\ About the the Lasso regression model:\\n\\ * intercept: {lasso.intercept_.round(round_coef)}\\n\\ * model coefficients: {lasso.coef_.round(round_coef).tolist()}\\n\\ * R^2 on training set: {R2_train_lasso.round(round_coef)}\\n\\ * R^2 on test set: {R2_test_lasso.round(round_coef)}\\n\\ \u0026#34;) About the the Lasso regression model: * intercept: 2.452 * model coefficients: [0.575, 0.23, -0.0, 0.105, 0.172, 0.0, 0.0, 0.065] * R^2 on training set: 0.648 * R^2 on test set: 0.569 15. Plot Lasso coefficients as a function of the regularization parameter # Lasso path: coefficients as functions of the regularization parameters # we can set either alphas or eps: the smaller eps, the longer the path is alphas_lasso, coefs_lasso, _ = lasso_path(X_train, Y_train, eps = 5e-3) # For each coefficient I compute the first index in which it becomes zero. nz_list = list((coefs_lasso !=0).argmax(axis=1)-1) # fig width, height = 8, 8 fig, ax = plt.subplots(figsize=(width,height)) xx = -np.log10(alphas_lasso) # neg_log_alphas_lasso for coef in coefs_lasso: ax.plot(xx, coef) # Info about when coefs are shrunken to 0 for nz in nz_list: ax.axvline(xx[nz], linestyle=\u0026#34;:\u0026#34;, linewidth=0.5, color=\u0026#34;grey\u0026#34;) # vertical line # dots where coefs become zero for coef in coefs_lasso: ax.plot(xx[nz], coef[nz], linestyle=\u0026#39;None\u0026#39;, marker = \u0026#39;o\u0026#39;, markersize=3.5, color = \u0026#39;grey\u0026#39;) ax.axhline(y=0, linestyle=\u0026#39;--\u0026#39;,linewidth=0.75, color=\u0026#39;black\u0026#39;) # horizontal line #ax.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); ax.set_xlabel(r\u0026#34;-log($\\alpha$)\u0026#34;, fontsize=15) ax.set_ylabel(\u0026#34;Coefficients\u0026#34;, fontsize=15) ax.set_title(\u0026#34;Lasso coefficients\u0026#34;, fontsize=20) ## Annotate the name of each variable at the last value ax.set_xlim(-0.05, ax.get_xlim()[1]*1.1) # enlarge xaxis alpha_end = xx[-1]*1.015 # last value of alpha last_val = [last for *_, last in coefs_lasso] # last value of all the 8 coeffs coords = zip([alpha_end]*8,last_val) labels = X_train.columns.tolist() for coord,lab in zip(coords,labels): ax.annotate(xy=coord, # The point (x, y) to annotate. xytext=coord, # The position (x, y) to place the text at. textcoords=\u0026#39;offset points\u0026#39;, text=lab, verticalalignment=\u0026#39;center\u0026#39;) plt.show() 16. Find the best regularization parameter by cross-validation Hint: use function LassoCV from sklearn Test different number of cv folds 17. Plot the mean square error curve for each fold and for the average of all the folds ## Some params cvFolds = np.arange(start = 5, stop = 25, step = 5) alpha_lasso_cv = np.logspace(-4,1,100,10) # where we store some info best_alpha, performances_lasso_cv = [], [] intercepts_lasso_cv, model_coefs = [], [] # Initialize the fig nrow, ncol = 2, 2 width, height = 16, 12 # single subplot fig, axes = plt.subplots(nrow,ncol,figsize=(width,height)) for folds, ax in zip(cvFolds,axes.flatten()): lasso_cv = LassoCV(cv=folds, alphas=alpha_lasso_cv) lasso_cv.fit(X_train,Y_train) # fitting the model using n-folds # R^2 train pred_train_lasso_cv = lasso_cv.predict(X_train) Rsquared_train_lasso_cv = lasso_cv.score(X_train,Y_train) # R^2 test pred_test_lasso_cv = lasso_cv.predict(X_test) Rsquared_test_lasso_cv = lasso_cv.score(X_test,Y_test) #store some info best_alpha.append(lasso_cv.alpha_) model_coefs.append(lasso_cv.coef_) intercepts_lasso_cv.append(lasso_cv.intercept_) performances_lasso_cv.append([Rsquared_train_lasso_cv,Rsquared_test_lasso_cv]) m_log_alphas = -np.log10(lasso_cv.alphas_) ax.plot(m_log_alphas, lasso_cv.mse_path_, linestyle=\u0026#34;--\u0026#34;, linewidth=0.5) ax.plot(m_log_alphas, lasso_cv.mse_path_.mean(axis=1), color=\u0026#34;black\u0026#34;, label=\u0026#34;Average across the folds\u0026#34;, linewidth=2.5) ax.axvline(-np.log10(lasso_cv.alpha_), linestyle=\u0026#34;:\u0026#34;, color=\u0026#34;black\u0026#34;, label=\u0026#34;alpha: CV estimate\u0026#34;) # best alpha #ax.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); ax.set_xlabel(r\u0026#34;$-log(\\alpha)$\u0026#34;, fontsize=15) ax.set_ylabel(\u0026#34;MSE\u0026#34;, fontsize=12) ax.set_title(f\u0026#34;{folds}-folds (best \\u03B1: {lasso_cv.alpha_.round(4)})\u0026#34;, fontsize=20) ax.legend(loc=\u0026#39;upper right\u0026#39;) plt.axis(\u0026#34;tight\u0026#34;) # set the spacing between subplots plt.subplots_adjust(wspace=0.3, hspace=0.3) plt.suptitle(\u0026#34;MSE curve for each fold and for the average of all the folds\u0026#34;,fontsize=25) plt.show() # Best regularization parameter by cross-validation print(\u0026#34;Best regularization parameter \\u03B1:\u0026#34;) for nfolds, alpha in zip(cvFolds,best_alpha): print(f\u0026#34; * For {nfolds}-folds: {alpha.round(4)}\u0026#34;) Best regularization parameter α: * For 5-folds: 0.0001 * For 10-folds: 0.0029 * For 15-folds: 0.0074 * For 20-folds: 0.0093 18. Show the best alpha, model coefficients and performance on training and test set ## For Lasso_cv model round_coef = 3 print(\u0026#34;Some stats about the performance of Lasso CV with different folds:\\n\u0026#34;) for k in range(0,len(cvFolds)): print(f\u0026#34;* For {cvFolds[k]}-folds:\\n\\ - best \\u03B1: {best_alpha[k].round(round_coef)}\\n\\ - intercept: {intercepts_lasso_cv[k].round(round_coef)}\\n\\ - coefficients: {model_coefs[k].round(round_coef).tolist()}\\n\\ - score on training set: {performances_lasso_cv[k][0].round(round_coef)}\\n\\ - score on test set: {performances_lasso_cv[k][1].round(round_coef)}\\n\\ \u0026#34;) Some stats about the performance of Lasso CV with different folds: * For 5-folds: - best α: 0.0 - intercept: 2.452 - coefficients: [0.716, 0.293, -0.142, 0.212, 0.309, -0.288, -0.02, 0.277] - score on training set: 0.694 - score on test set: 0.504 * For 10-folds: - best α: 0.003 - intercept: 2.452 - coefficients: [0.706, 0.292, -0.137, 0.209, 0.304, -0.27, -0.008, 0.258] - score on training set: 0.694 - score on test set: 0.511 * For 15-folds: - best α: 0.007 - intercept: 2.452 - coefficients: [0.692, 0.289, -0.127, 0.204, 0.295, -0.241, -0.0, 0.237] - score on training set: 0.694 - score on test set: 0.521 * For 20-folds: - best α: 0.009 - intercept: 2.452 - coefficients: [0.687, 0.287, -0.123, 0.202, 0.29, -0.228, -0.0, 0.23] - score on training set: 0.693 - score on test set: 0.524 Comparison between Lasso with different CV folds ## table round_coef = 3 headers = [\u0026#39;Term\u0026#39;,\u0026#39;Lasso CV\\n(5-folds)\u0026#39;, \u0026#39;Lasso CV\\n(10-folds)\u0026#39;, \u0026#39;Lasso CV\\n(15-folds)\u0026#39;, \u0026#39;Lasso CV\\n(20-folds)\u0026#39;] lasso_cv_values = [] # labels (0-th column) intercept_label = np.array([\u0026#39;Intercept\u0026#39;]) alpha_label = np.array([\u0026#39;Best Alpha\u0026#39;]) coefs_label = X_train.columns.tolist() labels = np.concatenate((intercept_label,alpha_label,coefs_label),axis=0) for idx in range(0,4): model_intercept_val = np.array([intercepts_lasso_cv[idx]]) model_alpha_val = np.array([best_alpha[idx]]) model_coefs_val = model_coefs[idx] tmp = np.concatenate((model_intercept_val,model_alpha_val,model_coefs_val), axis=0).round(round_coef) lasso_cv_values.append(tmp) # table = np.column_stack((labels,lasso_cv_values)) table = np.column_stack((labels, lasso_cv_values[0], lasso_cv_values[1], lasso_cv_values[2], lasso_cv_values[3])) print(tabulate(table, headers=headers, tablefmt=\u0026#39;fancy_grid\u0026#39;)) ╒════════════╤═════════════╤══════════════╤══════════════╤══════════════╕ │ Term │ Lasso CV │ Lasso CV │ Lasso CV │ Lasso CV │ │ │ (5-folds) │ (10-folds) │ (15-folds) │ (20-folds) │ ╞════════════╪═════════════╪══════════════╪══════════════╪══════════════╡ │ Intercept │ 2.452 │ 2.452 │ 2.452 │ 2.452 │ ├────────────┼─────────────┼──────────────┼──────────────┼──────────────┤ │ Best Alpha │ 0 │ 0.003 │ 0.007 │ 0.009 │ ├────────────┼─────────────┼──────────────┼──────────────┼──────────────┤ │ lcavol │ 0.716 │ 0.706 │ 0.692 │ 0.687 │ ├────────────┼─────────────┼──────────────┼──────────────┼──────────────┤ │ lweight │ 0.293 │ 0.292 │ 0.289 │ 0.287 │ ├────────────┼─────────────┼──────────────┼──────────────┼──────────────┤ │ age │ -0.142 │ -0.137 │ -0.127 │ -0.123 │ ├────────────┼─────────────┼──────────────┼──────────────┼──────────────┤ │ lbph │ 0.212 │ 0.209 │ 0.204 │ 0.202 │ ├────────────┼─────────────┼──────────────┼──────────────┼──────────────┤ │ svi │ 0.309 │ 0.304 │ 0.295 │ 0.29 │ ├────────────┼─────────────┼──────────────┼──────────────┼──────────────┤ │ lcp │ -0.288 │ -0.27 │ -0.241 │ -0.228 │ ├────────────┼─────────────┼──────────────┼──────────────┼──────────────┤ │ gleason │ -0.02 │ -0.008 │ -0 │ -0 │ ├────────────┼─────────────┼──────────────┼──────────────┼──────────────┤ │ pgg45 │ 0.277 │ 0.258 │ 0.237 │ 0.23 │ ╘════════════╧═════════════╧══════════════╧══════════════╧══════════════╛ Results and comparisons #OLS ## For OLS model ## Create an OLS model fitted on training set ols = LinearRegression(fit_intercept=True); ols.fit(X_train,Y_train); # Stats on train R2_train_ols = ols.score(X_train,Y_train) # Stats on test R2_test_ols = ols.score(X_test,Y_test) round_coef = 3 print(f\u0026#34;About the the OLS regression model:\\n\\ * coefficients: {ols.coef_.round(round_coef).tolist()}\\n\\ * intercept: {ols.intercept_.round(round_coef).round(round_coef)}\\n\\ * R^2 on training set: {R2_train_ols.round(round_coef)}\\n\\ * R^2 on test set: {R2_test_ols.round(round_coef)}\\n\\ \u0026#34;) About the the OLS regression model: * coefficients: [0.716, 0.293, -0.143, 0.212, 0.31, -0.289, -0.021, 0.277] * intercept: 2.452 * R^2 on training set: 0.694 * R^2 on test set: 0.503 Best Subset Selection ## For Best Subset model - RSS - Andrea predLabels = [\u0026#39;lcavol\u0026#39;,\u0026#39;svi\u0026#39;,\u0026#39;gleason\u0026#39;] X1, y1 = X_train[predLabels], Y_train X2, y2 = X_test[predLabels], Y_test # Create the linear model, fitting also the intecept (non-zero) bssA = LinearRegression(fit_intercept = True).fit(X1,y1) # Stats on train R2_train_bssA = bssA.score(X1,y1) # Stats on test R2_test_bssA = bssA.score(X2,y2) round_coef = 3 print(f\u0026#34;About the the Linear Regression regression model (choosing the predictors selected by BSS - performed by Andrea):\\n\\ * predictors: {predLabels}\\n\\ * coefficients: {bssA.coef_.round(round_coef).tolist()}\\n\\ * intercept: {bssA.intercept_.round(round_coef).round(round_coef)}\\n\\ * R^2 on training set: {R2_train_bssA.round(round_coef)}\\n\\ * R^2 on test set: {R2_test_bssA.round(round_coef)}\\n\\ \u0026#34;) About the the Linear Regression regression model (choosing the predictors selected by BSS - performed by Andrea): * predictors: ['lcavol', 'svi', 'gleason'] * coefficients: [0.74, 0.225, 0.029] * intercept: 2.452 * R^2 on training set: 0.561 * R^2 on test set: 0.635 Warning: Choosing the bestBest RSS for Best Selection, Backward Selection and Forward Selection gave the same choice of predictors: [lcavol, svi, gleason] ## For Best Subset model - RSS - Hastie and Tibshirani (book) predLabels = [\u0026#39;lcavol\u0026#39;,\u0026#39;lweight\u0026#39;] X1, y1 = X_train[predLabels], Y_train X2, y2 = X_test[predLabels], Y_test # Create the linear model, fitting also the intecept (non-zero) bssHT = LinearRegression(fit_intercept = True).fit(X1,y1) # Stats on train R2_train_bssHT = bssHT.score(X1,y1) # Stats on test R2_test_bssHT = bssHT.score(X2,y2) round_coef = 3 print(f\u0026#34;About the the Linear Regression regression model (choosing the predictors selected by BSS - performed by HT):\\n\\ * predictors: {predLabels}\\n\\ * coefficients: {bssHT.coef_.round(round_coef).tolist()}\\n\\ * intercept: {bssHT.intercept_.round(round_coef).round(round_coef)}\\n\\ * R^2 on training set: {R2_train_bssHT.round(round_coef)}\\n\\ * R^2 on test set: {R2_test_bssHT.round(round_coef)}\\n\\ \u0026#34;) About the the Linear Regression regression model (choosing the predictors selected by BSS - performed by HT): * predictors: ['lcavol', 'lweight'] * coefficients: [0.78, 0.352] * intercept: 2.452 * R^2 on training set: 0.615 * R^2 on test set: 0.531 Warning: BSS performed by HT [book] ols.coef_ array([ 0.71640701, 0.2926424 , -0.14254963, 0.2120076 , 0.30961953, -0.28900562, -0.02091352, 0.27734595]) # table round_coef = 3 headers = [\u0026#39;Term\u0026#39;,\u0026#39;OLS\u0026#39;,\u0026#39;BSS\\n(Andrea)\u0026#39;,\u0026#39;BSS\\n(HT)\u0026#39;,\u0026#39;Ridge CV\\n(LOOCV)\u0026#39;,\u0026#39;Ridge CV\\n(10 Folds)\u0026#39;, \u0026#39;Lasso\\n(alpha=0.1)\u0026#39;] # labels (0-th column) intercept_label = np.array([\u0026#39;Intercept\u0026#39;]) coefs_label = X_train.columns.tolist() te_label = np.array([\u0026#39;Test Error\\n(R^2 on test set)\u0026#39;]) labels = np.concatenate((intercept_label,coefs_label,te_label),axis=0) # ols columns values ols_int = np.array([ols.intercept_]) ols_coefs = ols.coef_ ols_te = np.array([R2_test_ols]) ols_values = np.concatenate((ols_int,ols_coefs,ols_te), axis=0).round(round_coef) # BSS Andrea columns values bssA_int = np.array([bssA.intercept_]) tmp = [bssA.coef_[0],0,0,0,bssA.coef_[1],0,bssA.coef_[2],0] bssA_coefs = np.array(tmp) bssA_te = np.array([R2_test_bssA]) bssA_values = np.concatenate((bssA_int, bssA_coefs, bssA_te), axis=0).round(round_coef) # BSS Hastie and Tibshirani [book] columns values bssHT_int = np.array([bssHT.intercept_]) tmp = [bssHT.coef_[0],bssHT.coef_[1],0,0,0,0,0,0] bssHT_coefs = np.array(tmp) bssHT_te = np.array([R2_test_bssHT]) bssHT_values = np.concatenate((bssHT_int, bssHT_coefs, bssHT_te), axis=0).round(round_coef) # ridge LOO columns values model = ridgeLOO_best model_int = np.array([model.intercept_]) model_coefs = model.coef_ model_te = np.array([R2_test_ridgeLOO_best]) ridge_cv_values = np.concatenate((model_int,model_coefs, model_te), axis=0).round(round_coef) # ridge 10 CV columns values model = ridge_10cv_best model_int = np.array([model.intercept_]) model_coefs = model.coef_ model_te = np.array([R2_test_ridge_10cv_best]) ridge_10cv_values = np.concatenate((model_int,model_coefs, model_te), axis=0).round(round_coef) # lasso columns values lasso_int = np.array([lasso.intercept_]) lasso_coefs = lasso.coef_ lasso_te = np.array([R2_test_lasso]) lasso_values = np.concatenate((lasso_int,lasso_coefs, lasso_te), axis=0).round(round_coef) table = np.column_stack((labels,ols_values,bssA_values,bssHT_values,ridge_cv_values,ridge_10cv_values, lasso_values)) print(tabulate(table, headers=headers, tablefmt=\u0026#39;fancy_grid\u0026#39;)) ╒═══════════════════╤════════╤════════════╤════════╤════════════╤══════════════╤═══════════════╕ │ Term │ OLS │ BSS │ BSS │ Ridge CV │ Ridge CV │ Lasso │ │ │ │ (Andrea) │ (HT) │ (LOOCV) │ (10 Folds) │ (alpha=0.1) │ ╞═══════════════════╪════════╪════════════╪════════╪════════════╪══════════════╪═══════════════╡ │ Intercept │ 2.452 │ 2.452 │ 2.452 │ 2.478 │ 2.478 │ 2.452 │ ├───────────────────┼────────┼────────────┼────────┼────────────┼──────────────┼───────────────┤ │ lcavol │ 0.716 │ 0.74 │ 0.78 │ 0.59 │ 0.195 │ 0.575 │ ├───────────────────┼────────┼────────────┼────────┼────────────┼──────────────┼───────────────┤ │ lweight │ 0.293 │ 0 │ 0.352 │ 0.26 │ 0.118 │ 0.23 │ ├───────────────────┼────────┼────────────┼────────┼────────────┼──────────────┼───────────────┤ │ age │ -0.143 │ 0 │ 0 │ -0.128 │ 0.01 │ -0 │ ├───────────────────┼────────┼────────────┼────────┼────────────┼──────────────┼───────────────┤ │ lbph │ 0.212 │ 0 │ 0 │ 0.126 │ 0.047 │ 0.105 │ ├───────────────────┼────────┼────────────┼────────┼────────────┼──────────────┼───────────────┤ │ svi │ 0.31 │ 0.225 │ 0 │ 0.287 │ 0.131 │ 0.172 │ ├───────────────────┼────────┼────────────┼────────┼────────────┼──────────────┼───────────────┤ │ lcp │ -0.289 │ 0 │ 0 │ -0.065 │ 0.101 │ 0 │ ├───────────────────┼────────┼────────────┼────────┼────────────┼──────────────┼───────────────┤ │ gleason │ -0.021 │ 0.029 │ 0 │ 0.045 │ 0.058 │ 0 │ ├───────────────────┼────────┼────────────┼────────┼────────────┼──────────────┼───────────────┤ │ pgg45 │ 0.277 │ 0 │ 0 │ 0.099 │ 0.068 │ 0.065 │ ├───────────────────┼────────┼────────────┼────────┼────────────┼──────────────┼───────────────┤ │ Test Error │ 0.503 │ 0.635 │ 0.531 │ 0.609 │ 0.459 │ 0.569 │ │ (R^2 on test set) │ │ │ │ │ │ │ ╘═══════════════════╧════════╧════════════╧════════╧════════════╧══════════════╧═══════════════╛ # We can compare the above taqble with the one in the book: Image(\u0026#34;../input/prostate-data/tab3.png\u0026#34;) Warning: In the book is not provided the reg parameter choosen to perform Lasso for the table The above chart is the figure 3.8 in the book The Elements of Statistical Learning: Data Mining, Inference, and Prediction.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":null,"permalink":"/posts/some-exercise-about-statistical-learning/sl-ex4-prostate-shrinkage/","section":"Posts","summary":"","title":"SL4: Analysis of Prostate Cancer dataset – shrinkage methods"},{"content":" # data analysis and wrangling import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # import random as rnd # visualization import seaborn as sns import matplotlib.pyplot as plt # %matplotlib inline # machine learning from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score from sklearn.metrics import mean_squared_error import statsmodels.api as sm import itertools # itertools functions create iterators for efficient looping # itertools.combinations(p,r) creates r-length tuples, in sorted order, no repeated elements # ex. combinations(\u0026#39;ABCD\u0026#39;, 2) = [AB AC AD BC BD CD] #from IPython.display import Image # to visualize images #from tabulate import tabulate # to create tables import os for dirname, _, filenames in os.walk(\u0026#39;/kaggle/input\u0026#39;): for filename in filenames: print(os.path.join(dirname, filename)) /kaggle/input/prostate-data/tab4.png /kaggle/input/prostate-data/tab4-no-cap.png /kaggle/input/prostate-data/tab3.png /kaggle/input/prostate-data/prostate.data /kaggle/input/prostate-data/tab.png /kaggle/input/prostate-data/tab2.png 1. Open your kernel SL_EX2_ProstateCancer_Surname in Kaggle 2. Generate a copy called SL_EX3_SubsetSelection_Surname by the Fork button Data acquisition ## Load the Prostate Cancer dataset data = pd.read_csv(\u0026#39;../input/prostate-data/prostate.data\u0026#39;,sep=\u0026#39;\\t\u0026#39;) # Save \u0026#34;train\u0026#34; and lpsa\u0026#34; columns into Pandas Series variables train = data[\u0026#39;train\u0026#39;] lpsa = data[\u0026#39;lpsa\u0026#39;] # Drop \u0026#34;train\u0026#34; and lpsa\u0026#34; variable from data data = data.drop(columns=[\u0026#39;Unnamed: 0\u0026#39;,\u0026#39;lpsa\u0026#39;,\u0026#39;train\u0026#39;],axis=1) Data pre-processing ### X VARIABLE: # Split the data in train and test sets dataTrain = data.loc[train == \u0026#39;T\u0026#39;] # Obviously, len(idx)==len(dataTrain) is True! dataTest = data.loc[train == \u0026#39;F\u0026#39;] # Rename these two variables as \u0026#34;predictorsTrain\u0026#34; and \u0026#34;predictorsTest\u0026#34; predictorsTrain = dataTrain predictorsTest = dataTest ## Y VARIABLE: # Split the \u0026#34;lpsa\u0026#34; in train and test sets lpsaTrain = lpsa.loc[train == \u0026#39;T\u0026#39;] lpsaTest = lpsa.loc[train == \u0026#39;F\u0026#39;] Standardization \\[\\dfrac{predictors - predictorsMeans}{predictorsStd}\\]\n# Standardize \u0026#34;predictorsTrain\u0026#34; predictorsTrainMeans = predictorsTrain.mean() predictorsTrainStds = predictorsTrain.std() predictorsTrain_std = (predictorsTrain - predictorsTrainMeans)/predictorsTrainStds # standardized variables of predictorTrain # Standardize \u0026#34;predictorsTest\u0026#34; (using the mean and std of predictorsTrain, it\u0026#39;s better!) predictorsTest_std = (predictorsTest - predictorsTrainMeans)/predictorsTrainStds # standardized variables of predictorTest Split into Training and Test sets ## TRAINING SET X_train = predictorsTrain_std Y_train = lpsaTrain ## TEST SET X_test = predictorsTest_std Y_test = lpsaTest Useful functions for the analysis of the Prostate_Cancer_dataset #For Linear Regression # Useful functions in order to compute RSS, R_squared and Zscore. def LinReg(X_train,Y_train,X_test,Y_test): # Create the linear model, fitting also the intecept (non-zero) model = LinearRegression(fit_intercept = True) # Train the model on training set model.fit(X_train,Y_train) # Stats on Training set Y_train_pred = model.predict(X_train) RSS_train = mean_squared_error(Y_train,Y_train_pred) * len(Y_train) R2_train = model.score(X_train,Y_train) # Stats on Test set Y_test_pred = model.predict(X_test) RSS_test = mean_squared_error(Y_test,Y_test_pred) * len(Y_test) R2_test = model.score(X_test,Y_test) return RSS_train, RSS_test, R2_train def Zscore(X_train,Y_train): # fitting the model model = sm.OLS(Y_train, sm.add_constant(X_train)).fit() Zscores = model.tvalues[1:] # we don\u0026#39;t want const min_Zscore = min(abs(Zscores)) idx_min_Zscore = abs(Zscores).idxmin() # it\u0026#39;s the nearest to zero, so the variable less significant! return Zscores, min_Zscore, idx_min_Zscore To print some info ## Print information about an iteration forward subset selection with subset size ncomb # ncomb = number of the iteration # features = remaining_features, selected_features (used to do the iteration and take track of the selection) # params = all_RSS, all_R_squared, all_combs # results = best_RSS, best_feature (results of the selection) # detailed = parameter to see more detail about each single combination def get_info_forwardS(ncomb,features,params,results,detailed): sepComb = \u0026#34;===\u0026#34;*30 sepIter = \u0026#34;---\u0026#34;*30 remaining, selected = features bestFeat, bestRSS = results print(f\u0026#34;{sepComb}\\nIter n.{ncomb}:\\n\\ Choose {ncomb}-length combinations of the remaining variables\\n\\n\\ Remaining features: {remaining}\\n\\ Features selected: {selected}\u0026#34;) if detailed == 1: RSS, R_squared, Combs = params for niter in range(0,len(Combs)): var0 = Combs[niter] var1 = RSS[niter] var2 = R_squared[niter] print(f\u0026#34;\\nComb n.{niter+1}: {var0}\\n\\ {sepIter}\\n\\ RSS test: {var1}\\n\\ R_squared: {var2}\\ \u0026#34;) print(f\u0026#34;\\nSelected variables: {bestFeat}\\n\\ min RSS: {bestRSS}\\n\\ {sepComb}\\n\u0026#34;) return ## Print information about an iteration backward subset selection with subset size ncomb # ncomb = number of the iteration # features = [remaining_features,dropped_features_list] (used to do the iteration and take track of the selection) # params = [all_RSS,all_R_squared,all_combs] # results = [best_RSS,dropped_feature] # detailed = parameter to see more detail about each single combination def get_info_backwardS(ncomb,features,params,results,detailed): sepComb = \u0026#34;===\u0026#34;*30 sepIter = \u0026#34;---\u0026#34;*30 remaining, dropped = features droppedFeat, bestRSS = results print(f\u0026#34;{sepComb}\\nIter n.{8 - ncomb}:\\n\\n\\ At the beginning we have:\\n\\ Remaining features: {remaining}\\n\\ Dropped features: {dropped}\\n\\n\\ Now we compare the model selecting {ncomb} variables\u0026#34;) if detailed == 1: RSS, R_squared, Combs = params for niter in range(0,len(Combs)): var0 = Combs[niter] var1 = RSS[niter] var2 = R_squared[niter] print(f\u0026#34;\\n\\nComb n.{niter+1}: {var0}\\n\\ {sepIter}\\n\\ candidate dropped feature: {list(set(remaining)-set(var0))}\\n\\ RSS test: {var1}\\ \u0026#34;) print(f\u0026#34;\\n\\nAt the end we have:\\n\\ min RSS: {bestRSS}\\n\\ We drop: {droppedFeat}\\n\\ {sepComb}\\n\u0026#34;) return ## Print information about an iteration backward subset selection with subset size ncomb # ncomb = number of the iteration # features = [remaining_features,dropped_features_list] (used to do the iteration and take track of the selection) # params = [all_RSS,all_R_squared,all_combs] # results = [best_RSS,dropped_feature] # detailed = parameter to see more detail about each single combination def get_info_backwardS_Zscore(ncomb,features,params,results,detailed): sepComb = \u0026#34;===\u0026#34;*30 sepIter = \u0026#34;---\u0026#34;*30 remaining, dropped = features droppedFeat, bestRSS = results print(f\u0026#34;{sepComb}\\nIter n.{8 - ncomb}:\\n\\n\\ At the beginning we have:\\n\\ Remaining features: {remaining}\\n\\ Dropped features: {dropped}\\n\u0026#34;) print(\u0026#34;\\nThe Z-scores are:\\n\u0026#34;,Zscores) print(f\u0026#34;\\n\\nAt the end we have:\\n\\ min RSS: {bestRSS}\\n\\ We drop: {droppedFeat}\\n\\ {sepComb}\\n\u0026#34;) return Best Subset Selection #3. Starting from the `ols models` achieved in the last steps, perform best-subset selection. Generate one model for each combination of the 8 variables available For each model compute the RSS on training and test set, the number of variables and the \\(R^2\\) of the model Save these numbers in suitable data structures ## range variables = data.columns.tolist() # excluding \u0026#39;const\u0026#39; ## Initialize the list where we temporarily store data RSS_train_list, RSS_test_list, R_squared_list = [], [], [] numb_features, features_list = [], [] for k in range(1,len(variables) + 1): # niter = 0 # print(\u0026#34;---\u0026#34;*30,f\u0026#34;\\nStart by choosing {k} variables\\n\u0026#34;) # Looping over all possible combinations of k variables for combo in itertools.combinations(variables,k): # niter = niter+1 # Compute all the statistics we need RSS_train, RSS_test, Rsquared_train = LinReg(X_train[list(combo)], Y_train, X_test[list(combo)], Y_test) # rnd = 4 # print(f\u0026#34;{niter}. Variables: {list(combo)}\\n\\ # RSS train: {RSS_train.round(rnd)}\\n\\ # RSS test: {RSS_test.round(rnd)}\\n\\ # R^2 train: {Rsquared_train.round(rnd)}\\n\u0026#34;) # Save the statistics RSS_train_list.append(RSS_train) RSS_test_list.append(RSS_test) R_squared_list.append(Rsquared_train) # Save features and number of features features_list.append(combo) numb_features.append(len(combo)) # print(f\u0026#34;\\nUsing {k} variables we have computed {niter} models\u0026#34;) # print(\u0026#34;---\u0026#34;*30,\u0026#34;\\n\u0026#34;) #Store in DataFrame df_BestS = pd.DataFrame({\u0026#39;numb_features\u0026#39;: numb_features,\\ \u0026#39;RSS_train\u0026#39;: RSS_train_list,\\ \u0026#39;RSS_test\u0026#39;: RSS_test_list,\\ \u0026#39;R_squared\u0026#39;: R_squared_list,\\ \u0026#39;features\u0026#39;: features_list}) df_BestS numb_features RSS_train RSS_test R_squared features 0 1 44.528583 14.392162 0.537516 (lcavol,) 1 1 73.613540 30.402846 0.235434 (lweight,) 2 1 91.292039 33.846748 0.051821 (age,) 3 1 89.624912 35.298771 0.069136 (lbph,) 4 1 66.422403 20.632078 0.310122 (svi,) ... ... ... ... ... ... 250 7 31.570706 14.702112 0.672100 (lcavol, lweight, age, svi, lcp, gleason, pgg45) 251 7 30.414990 17.034552 0.684103 (lcavol, lweight, lbph, svi, lcp, gleason, pgg45) 252 7 33.265433 16.754443 0.654498 (lcavol, age, lbph, svi, lcp, gleason, pgg45) 253 7 44.036622 22.633329 0.542626 (lweight, age, lbph, svi, lcp, gleason, pgg45) 254 8 29.426384 15.638220 0.694371 (lcavol, lweight, age, lbph, svi, lcp, gleason... 255 rows × 5 columns\nFind the best subset (of variables) for each number of features Now our dataframe df_BestS has a row for each model computed, and it\u0026rsquo;s not easy to handle. For this reason, we extract the best model for each number of variables by observing the interesting parameter.\nWe consider as interesting parameter:\nminimum RSS train minimum RSS test maximum \\(R^2\\) # Create new df, selection only best subsets of variables (based on RSS and R^2) df_BestS_RSS_train= df_BestS[df_BestS.groupby(\u0026#39;numb_features\u0026#39;)[\u0026#39;RSS_train\u0026#39;].transform(min) == df_BestS[\u0026#39;RSS_train\u0026#39;]] df_BestS_RSS_test = df_BestS[df_BestS.groupby(\u0026#39;numb_features\u0026#39;)[\u0026#39;RSS_test\u0026#39;].transform(min) == df_BestS[\u0026#39;RSS_test\u0026#39;]] df_BestS_R_squared = df_BestS[df_BestS.groupby(\u0026#39;numb_features\u0026#39;)[\u0026#39;R_squared\u0026#39;].transform(max) == df_BestS[\u0026#39;R_squared\u0026#39;]] df_BestS_RSS_train numb_features RSS_train RSS_test R_squared features 0 1 44.528583 14.392162 0.537516 (lcavol,) 8 2 37.091846 14.774470 0.614756 (lcavol, lweight) 38 3 34.907749 12.015924 0.637441 (lcavol, lweight, svi) 97 4 32.814995 13.689964 0.659176 (lcavol, lweight, lbph, svi) 174 5 32.069447 14.577726 0.666920 (lcavol, lweight, lbph, svi, pgg45) 229 6 30.539778 16.457800 0.682807 (lcavol, lweight, lbph, svi, lcp, pgg45) 247 7 29.437300 15.495405 0.694258 (lcavol, lweight, age, lbph, svi, lcp, pgg45) 254 8 29.426384 15.638220 0.694371 (lcavol, lweight, age, lbph, svi, lcp, gleason... df_BestS_RSS_test numb_features RSS_train RSS_test R_squared features 0 1 44.528583 14.392162 0.537516 (lcavol,) 11 2 42.312584 11.583584 0.560532 (lcavol, svi) 52 3 42.267034 11.484038 0.561005 (lcavol, svi, gleason) 112 4 42.223638 11.612573 0.561456 (lcavol, age, svi, gleason) 167 5 34.170209 11.497692 0.645101 (lcavol, lweight, age, svi, gleason) 226 6 33.642783 12.009380 0.650579 (lcavol, lweight, age, svi, gleason, pgg45) 246 7 30.958630 13.492898 0.678457 (lcavol, lweight, age, lbph, svi, lcp, gleason) 254 8 29.426384 15.638220 0.694371 (lcavol, lweight, age, lbph, svi, lcp, gleason... # the same as selecting min RSS on training set df_BestS_R_squared numb_features RSS_train RSS_test R_squared features 0 1 44.528583 14.392162 0.537516 (lcavol,) 8 2 37.091846 14.774470 0.614756 (lcavol, lweight) 38 3 34.907749 12.015924 0.637441 (lcavol, lweight, svi) 97 4 32.814995 13.689964 0.659176 (lcavol, lweight, lbph, svi) 174 5 32.069447 14.577726 0.666920 (lcavol, lweight, lbph, svi, pgg45) 229 6 30.539778 16.457800 0.682807 (lcavol, lweight, lbph, svi, lcp, pgg45) 247 7 29.437300 15.495405 0.694258 (lcavol, lweight, age, lbph, svi, lcp, pgg45) 254 8 29.426384 15.638220 0.694371 (lcavol, lweight, age, lbph, svi, lcp, gleason... Plots for Best Selection #4. + 5. + 6. Generate some charts: x-axis: the subset size y-axis: RSS for training set of all the models generated at step 3 \\(R^2\\) of all the models generated at step 3 RSS for test set of all the models generated at step 3 # Initialize the figure width = 6 height = 6 nfig = 3 fig = plt.figure(figsize = (width*nfig,height)) # 1. RSS Training set plot tmp_df1 = df_BestS; # scatter plot tmp_df2 = df_BestS_RSS_train; # plot the line of best values ax1 = fig.add_subplot(1, nfig, 1) ax1.scatter(tmp_df1.numb_features,tmp_df1.RSS_train, alpha = .2, color = \u0026#39;darkblue\u0026#39;); ax1.set_xlabel(\u0026#39;Subset Size k\u0026#39;,fontsize=14); ax1.set_ylabel(\u0026#39;RSS\u0026#39;,fontsize=14); ax1.set_title(\u0026#39;RSS on training set\u0026#39;,fontsize=18); ax1.plot(tmp_df2.numb_features,tmp_df2.RSS_train,color = \u0026#39;r\u0026#39;, label = \u0026#39;Best subset\u0026#39;); # line of best values ax1.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); ax1.legend(); # 2. RSS Test set plot tmp_df1 = df_BestS; # scatter plot tmp_df2 = df_BestS_RSS_test; # plot the line of best values ax2 = fig.add_subplot(1, nfig, 2); ax2.scatter(tmp_df1.numb_features,tmp_df1.RSS_test, alpha = .2, color = \u0026#39;darkblue\u0026#39;); ax2.set_xlabel(\u0026#39;Subset Size k\u0026#39;,fontsize=14); ax2.set_ylabel(\u0026#39;RSS\u0026#39;,fontsize=14); ax2.set_title(\u0026#39;RSS on test set\u0026#39;,fontsize=18); ax2.plot(tmp_df2.numb_features,tmp_df2.RSS_test,color = \u0026#39;r\u0026#39;, label = \u0026#39;Best subset\u0026#39;); # line of best values ax2.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); ax2.legend(); # 3. R^2 plot tmp_df1 = df_BestS; # scatter plot tmp_df2 = df_BestS_R_squared; # plot the line of best values ax3 = fig.add_subplot(1, nfig, 3); ax3.scatter(tmp_df1.numb_features,tmp_df1.R_squared, alpha = .2, color = \u0026#39;darkblue\u0026#39;); ax3.set_xlabel(\u0026#39;Subset Size k\u0026#39;,fontsize=14); ax3.set_ylabel(\u0026#39;$R^2$\u0026#39;,fontsize=14); ax3.set_ylim(bottom=-0.1,top=1.1) ax3.set_title(\u0026#39;$R^2$ on training set\u0026#39;,fontsize=18); ax3.plot(tmp_df2.numb_features,tmp_df2.R_squared,color = \u0026#39;r\u0026#39;, label = \u0026#39;Best subset\u0026#39;); # line of best values ax3.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); ax3.legend(); fig.suptitle(\u0026#39;Best Subset Selection\u0026#39;,fontsize=25, y=0.98); fig.subplots_adjust(top=0.8) plt.show(); Estimating test error by adjusting training error #In general, the training set MSE underestimates the test MSE. In fact we fit a model to the training data using least squares and we estimate the coefficients of the regression in such a way the training RSS is minimized. So the training RSS decreases as we add more variables to the model, but the test RSS may not!\nFor this reason the training RSS (and so also \\(R^2\\)) may not be used directly for selecting the best model, but we need to adjust them to get an estimate of the test error.\nAttempt n.1: using sklearn library AIC_list, BIC_list, R_squared_adj_list =[], [], [] # from sklearn I can get no Cp estimate out of the box. best_features = df_BestS_RSS_train[\u0026#39;features\u0026#39;] for features in best_features: regr = sm.OLS(Y_train, sm.add_constant(X_train[list(features)])).fit() AIC_list.append(regr.aic) BIC_list.append(regr.bic) R_squared_adj_list.append(regr.rsquared_adj) #Store in DataFrame df1 = pd.DataFrame({\u0026#39;numb_features\u0026#39;: df_BestS_RSS_train[\u0026#39;numb_features\u0026#39;],\\ \u0026#39;RSS_train\u0026#39;: df_BestS_RSS_train[\u0026#39;RSS_train\u0026#39;],\\ \u0026#39;features\u0026#39;: df_BestS_RSS_train[\u0026#39;features\u0026#39;],\\ \u0026#39;AIC\u0026#39;: AIC_list,\\ \u0026#39;BIC\u0026#39;: BIC_list,\\ \u0026#39;R_squared_adj\u0026#39;: R_squared_adj_list}) df1 numb_features RSS_train features AIC BIC R_squared_adj 0 1 44.528583 (lcavol,) 166.764154 171.173540 0.530401 8 2 37.091846 (lcavol, lweight) 156.520967 163.135045 0.602717 38 3 34.907749 (lcavol, lweight, svi) 154.454850 163.273620 0.620176 97 4 32.814995 (lcavol, lweight, lbph, svi) 152.312691 163.336154 0.637188 174 5 32.069447 (lcavol, lweight, lbph, svi, pgg45) 152.772911 166.001067 0.639618 229 6 30.539778 (lcavol, lweight, lbph, svi, lcp, pgg45) 151.498370 166.931219 0.651088 247 7 29.437300 (lcavol, lweight, age, lbph, svi, lcp, pgg45) 151.034951 168.672492 0.657983 254 8 29.426384 (lcavol, lweight, age, lbph, svi, lcp, gleason... 153.010102 172.852336 0.652215 # Initialize the figure width = 6 height = 6 nfig = 3 fig = plt.figure(figsize = (width*nfig,height)) # 1. AIC ax1 = fig.add_subplot(1, nfig, 1) ax1.scatter(df1[\u0026#39;numb_features\u0026#39;],df1[\u0026#39;AIC\u0026#39;], alpha = .5, color = \u0026#39;darkblue\u0026#39;); ax1.plot(df1[\u0026#39;numb_features\u0026#39;],df1[\u0026#39;AIC\u0026#39;],color = \u0026#39;r\u0026#39;, alpha = .5); # line of best values ax1.plot(df1[\u0026#39;AIC\u0026#39;].argmin()+1, df1[\u0026#39;AIC\u0026#39;].min(), marker=\u0026#39;x\u0026#39;, markersize=20, color = \u0026#39;b\u0026#39;); # best val ax1.set_xlabel(\u0026#39;Number of predictors\u0026#39;,fontsize=14); ax1.set_ylabel(\u0026#39;AIC\u0026#39;,fontsize=14); ax1.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); # 2. BIC ax2 = fig.add_subplot(1, nfig, 2) ax2.scatter(df1[\u0026#39;numb_features\u0026#39;],df1[\u0026#39;BIC\u0026#39;], alpha = .5, color = \u0026#39;darkblue\u0026#39;); ax2.plot(df1[\u0026#39;numb_features\u0026#39;],df1[\u0026#39;BIC\u0026#39;],color = \u0026#39;r\u0026#39;, alpha = .5); # line of best values ax2.plot(df1[\u0026#39;BIC\u0026#39;].argmin()+1, df1[\u0026#39;BIC\u0026#39;].min(), marker=\u0026#39;x\u0026#39;, markersize=20, color = \u0026#39;b\u0026#39;); # best val ax2.set_xlabel(\u0026#39;Number of predictors\u0026#39;,fontsize=14); ax2.set_ylabel(\u0026#39;BIC\u0026#39;,fontsize=14); ax2.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); # 3. R2_adj ax3 = fig.add_subplot(1, nfig, 3) ax3.scatter(df1[\u0026#39;numb_features\u0026#39;],df1[\u0026#39;R_squared_adj\u0026#39;], alpha = .5, color = \u0026#39;darkblue\u0026#39;); ax3.plot(df1[\u0026#39;numb_features\u0026#39;],df1[\u0026#39;R_squared_adj\u0026#39;],color = \u0026#39;r\u0026#39;, alpha = .5); # line of best values ax3.plot(df1[\u0026#39;R_squared_adj\u0026#39;].argmax()+1, df1[\u0026#39;R_squared_adj\u0026#39;].max(), marker=\u0026#39;x\u0026#39;, markersize=20, color = \u0026#39;b\u0026#39;); # best val ax3.set_xlabel(\u0026#39;Number of predictors\u0026#39;,fontsize=14); ax3.set_ylabel(r\u0026#39;$R^2$ adj\u0026#39;,fontsize=14); ax3.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); fig.suptitle(\u0026#39;Best Subset Selection:\\n Estimating test error by adjusting training error\u0026#39;,fontsize=25, y=0.98); fig.subplots_adjust(top=0.8) fig.subplots_adjust(wspace=0.275) # the amount of width reserved for blank space between subplots plt.show(); Attempt n.2: estimating AIC, BIC and R^2 adjusted by our own #Initializing useful variables n = len(Y_train) d = df_BestS_RSS_train[\u0026#39;numb_features\u0026#39;] p = 8 RSS = df_BestS_RSS_train[\u0026#39;RSS_train\u0026#39;] Rsquared = df_BestS_RSS_train[\u0026#39;R_squared\u0026#39;] # Estimation of sigma^2: RSE of the general multiple Linear regression with p features RSE = np.sqrt(min(RSS)/(n - p -1)) # min(RSS) is the RSS of the full model with p predictors hat_sigma_squared = RSE**2 #Computing AIC = (1/(n*hat_sigma_squared)) * (RSS + 2 * d * hat_sigma_squared ) BIC = (1/(n*hat_sigma_squared)) * (RSS + np.log(n) * d * hat_sigma_squared ) ratio = 1 - Rsquared # RSS/TSS R_squared_adj = 1 - ratio*(n-1)/(n-d-1) #Store in DataFrame df2 = pd.DataFrame({\u0026#39;numb_features\u0026#39;: df_BestS_RSS_train[\u0026#39;numb_features\u0026#39;],\\ \u0026#39;RSS_train\u0026#39;: df_BestS_RSS_train[\u0026#39;RSS_train\u0026#39;],\\ \u0026#39;features\u0026#39;: df_BestS_RSS_train[\u0026#39;features\u0026#39;],\\ \u0026#39;AIC\u0026#39;: AIC,\\ \u0026#39;BIC\u0026#39;: BIC,\\ \u0026#39;R_squared_adj\u0026#39;: R_squared_adj}) df2 numb_features RSS_train features AIC BIC R_squared_adj 0 1 44.528583 (lcavol,) 1.339802 1.372708 0.530401 8 2 37.091846 (lcavol, lweight) 1.150877 1.216689 0.602717 38 3 34.907749 (lcavol, lweight, svi) 1.116476 1.215193 0.620176 97 4 32.814995 (lcavol, lweight, lbph, svi) 1.084761 1.216385 0.637188 174 5 32.069447 (lcavol, lweight, lbph, svi, pgg45) 1.092680 1.257209 0.639618 229 6 30.539778 (lcavol, lweight, lbph, svi, lcp, pgg45) 1.077530 1.274965 0.651088 247 7 29.437300 (lcavol, lweight, age, lbph, svi, lcp, pgg45) 1.074948 1.305289 0.657983 254 8 29.426384 (lcavol, lweight, age, lbph, svi, lcp, gleason... 1.104478 1.367724 0.652215 # Initialize the figure width = 6 height = 6 nfig = 3 fig = plt.figure(figsize = (width*nfig,height)) # 1. AIC ax1 = fig.add_subplot(1, nfig, 1) ax1.scatter(df2[\u0026#39;numb_features\u0026#39;],df2[\u0026#39;AIC\u0026#39;], alpha = .5, color = \u0026#39;darkblue\u0026#39;); ax1.plot(df2[\u0026#39;numb_features\u0026#39;],df2[\u0026#39;AIC\u0026#39;],color = \u0026#39;r\u0026#39;, alpha = .5); # line of best values ax1.plot(df2[\u0026#39;AIC\u0026#39;].argmin()+1, df2[\u0026#39;AIC\u0026#39;].min(), marker=\u0026#39;x\u0026#39;, markersize=20, color = \u0026#39;b\u0026#39;); # best val ax1.set_xlabel(\u0026#39;Number of predictors\u0026#39;,fontsize=14); ax1.set_ylabel(\u0026#39;AIC\u0026#39;,fontsize=14); ax1.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); # 2. BIC ax2 = fig.add_subplot(1, nfig, 2) ax2.scatter(df2[\u0026#39;numb_features\u0026#39;],df2[\u0026#39;BIC\u0026#39;], alpha = .5, color = \u0026#39;darkblue\u0026#39;); ax2.plot(df2[\u0026#39;numb_features\u0026#39;],df2[\u0026#39;BIC\u0026#39;],color = \u0026#39;r\u0026#39;, alpha = .5); # line of best values ax2.plot(df2[\u0026#39;BIC\u0026#39;].argmin()+1, df2[\u0026#39;BIC\u0026#39;].min(), marker=\u0026#39;x\u0026#39;, markersize=20, color = \u0026#39;b\u0026#39;); # best val ax2.set_xlabel(\u0026#39;Number of predictors\u0026#39;,fontsize=14); ax2.set_ylabel(\u0026#39;BIC\u0026#39;,fontsize=14); ax2.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); # 3. R2_adj ax3 = fig.add_subplot(1, nfig, 3) ax3.scatter(df2[\u0026#39;numb_features\u0026#39;],df2[\u0026#39;R_squared_adj\u0026#39;], alpha = .5, color = \u0026#39;darkblue\u0026#39;); ax3.plot(df2[\u0026#39;numb_features\u0026#39;],df2[\u0026#39;R_squared_adj\u0026#39;],color = \u0026#39;r\u0026#39;, alpha = .5); # line of best values ax3.plot(df2[\u0026#39;R_squared_adj\u0026#39;].argmax()+1, df2[\u0026#39;R_squared_adj\u0026#39;].max(), marker=\u0026#39;x\u0026#39;, markersize=20, color = \u0026#39;b\u0026#39;); # best val ax3.set_xlabel(\u0026#39;Number of predictors\u0026#39;,fontsize=14); ax3.set_ylabel(r\u0026#39;$R^2$ adj\u0026#39;,fontsize=14); ax3.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); fig.suptitle(\u0026#39;Best Subset Selection:\\n Estimating test error by adjusting training error\u0026#39;,fontsize=25, y=0.98); fig.subplots_adjust(top=0.8) fig.subplots_adjust(wspace=0.275) # the amount of width reserved for blank space between subplots plt.show(); Forward selection #For computational reasons, the best subset cannot be applied for any large n due to the \\(2^n\\) complexity.\nForward Stepwise begins with a model containing no predictors, and then adds predictors to the model, one at the time. At each step, the variable that gives the greatest additional improvement to the fit is added to the model.\n7. Perform forward selection Start from the empty model Add at each step the variable that minimizes the RSS computed on test set (other performance measures can be used) # to print some info: flag = 1 detailed = 1 ## range variables = data.columns.tolist() remaining_features, selected_features = variables.copy(), [] ## Initialize the list where we temporarily store data RSS_test_list, min_RSS_test_list, R_squared_list = [], [], [] numb_features, features_list = [], [] # Loop over the number of variables for k in range(1,len(variables)+1): # store some info for each k all_RSS, all_R_squared, all_combs = [],[], [] best_RSS = np.inf # initialize the best RSS as +inf # choose one variable in the remaining features for var in remaining_features: tmpComb = selected_features + [var]; # combination of variables # Compute all the statistics we need _, RSS_test, R_squared = LinReg(X_train[tmpComb], Y_train, X_test[tmpComb], Y_test) # we don\u0026#39;t want RSS on training set # save temporary stats all_RSS.append(RSS_test) all_R_squared.append(R_squared) all_combs.append(tmpComb) # update if we reach a better RSS if RSS_test \u0026lt; best_RSS: best_RSS = RSS_test best_R_squared = R_squared best_feature = var # Print some information, before upgrading the features if flag == 1: features = [remaining_features,selected_features] params = [all_RSS,all_R_squared,all_combs] results = [best_feature,best_RSS] get_info_forwardS(k,features,params,results,detailed) # Save the statistics RSS_test_list.append(all_RSS) min_RSS_test_list.append(best_RSS) R_squared_list.append(best_R_squared) # Update variables for next loop selected_features.append(best_feature) remaining_features.remove(best_feature) # Save features and number of features features_list.append(selected_features.copy()) numb_features.append(len(selected_features)) # Store in DataFrame df_ForwardS = pd.DataFrame({\u0026#39;numb_features\u0026#39;: numb_features,\\ \u0026#39;RSS_test\u0026#39; : RSS_test_list,\\ \u0026#39;min_RSS_test\u0026#39;: min_RSS_test_list,\\ \u0026#39;R_squared\u0026#39;: R_squared_list,\\ \u0026#39;features\u0026#39;: features_list}) ========================================================================================== Iter n.1: Choose 1-length combinations of the remaining variables Remaining features: ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45'] Features selected: [] Comb n.1: ['lcavol'] ------------------------------------------------------------------------------------------ RSS test: 14.392161587304827 R_squared: 0.5375164690552883 Comb n.2: ['lweight'] ------------------------------------------------------------------------------------------ RSS test: 30.402845602615997 R_squared: 0.23543378299009432 Comb n.3: ['age'] ------------------------------------------------------------------------------------------ RSS test: 33.846748424133 R_squared: 0.05182105437299367 Comb n.4: ['lbph'] ------------------------------------------------------------------------------------------ RSS test: 35.29877101280492 R_squared: 0.06913619684911343 Comb n.5: ['svi'] ------------------------------------------------------------------------------------------ RSS test: 20.632078139876853 R_squared: 0.3101224985902339 Comb n.6: ['lcp'] ------------------------------------------------------------------------------------------ RSS test: 16.34576112489144 R_squared: 0.23931977441332264 Comb n.7: ['gleason'] ------------------------------------------------------------------------------------------ RSS test: 25.529830407597565 R_squared: 0.11725680432657692 Comb n.8: ['pgg45'] ------------------------------------------------------------------------------------------ RSS test: 28.61697167270323 R_squared: 0.20074696985742568 Selected variables: lcavol min RSS: 14.392161587304827 ========================================================================================== ========================================================================================== Iter n.2: Choose 2-length combinations of the remaining variables Remaining features: ['lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45'] Features selected: ['lcavol'] Comb n.1: ['lcavol', 'lweight'] ------------------------------------------------------------------------------------------ RSS test: 14.77447043041511 R_squared: 0.614756035022443 Comb n.2: ['lcavol', 'age'] ------------------------------------------------------------------------------------------ RSS test: 14.454316401468965 R_squared: 0.53785859616379 Comb n.3: ['lcavol', 'lbph'] ------------------------------------------------------------------------------------------ RSS test: 16.237234684619743 R_squared: 0.5846312407995875 Comb n.4: ['lcavol', 'svi'] ------------------------------------------------------------------------------------------ RSS test: 11.58358360007023 R_squared: 0.5605323092792945 Comb n.5: ['lcavol', 'lcp'] ------------------------------------------------------------------------------------------ RSS test: 15.038931408454383 R_squared: 0.5381501805446671 Comb n.6: ['lcavol', 'gleason'] ------------------------------------------------------------------------------------------ RSS test: 14.140851461764317 R_squared: 0.5386018755651162 Comb n.7: ['lcavol', 'pgg45'] ------------------------------------------------------------------------------------------ RSS test: 13.827662568490943 R_squared: 0.5489982126930548 Selected variables: svi min RSS: 11.58358360007023 ========================================================================================== ========================================================================================== Iter n.3: Choose 3-length combinations of the remaining variables Remaining features: ['lweight', 'age', 'lbph', 'lcp', 'gleason', 'pgg45'] Features selected: ['lcavol', 'svi'] Comb n.1: ['lcavol', 'svi', 'lweight'] ------------------------------------------------------------------------------------------ RSS test: 12.015924403078804 R_squared: 0.6374405385171893 Comb n.2: ['lcavol', 'svi', 'age'] ------------------------------------------------------------------------------------------ RSS test: 11.71416952499475 R_squared: 0.5612383365966047 Comb n.3: ['lcavol', 'svi', 'lbph'] ------------------------------------------------------------------------------------------ RSS test: 14.353733928669755 R_squared: 0.6264131675413883 Comb n.4: ['lcavol', 'svi', 'lcp'] ------------------------------------------------------------------------------------------ RSS test: 13.285116624863331 R_squared: 0.5714253936028364 Comb n.5: ['lcavol', 'svi', 'gleason'] ------------------------------------------------------------------------------------------ RSS test: 11.484037587414818 R_squared: 0.5610054092768177 Comb n.6: ['lcavol', 'svi', 'pgg45'] ------------------------------------------------------------------------------------------ RSS test: 11.632246428034497 R_squared: 0.5651377944544718 Selected variables: gleason min RSS: 11.484037587414818 ========================================================================================== ========================================================================================== Iter n.4: Choose 4-length combinations of the remaining variables Remaining features: ['lweight', 'age', 'lbph', 'lcp', 'pgg45'] Features selected: ['lcavol', 'svi', 'gleason'] Comb n.1: ['lcavol', 'svi', 'gleason', 'lweight'] ------------------------------------------------------------------------------------------ RSS test: 11.95602065641986 R_squared: 0.6405674126734184 Comb n.2: ['lcavol', 'svi', 'gleason', 'age'] ------------------------------------------------------------------------------------------ RSS test: 11.612572746859495 R_squared: 0.5614561297716232 Comb n.3: ['lcavol', 'svi', 'gleason', 'lbph'] ------------------------------------------------------------------------------------------ RSS test: 14.355178421938225 R_squared: 0.6266591566646609 Comb n.4: ['lcavol', 'svi', 'gleason', 'lcp'] ------------------------------------------------------------------------------------------ RSS test: 13.185630584098526 R_squared: 0.5741698339447385 Comb n.5: ['lcavol', 'svi', 'gleason', 'pgg45'] ------------------------------------------------------------------------------------------ RSS test: 11.919697961636555 R_squared: 0.5664871741058846 Selected variables: age min RSS: 11.612572746859495 ========================================================================================== ========================================================================================== Iter n.5: Choose 5-length combinations of the remaining variables Remaining features: ['lweight', 'lbph', 'lcp', 'pgg45'] Features selected: ['lcavol', 'svi', 'gleason', 'age'] Comb n.1: ['lcavol', 'svi', 'gleason', 'age', 'lweight'] ------------------------------------------------------------------------------------------ RSS test: 11.497691985782547 R_squared: 0.64510079253458 Comb n.2: ['lcavol', 'svi', 'gleason', 'age', 'lbph'] ------------------------------------------------------------------------------------------ RSS test: 13.947298386201728 R_squared: 0.6294946547801643 Comb n.3: ['lcavol', 'svi', 'gleason', 'age', 'lcp'] ------------------------------------------------------------------------------------------ RSS test: 13.234367776364218 R_squared: 0.574264491209014 Comb n.4: ['lcavol', 'svi', 'gleason', 'age', 'pgg45'] ------------------------------------------------------------------------------------------ RSS test: 12.141763999596828 R_squared: 0.5670143538267589 Selected variables: lweight min RSS: 11.497691985782547 ========================================================================================== ========================================================================================== Iter n.6: Choose 6-length combinations of the remaining variables Remaining features: ['lbph', 'lcp', 'pgg45'] Features selected: ['lcavol', 'svi', 'gleason', 'age', 'lweight'] Comb n.1: ['lcavol', 'svi', 'gleason', 'age', 'lweight', 'lbph'] ------------------------------------------------------------------------------------------ RSS test: 12.98960324749752 R_squared: 0.6696630706385877 Comb n.2: ['lcavol', 'svi', 'gleason', 'age', 'lweight', 'lcp'] ------------------------------------------------------------------------------------------ RSS test: 12.690514046312407 R_squared: 0.6563534839394491 Comb n.3: ['lcavol', 'svi', 'gleason', 'age', 'lweight', 'pgg45'] ------------------------------------------------------------------------------------------ RSS test: 12.009380388381148 R_squared: 0.6505787510646934 Selected variables: pgg45 min RSS: 12.009380388381148 ========================================================================================== ========================================================================================== Iter n.7: Choose 7-length combinations of the remaining variables Remaining features: ['lbph', 'lcp'] Features selected: ['lcavol', 'svi', 'gleason', 'age', 'lweight', 'pgg45'] Comb n.1: ['lcavol', 'svi', 'gleason', 'age', 'lweight', 'pgg45', 'lbph'] ------------------------------------------------------------------------------------------ RSS test: 13.834231490831279 R_squared: 0.6760051914465672 Comb n.2: ['lcavol', 'svi', 'gleason', 'age', 'lweight', 'pgg45', 'lcp'] ------------------------------------------------------------------------------------------ RSS test: 14.702111705571856 R_squared: 0.6720997902395782 Selected variables: lbph min RSS: 13.834231490831279 ========================================================================================== ========================================================================================== Iter n.8: Choose 8-length combinations of the remaining variables Remaining features: ['lcp'] Features selected: ['lcavol', 'svi', 'gleason', 'age', 'lweight', 'pgg45', 'lbph'] Comb n.1: ['lcavol', 'svi', 'gleason', 'age', 'lweight', 'pgg45', 'lbph', 'lcp'] ------------------------------------------------------------------------------------------ RSS test: 15.638220165228002 R_squared: 0.6943711796768238 Selected variables: lcp min RSS: 15.638220165228002 ========================================================================================== # look at the result df_ForwardS numb_features RSS_test min_RSS_test R_squared features 0 1 [14.392161587304827, 30.402845602615997, 33.84... 14.392162 0.537516 [lcavol] 1 2 [14.77447043041511, 14.454316401468965, 16.237... 11.583584 0.560532 [lcavol, svi] 2 3 [12.015924403078804, 11.71416952499475, 14.353... 11.484038 0.561005 [lcavol, svi, gleason] 3 4 [11.95602065641986, 11.612572746859495, 14.355... 11.612573 0.561456 [lcavol, svi, gleason, age] 4 5 [11.497691985782547, 13.947298386201728, 13.23... 11.497692 0.645101 [lcavol, svi, gleason, age, lweight] 5 6 [12.98960324749752, 12.690514046312407, 12.009... 12.009380 0.650579 [lcavol, svi, gleason, age, lweight, pgg45] 6 7 [13.834231490831279, 14.702111705571856] 13.834231 0.676005 [lcavol, svi, gleason, age, lweight, pgg45, lbph] 7 8 [15.638220165228002] 15.638220 0.694371 [lcavol, svi, gleason, age, lweight, pgg45, lb... Plot for Forward Selection #8. Generate a chart having: - x-axis: the subset size - y-axis: the RSS for the test set of the models generated at step 7 # Initialize the figure width = 6 height = 6 nfig = 1 fig = plt.figure(figsize = (width*nfig,height)) # 1. RSS Test set plot tmp_df = df_ForwardS; ax = fig.add_subplot(1, nfig, 1) for i in range(0,len(tmp_df.RSS_test)): ax.scatter([tmp_df.numb_features[i]]*(len(tmp_df.RSS_test[i])),tmp_df.RSS_test[i], alpha = .2, color = \u0026#39;darkblue\u0026#39;); ax.set_xlabel(\u0026#39;Subset Size k\u0026#39;,fontsize=14); ax.set_ylabel(\u0026#39;RSS\u0026#39;,fontsize=14); ax.set_title(\u0026#39;RSS on test set\u0026#39;,fontsize=18); ax.plot(tmp_df.numb_features,tmp_df.min_RSS_test,color = \u0026#39;r\u0026#39;, label = \u0026#39;Best subset\u0026#39;); # line of best values ax.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); ax.legend(); fig.suptitle(\u0026#39;Forward Subset Selection\u0026#39;,fontsize=25, y=0.98); fig.subplots_adjust(top=0.8) plt.show() Backward selection #Another alternative to best subset selection is Backward Stepwise Selection.\nBackward Stepwise begins with a model containing all the predictors, and then removes predictors to the model, one at the time. At each step, the variable that gives the least improvement to the fit is removed to the model.\nBackward selection requires that the number of samples \\(n\\) is larger than the number of variables \\(p\\). Instead, Forward selection can be used evene \\(n\u0026lt;p\\).\n9. Perform backward selection Start from the full model Remove at each step the variable that minimizes the RSS (other performance measures can be used) # a flag to print some info, values {0,1} flag = 1 # for short info detailed = 1 ## range variables = data.columns.tolist() # excluding \u0026#39;const\u0026#39; remaining_features, dropped_features_list = variables.copy(), [] ## Initialize the list where we temporarily store data RSS_test_list, min_RSS_test_list, R_squared_list = [], [], [] numb_features, features_list = [], [] # run over the number of variables for k in range(len(variables),0,-1): # initialization best_RSS = np.inf all_RSS, all_R_squared, all_combs = [],[], [] for combo in itertools.combinations(remaining_features,k): # Compute the stats we need tmpComb = list(combo) _, RSS_test, R_squared = LinReg(X_train[tmpComb], Y_train, X_test[tmpComb], Y_test) # we don\u0026#39;t want RSS on training set # store all the RSS all_RSS.append(RSS_test) all_R_squared.append(R_squared) all_combs.append(tmpComb) if RSS_test \u0026lt; best_RSS: best_RSS = RSS_test best_R_squared = R_squared dropped_list = list(set(remaining_features)-set(tmpComb)) # Print some information, before upgrading the features if flag == 1: features = [remaining_features,dropped_features_list] params = [all_RSS,all_R_squared,all_combs] if dropped_list: # only if dropped_feature is not an empty list dropped_feature = dropped_list[0] results = [dropped_feature,best_RSS] else: results = [[],best_RSS] get_info_backwardS(k,features,params,results,detailed) # Updating variables for next loop if dropped_list: # only if dropped_feature is not an empty list dropped_feature = dropped_list[0] remaining_features.remove(dropped_feature) dropped_features_list.append(dropped_feature) else: dropped_features_list.append([]) # at the initial iteration we drop nothing! # Save stats min_RSS_test_list.append(best_RSS) RSS_test_list.append(all_RSS.copy()) R_squared_list.append(best_R_squared.copy()) # Save features and number of features numb_features.append(len(remaining_features)) features_list.append(remaining_features.copy()) # Store in DataFrame df_BackwardS = pd.DataFrame({\u0026#39;numb_features\u0026#39;: numb_features,\\ \u0026#39;RSS_test\u0026#39; : RSS_test_list,\\ \u0026#39;min_RSS_test\u0026#39;: min_RSS_test_list,\\ \u0026#39;R_squared\u0026#39;: R_squared_list,\\ \u0026#39;dropped_feature\u0026#39;: dropped_features_list,\\ \u0026#39;features\u0026#39;: features_list}) ========================================================================================== Iter n.0: At the beginning we have: Remaining features: ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45'] Dropped features: [] Now we compare the model selecting 8 variables Comb n.1: ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45'] ------------------------------------------------------------------------------------------ candidate dropped feature: [] RSS test: 15.638220165228002 At the end we have: min RSS: 15.638220165228002 We drop: [] ========================================================================================== ========================================================================================== Iter n.1: At the beginning we have: Remaining features: ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45'] Dropped features: [[]] Now we compare the model selecting 7 variables Comb n.1: ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['pgg45'] RSS test: 13.492898446056923 Comb n.2: ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'pgg45'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['gleason'] RSS test: 15.495404626758 Comb n.3: ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'gleason', 'pgg45'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['lcp'] RSS test: 13.83423149083128 Comb n.4: ['lcavol', 'lweight', 'age', 'lbph', 'lcp', 'gleason', 'pgg45'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['svi'] RSS test: 17.516627850269806 Comb n.5: ['lcavol', 'lweight', 'age', 'svi', 'lcp', 'gleason', 'pgg45'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['lbph'] RSS test: 14.702111705571852 Comb n.6: ['lcavol', 'lweight', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['age'] RSS test: 17.03455209459629 Comb n.7: ['lcavol', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['lweight'] RSS test: 16.754443499511755 Comb n.8: ['lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['lcavol'] RSS test: 22.63332934025175 At the end we have: min RSS: 13.492898446056923 We drop: pgg45 ========================================================================================== ========================================================================================== Iter n.2: At the beginning we have: Remaining features: ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason'] Dropped features: [[], 'pgg45'] Now we compare the model selecting 6 variables Comb n.1: ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['gleason'] RSS test: 13.43573406046562 Comb n.2: ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'gleason'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['lcp'] RSS test: 12.98960324749752 Comb n.3: ['lcavol', 'lweight', 'age', 'lbph', 'lcp', 'gleason'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['svi'] RSS test: 15.244637591703853 Comb n.4: ['lcavol', 'lweight', 'age', 'svi', 'lcp', 'gleason'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['lbph'] RSS test: 12.690514046312394 Comb n.5: ['lcavol', 'lweight', 'lbph', 'svi', 'lcp', 'gleason'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['age'] RSS test: 14.300531209782973 Comb n.6: ['lcavol', 'age', 'lbph', 'svi', 'lcp', 'gleason'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['lweight'] RSS test: 14.364169507348384 Comb n.7: ['lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['lcavol'] RSS test: 20.7394303261079 At the end we have: min RSS: 12.690514046312394 We drop: lbph ========================================================================================== ========================================================================================== Iter n.3: At the beginning we have: Remaining features: ['lcavol', 'lweight', 'age', 'svi', 'lcp', 'gleason'] Dropped features: [[], 'pgg45', 'lbph'] Now we compare the model selecting 5 variables Comb n.1: ['lcavol', 'lweight', 'age', 'svi', 'lcp'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['gleason'] RSS test: 12.70021855056121 Comb n.2: ['lcavol', 'lweight', 'age', 'svi', 'gleason'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['lcp'] RSS test: 11.497691985782552 Comb n.3: ['lcavol', 'lweight', 'age', 'lcp', 'gleason'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['svi'] RSS test: 14.720875322787679 Comb n.4: ['lcavol', 'lweight', 'svi', 'lcp', 'gleason'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['age'] RSS test: 13.183227370610318 Comb n.5: ['lcavol', 'age', 'svi', 'lcp', 'gleason'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['lweight'] RSS test: 13.23436777636421 Comb n.6: ['lweight', 'age', 'svi', 'lcp', 'gleason'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['lcavol'] RSS test: 16.98150806110247 At the end we have: min RSS: 11.497691985782552 We drop: lcp ========================================================================================== ========================================================================================== Iter n.4: At the beginning we have: Remaining features: ['lcavol', 'lweight', 'age', 'svi', 'gleason'] Dropped features: [[], 'pgg45', 'lbph', 'lcp'] Now we compare the model selecting 4 variables Comb n.1: ['lcavol', 'lweight', 'age', 'svi'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['gleason'] RSS test: 11.6363032699816 Comb n.2: ['lcavol', 'lweight', 'age', 'gleason'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['svi'] RSS test: 14.030539349222018 Comb n.3: ['lcavol', 'lweight', 'svi', 'gleason'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['age'] RSS test: 11.956020656419863 Comb n.4: ['lcavol', 'age', 'svi', 'gleason'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['lweight'] RSS test: 11.612572746859495 Comb n.5: ['lweight', 'age', 'svi', 'gleason'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['lcavol'] RSS test: 18.093182817451503 At the end we have: min RSS: 11.612572746859495 We drop: lweight ========================================================================================== ========================================================================================== Iter n.5: At the beginning we have: Remaining features: ['lcavol', 'age', 'svi', 'gleason'] Dropped features: [[], 'pgg45', 'lbph', 'lcp', 'lweight'] Now we compare the model selecting 3 variables Comb n.1: ['lcavol', 'age', 'svi'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['gleason'] RSS test: 11.71416952499475 Comb n.2: ['lcavol', 'age', 'gleason'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['svi'] RSS test: 14.18793001209106 Comb n.3: ['lcavol', 'svi', 'gleason'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['age'] RSS test: 11.484037587414818 Comb n.4: ['age', 'svi', 'gleason'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['lcavol'] RSS test: 19.78197534246865 At the end we have: min RSS: 11.484037587414818 We drop: age ========================================================================================== ========================================================================================== Iter n.6: At the beginning we have: Remaining features: ['lcavol', 'svi', 'gleason'] Dropped features: [[], 'pgg45', 'lbph', 'lcp', 'lweight', 'age'] Now we compare the model selecting 2 variables Comb n.1: ['lcavol', 'svi'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['gleason'] RSS test: 11.58358360007023 Comb n.2: ['lcavol', 'gleason'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['svi'] RSS test: 14.140851461764317 Comb n.3: ['svi', 'gleason'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['lcavol'] RSS test: 18.710131293690928 At the end we have: min RSS: 11.58358360007023 We drop: gleason ========================================================================================== ========================================================================================== Iter n.7: At the beginning we have: Remaining features: ['lcavol', 'svi'] Dropped features: [[], 'pgg45', 'lbph', 'lcp', 'lweight', 'age', 'gleason'] Now we compare the model selecting 1 variables Comb n.1: ['lcavol'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['svi'] RSS test: 14.392161587304827 Comb n.2: ['svi'] ------------------------------------------------------------------------------------------ candidate dropped feature: ['lcavol'] RSS test: 20.632078139876853 At the end we have: min RSS: 14.392161587304827 We drop: svi ========================================================================================== # look at the result df_BackwardS numb_features RSS_test min_RSS_test R_squared dropped_feature features 0 8 [15.638220165228002] 15.638220 0.694371 [] [lcavol, lweight, age, lbph, svi, lcp, gleason... 1 7 [13.492898446056923, 15.495404626758, 13.83423... 13.492898 0.678457 pgg45 [lcavol, lweight, age, lbph, svi, lcp, gleason] 2 6 [13.43573406046562, 12.98960324749752, 15.2446... 12.690514 0.656353 lbph [lcavol, lweight, age, svi, lcp, gleason] 3 5 [12.70021855056121, 11.497691985782552, 14.720... 11.497692 0.645101 lcp [lcavol, lweight, age, svi, gleason] 4 4 [11.6363032699816, 14.030539349222018, 11.9560... 11.612573 0.561456 lweight [lcavol, age, svi, gleason] 5 3 [11.71416952499475, 14.18793001209106, 11.4840... 11.484038 0.561005 age [lcavol, svi, gleason] 6 2 [11.58358360007023, 14.140851461764317, 18.710... 11.583584 0.560532 gleason [lcavol, svi] 7 1 [14.392161587304827, 20.632078139876853] 14.392162 0.537516 svi [lcavol] Plot for Backward Selection ## Initialize the figure width = 6 height = 6 nfig = 1 fig = plt.figure(figsize = (width*nfig,height)) # 1. RSS Test set plot tmp_df = df_BackwardS; ax = fig.add_subplot(1, nfig, 1) for i in range(0,len(tmp_df.RSS_test)): ax.scatter([tmp_df.numb_features[i]]*(len(tmp_df.RSS_test[i])),tmp_df.RSS_test[i], alpha = .2, color = \u0026#39;darkblue\u0026#39;); ax.set_xlabel(\u0026#39;Subset Size k\u0026#39;,fontsize=14); ax.set_ylabel(\u0026#39;RSS\u0026#39;,fontsize=14); ax.set_title(\u0026#39;RSS on test set\u0026#39;,fontsize=18); ax.plot(tmp_df.numb_features,tmp_df.min_RSS_test,color = \u0026#39;r\u0026#39;, label = \u0026#39;Best subset\u0026#39;); # line of best values ax.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); ax.legend(); fig.suptitle(\u0026#39;Backward Subset Selection\u0026#39;,fontsize=25, y=0.98); fig.subplots_adjust(top=0.8) plt.show() Comparison subset selection methods #Comparison between Best, Forward and Backward Selction results on test set\n# Initialize the figure width = 6 height = 6 nfig = 3 fig = plt.figure(figsize = (width*nfig,height)) # 1. BEST SUBSET SELECTION tmp_df1 = df_BestS; # scatter plot tmp_df2 = df_BestS_RSS_test; # plot the line of best values ax = fig.add_subplot(1, nfig, 1) ax.scatter(tmp_df1.numb_features,tmp_df1.RSS_test, alpha = .2, color = \u0026#39;darkblue\u0026#39;); ax.set_xlabel(\u0026#39;Subset Size k\u0026#39;,fontsize=14); ax.set_ylabel(\u0026#39;RSS\u0026#39;,fontsize=14); ax.set_title(\u0026#39;Best Selection\u0026#39;,fontsize=18); ax.plot(tmp_df2.numb_features,tmp_df2.RSS_test,color = \u0026#39;r\u0026#39;, label = \u0026#39;Best subset\u0026#39;); # line of best values ax.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); ax.legend(); # 2. FORWARD SUBSET SELECTION tmp_df = df_ForwardS; ax = fig.add_subplot(1, nfig, 2) for i in range(0,len(tmp_df.RSS_test)): ax.scatter([tmp_df.numb_features[i]]*(len(tmp_df.RSS_test[i])),tmp_df.RSS_test[i], alpha = .2, color = \u0026#39;darkblue\u0026#39;); ax.set_xlabel(\u0026#39;Subset Size k\u0026#39;,fontsize=14); ax.set_ylabel(\u0026#39;RSS\u0026#39;,fontsize=14); ax.set_title(\u0026#39;Forward Selection\u0026#39;,fontsize=18); ax.plot(tmp_df.numb_features,tmp_df.min_RSS_test,color = \u0026#39;r\u0026#39;, label = \u0026#39;Best subset\u0026#39;); # line of best values ax.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); ax.legend(); # 3. BACKWARD SUBSET SELECTION tmp_df = df_BackwardS; ax = fig.add_subplot(1, nfig, 3) for i in range(0,len(tmp_df.RSS_test)): ax.scatter([tmp_df.numb_features[i]]*(len(tmp_df.RSS_test[i])),tmp_df.RSS_test[i], alpha = .2, color = \u0026#39;darkblue\u0026#39;); ax.set_xlabel(\u0026#39;Subset Size k\u0026#39;,fontsize=14); ax.set_ylabel(\u0026#39;RSS\u0026#39;,fontsize=14); ax.set_title(\u0026#39;Backward Selection\u0026#39;,fontsize=18); ax.plot(tmp_df.numb_features,tmp_df.min_RSS_test,color = \u0026#39;r\u0026#39;, label = \u0026#39;Best subset\u0026#39;); # line of best values ax.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); ax.legend(); fig.suptitle(\u0026#39;Comparison Subset Selection\u0026#39;,fontsize=25, y=0.98); fig.subplots_adjust(top=0.8) plt.show() # Initialize the figure width = 6 height = 6 nfig = 1 fig = plt.figure(figsize = (width*nfig,height)) ax = fig.add_subplot(1, nfig, 1) # Best Selection ax.plot(df_BestS_RSS_test.numb_features,df_BestS_RSS_test.RSS_test,color = \u0026#39;r\u0026#39;, label = \u0026#39;Best subset\u0026#39;); # line of best values # Forward Selection ax.plot(df_ForwardS.numb_features,df_ForwardS.min_RSS_test,color = \u0026#39;b\u0026#39;, label = \u0026#39;Best subset\u0026#39;); # line of best values # Backward Selection ax.plot(df_BackwardS.numb_features,df_BackwardS.min_RSS_test,color = \u0026#39;g\u0026#39;, label = \u0026#39;Best subset\u0026#39;); # line of best values ax.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); ax.set_xlabel(\u0026#39;Subset Size k\u0026#39;,fontsize=14); ax.set_ylabel(\u0026#39;RSS\u0026#39;,fontsize=14); ax.set_title(\u0026#39;Comparison minimum RSS\u0026#39;,fontsize=18); ax.legend([\u0026#39;best\u0026#39;,\u0026#39;forward\u0026#39;,\u0026#39;backward\u0026#39;]) plt.show(); Best method from Subset Selection We see below the results of Best, Forward and Backward Subset Selection.\ndf_range = [df_BestS_RSS_test, df_ForwardS, df_BackwardS] columns_range = [\u0026#39;RSS_test\u0026#39;,\u0026#39;min_RSS_test\u0026#39;,\u0026#39;min_RSS_test\u0026#39;] methods = [\u0026#39;Best Selection\u0026#39;, \u0026#39;Forward Selection\u0026#39;, \u0026#39;Backward Selection\u0026#39;] for df, col, meth in zip(df_range,columns_range,methods): idx = df[col].idxmin() print(f\u0026#34;\\nFor {meth} the best method has:\\n\\ n. features: {df[\u0026#39;numb_features\u0026#39;][idx]}\\n\\ features: {df[\u0026#39;features\u0026#39;][idx]}\\n\\ RSS test: {df[col][idx]}\\n\u0026#34;) For Best Selection the best method has: n. features: 3 features: ('lcavol', 'svi', 'gleason') RSS test: 11.484037587414818 For Forward Selection the best method has: n. features: 3 features: ['lcavol', 'svi', 'gleason'] RSS test: 11.484037587414818 For Backward Selection the best method has: n. features: 3 features: ['lcavol', 'svi', 'gleason'] RSS test: 11.484037587414818 Backward selection with Z-score #11. Perform backward selection using the z-score as a statistics for selecting the predictor to drop * Start from the full model * Remove at each step the variable having the smallest Z-score (which library is more suitable for this purpose?) # a flag to print some info, values {0,1} flag = 1 # for short info ## range variables = data.columns.tolist() # excluding \u0026#39;const\u0026#39; remaining_features = variables.copy() tmpComb = remaining_features.copy() dropped_features_list= [] ## Initialize the list where we temporarily store data RSS_test_list, R_squared_list = [], [] numb_features, features_list = [], [] # Loop over the number of variables for k in range(len(variables),0,-1): # Compute the stats we need Zscores, minZ, idx_minZ = Zscore(X_train[tmpComb],Y_train) _, RSS_test, R_squared = LinReg(X_train[tmpComb], Y_train, X_test[tmpComb], Y_test) # Save stats RSS_test_list.append(RSS_test) R_squared_list.append(best_R_squared.copy()) # Print some information, before upgrading the features if flag == 1: features = [remaining_features,dropped_features_list] params = [RSS_test,R_squared_list,tmpComb] if dropped_list: # only if dropped_feature is not an empty list dropped_feature = dropped_list[0] results = [idx_minZ,RSS_test] else: results = [RSS_test,[]] get_info_backwardS_Zscore(k,features,params,results,detailed) # Save features and number of features numb_features.append(k) features_list.append(tmpComb.copy()) # update combinations tmpComb.remove(idx_minZ) dropped_list = list(set(remaining_features)-set(tmpComb)) # Updating variables for next loop if dropped_list: # only if dropped_feature is not an empty list dropped_feature = dropped_list[0] remaining_features.remove(dropped_feature) dropped_features_list.append(dropped_feature) else: dropped_features_list.append([]) # at the initial iteration we drop nothing! # Store in DataFrame df_BackwardS_minZ = pd.DataFrame({\u0026#39;numb_features\u0026#39;: numb_features,\\ \u0026#39;RSS_test\u0026#39; : RSS_test_list,\\ \u0026#39;R_squared\u0026#39;: R_squared_list,\\ \u0026#39;dropped_feature\u0026#39;: dropped_features_list,\\ \u0026#39;features\u0026#39;: features_list}) ========================================================================================== Iter n.0: At the beginning we have: Remaining features: ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45'] Dropped features: [] The Z-scores are: lcavol 5.366290 lweight 2.750789 age -1.395909 lbph 2.055846 svi 2.469255 lcp -1.866913 gleason -0.146681 pgg45 1.737840 dtype: float64 At the end we have: min RSS: 15.638220165228002 We drop: gleason ========================================================================================== ========================================================================================== Iter n.1: At the beginning we have: Remaining features: ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'pgg45'] Dropped features: ['gleason'] The Z-scores are: lcavol 5.462426 lweight 2.833132 age -1.486490 lbph 2.068796 svi 2.519204 lcp -1.877253 pgg45 2.182013 dtype: float64 At the end we have: min RSS: 15.495404626758 We drop: age ========================================================================================== ========================================================================================== Iter n.2: At the beginning we have: Remaining features: ['lcavol', 'lweight', 'lbph', 'svi', 'lcp', 'pgg45'] Dropped features: ['gleason', 'age'] The Z-scores are: lcavol 5.243670 lweight 2.589758 lbph 1.815545 svi 2.544601 lcp -1.733570 pgg45 1.871654 dtype: float64 At the end we have: min RSS: 16.457800398803407 We drop: lcp ========================================================================================== ========================================================================================== Iter n.3: At the beginning we have: Remaining features: ['lcavol', 'lweight', 'lbph', 'svi', 'pgg45'] Dropped features: ['gleason', 'age', 'lcp'] The Z-scores are: lcavol 4.899985 lweight 2.551974 lbph 1.952740 svi 2.039752 pgg45 1.190849 dtype: float64 At the end we have: min RSS: 14.577726321419373 We drop: pgg45 ========================================================================================== ========================================================================================== Iter n.4: At the beginning we have: Remaining features: ['lcavol', 'lweight', 'lbph', 'svi'] Dropped features: ['gleason', 'age', 'lcp', 'pgg45'] The Z-scores are: lcavol 5.461353 lweight 2.441316 lbph 1.988469 svi 2.458931 dtype: float64 At the end we have: min RSS: 13.689963661204882 We drop: lbph ========================================================================================== ========================================================================================== Iter n.5: At the beginning we have: Remaining features: ['lcavol', 'lweight', 'svi'] Dropped features: ['gleason', 'age', 'lcp', 'pgg45', 'lbph'] The Z-scores are: lcavol 5.507417 lweight 3.655671 svi 1.985388 dtype: float64 At the end we have: min RSS: 12.015924403078804 We drop: svi ========================================================================================== ========================================================================================== Iter n.6: At the beginning we have: Remaining features: ['lcavol', 'lweight'] Dropped features: ['gleason', 'age', 'lcp', 'pgg45', 'lbph', 'svi'] The Z-scores are: lcavol 7.938277 lweight 3.582135 dtype: float64 At the end we have: min RSS: 14.77447043041511 We drop: lweight ========================================================================================== ========================================================================================== Iter n.7: At the beginning we have: Remaining features: ['lcavol'] Dropped features: ['gleason', 'age', 'lcp', 'pgg45', 'lbph', 'svi', 'lweight'] The Z-scores are: lcavol 8.691694 dtype: float64 At the end we have: min RSS: 14.392161587304827 We drop: lcavol ========================================================================================== df_BackwardS_minZ numb_features RSS_test R_squared dropped_feature features 0 8 15.638220 0.537516 gleason [lcavol, lweight, age, lbph, svi, lcp, gleason... 1 7 15.495405 0.537516 age [lcavol, lweight, age, lbph, svi, lcp, pgg45] 2 6 16.457800 0.537516 lcp [lcavol, lweight, lbph, svi, lcp, pgg45] 3 5 14.577726 0.537516 pgg45 [lcavol, lweight, lbph, svi, pgg45] 4 4 13.689964 0.537516 lbph [lcavol, lweight, lbph, svi] 5 3 12.015924 0.537516 svi [lcavol, lweight, svi] 6 2 14.774470 0.537516 lweight [lcavol, lweight] 7 1 14.392162 0.537516 lcavol [lcavol] 12. Generate a chart having * x-axis: the subset size * y-axis: the RSS for the test set of the models generated at step 11 * Compare it with the chart generated at point 10. # Initialize the figure width = 6 height = 6 nfig = 1 fig = plt.figure(figsize = (width*nfig,height)) # 1. RSS Test set plot tmp_df = df_BackwardS_minZ; ax = fig.add_subplot(1, nfig, 1) ax.scatter(tmp_df.numb_features,tmp_df.RSS_test, alpha = .2, color = \u0026#39;darkblue\u0026#39;); ax.set_xlabel(\u0026#39;Subset Size k\u0026#39;,fontsize=14); ax.set_ylabel(\u0026#39;RSS\u0026#39;,fontsize=14); ax.set_title(\u0026#39;RSS on test set\u0026#39;,fontsize=18); ax.plot(tmp_df.numb_features,tmp_df.RSS_test,color = \u0026#39;r\u0026#39;, label = \u0026#39;Best subset\u0026#39;); # line of best values ax.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5); ax.legend(); fig.suptitle(\u0026#39;Backward Subset Selection\u0026#39;,fontsize=25, y=0.98); fig.subplots_adjust(top=0.85) plt.show() Comparison with Z-score #Now compare the results by choosing the best model not by minimizing RSS but maximizing Zscore.\ndf_range = [df_BestS_RSS_test, df_ForwardS, df_BackwardS, df_BackwardS_minZ] columns_range = [\u0026#39;RSS_test\u0026#39;,\u0026#39;min_RSS_test\u0026#39;,\u0026#39;min_RSS_test\u0026#39;,\u0026#39;RSS_test\u0026#39;] methods = [\u0026#39;Best Selection - RSS test\u0026#39;, \u0026#39;Forward Selection - RSS test\u0026#39;, \u0026#39;Backward Selection - RSS test\u0026#39;, \u0026#39;Backward Selection - Z score\u0026#39;] for df, col, meth in zip(df_range,columns_range,methods): idx = df[col].idxmin() print(f\u0026#34;\\nFor {meth}, the best method has:\\n\\ n. features: {df[\u0026#39;numb_features\u0026#39;][idx]}\\n\\ features: {df[\u0026#39;features\u0026#39;][idx]}\\n\\ RSS test: {df[col][idx]}\\n\u0026#34;) For Best Selection - RSS test, the best method has: n. features: 3 features: ('lcavol', 'svi', 'gleason') RSS test: 11.484037587414818 For Forward Selection - RSS test, the best method has: n. features: 3 features: ['lcavol', 'svi', 'gleason'] RSS test: 11.484037587414818 For Backward Selection - RSS test, the best method has: n. features: 3 features: ['lcavol', 'svi', 'gleason'] RSS test: 11.484037587414818 For Backward Selection - Z score, the best method has: n. features: 3 features: ['lcavol', 'lweight', 'svi'] RSS test: 12.015924403078804 ","date":null,"permalink":"/posts/some-exercise-about-statistical-learning/sl-ex3-subsetselection/","section":"Posts","summary":"","title":"SL3: Analysis of Prostate Cancer dataset – variable subset selection"},{"content":" Aim of the analysis We want to examinate correlation between the level of prostate-specific antigen and a number of clinical parameters in men, who are about to receive a prostatectomy. # data analysis and wrangling import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # import random as rnd # visualization import seaborn as sns import matplotlib.pyplot as plt # %matplotlib inline # machine learning from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score from sklearn.metrics import mean_squared_error import statsmodels.api as sm from IPython.display import Image # to visualize images from tabulate import tabulate # to create tables import os for dirname, _, filenames in os.walk(\u0026#39;/kaggle/input\u0026#39;): for filename in filenames: print(os.path.join(dirname, filename)) /kaggle/input/prostate-data/prostate.data /kaggle/input/prostate-data/tab.png /kaggle/input/prostate-data/tab2.png 1. Open the webpage of the book “The Elements of Statistical Learning”, go to the “Data” section and download the info and data files for the dataset called Prostate 2. Open the file `prostate.info.txt` Hint: please, refer also to Section 3.2.1 (page 49) of the book “The Elements of Statistical Learning” to gather this information\nHow many predictors are present in the dataset? What are those names?\nThere are 8 predictors:\n1. lcavol (log cancer volume)\n2. lweight (log prostate weight)\n3. age\n4. lbph (log of the amount of benign prostatic hyperplasia)\n5. svi (seminal veiscle invasion)\n6. lcp (log of capsular penetration)\n7. gleason (Gleason score)\n8. pgg45 (percent of Gleason scores 4 or 5)\nHow many responses are present in the dataset? What are their names?\nThere is one response: 1. lpsa (log of prostate-specific antigen)\nHow did the authors split the dataset in training and test set?\nThey randomly split the dataset (containing 97 observations, in prostate.data) into a training set of size 67 and a test set of size 30. In the file prostate.data there is a column train which is of boolean type in order to distinguish if an observation is used (T) or not (F) to train the model.\n3. Open the file `prostate.data` by a text editor or a spreadsheet and have a quick look at the data How many observations are present? There are 97 observations in total.\nWhich is the symbol used to separate the columns? To separate the columns there is the escape character \\t tab.\n4. Open Kaggle, generate a new kernel and give it the name “SL_EX2_ProstateCancer_Surname” 5. Add the dataset `prostate.data` to the kernel Hint: See the Add Dataset button on the right Hint: use import option “Convert tabular files to csv” 6. Run the first cell of the kernel to check if the data file is present in folder ../input 7. Add to the first cell new lines to load the following libraries: seaborn, matplotlib.pyplot, sklearn.linear_model.LinearRegression Tip: We import also pandas. 8. Add a Markdown cell on top of the notebook, copy and paste in it the text of this exercise and provide in the same cell the answers to the questions that you get step-by-step. 9. Load the Prostate Cancer dataset into a Pandas DataFrame variable called \"data\" How can you say Python to use the right separator between columns? I need to specify sep='\\t into read_csv method in order to load the dataset.\nData acquisition ## Load the Prostate Cancer dataset data = pd.read_csv(\u0026#39;../input/prostate-data/prostate.data\u0026#39;,sep=\u0026#39;\\t\u0026#39;) # data.info() # to check if it is correct 10. Display the number of rows and columns of variable data [num_rows,num_columns]=data.shape; print(f\u0026#34;The number of rows is {num_rows} and the number of columns is {num_columns}.\u0026#34;) The number of rows is 97 and the number of columns is 11. 11. Show the first 5 rows of the dataset data.head(5) Unnamed: 0 lcavol lweight age lbph svi lcp gleason pgg45 lpsa train 0 1 -0.579818 2.769459 50 -1.386294 0 -1.386294 6 0 -0.430783 T 1 2 -0.994252 3.319626 58 -1.386294 0 -1.386294 6 0 -0.162519 T 2 3 -0.510826 2.691243 74 -1.386294 0 -1.386294 7 20 -0.162519 T 3 4 -1.203973 3.282789 58 -1.386294 0 -1.386294 6 0 -0.162519 T 4 5 0.751416 3.432373 62 -1.386294 0 -1.386294 6 0 0.371564 T 12. Remove the first column of the dataset which contains observation indices print(\u0026#34;* Before to drop the first column:\u0026#34;) data.info() #data1 = data1.drop(columns=\u0026#39;Unnamed: 0\u0026#39;) #data1 = data1.drop(labels=[\u0026#39;Unnamed: 0\u0026#39;],axis=1) print(\u0026#34;\\n* After having dropped the first column:\u0026#34;) data = data.drop(data.columns[0],axis=1) # without specifying the name of the variable (axis=0 indicates rows, axis=1 indicates columns) data.info() data[\u0026#39;train\u0026#39;].value_counts() * Before to drop the first column: \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 97 entries, 0 to 96 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Unnamed: 0 97 non-null int64 1 lcavol 97 non-null float64 2 lweight 97 non-null float64 3 age 97 non-null int64 4 lbph 97 non-null float64 5 svi 97 non-null int64 6 lcp 97 non-null float64 7 gleason 97 non-null int64 8 pgg45 97 non-null int64 9 lpsa 97 non-null float64 10 train 97 non-null object dtypes: float64(5), int64(5), object(1) memory usage: 8.5+ KB * After having dropped the first column: \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 97 entries, 0 to 96 Data columns (total 10 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 lcavol 97 non-null float64 1 lweight 97 non-null float64 2 age 97 non-null int64 3 lbph 97 non-null float64 4 svi 97 non-null int64 5 lcp 97 non-null float64 6 gleason 97 non-null int64 7 pgg45 97 non-null int64 8 lpsa 97 non-null float64 9 train 97 non-null object dtypes: float64(5), int64(4), object(1) memory usage: 7.7+ KB T 67 F 30 Name: train, dtype: int64 Warning: Keep attention to do not run the above cell twice. Otherwise it will drop again the first column. 13. Save column train in a new variable called \"train\" and having type `Series` (the Pandas data structure used to represent DataFrame columns), then drop the column train from the data DataFrame # Save \u0026#34;train\u0026#34; column in a Pandas Series variable train = data[\u0026#39;train\u0026#39;] # train = pd.Series(data[\u0026#39;train\u0026#39;]) # Drop \u0026#34;train\u0026#34; variable from data data = data.drop(columns=[\u0026#39;train\u0026#39;]) Warning: Keep attention to do not run the above cell twice. In this case you have already dropped \u0026rsquo;train'. 14. Save column lpsa in a new variable called \"lpsa\" and having type `Series` (the Pandas data structure used to represent DataFrame columns), then drop the column lpsa from the data DataFrame and save the result in a new DataFrame called predictors How many predictors are available? There are 8 predictors available for each one of the 97 observations.\n# Save \u0026#34;lpsa\u0026#34; column in a Pandas Series variable lpsa = data[\u0026#39;lpsa\u0026#39;] # lpsa = pd.Series(data[\u0026#39;lpsa\u0026#39;]) # Drop \u0026#34;train\u0026#34; variable from data data = data.drop(columns=[\u0026#39;lpsa\u0026#39;]) predictors = data predictors.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 97 entries, 0 to 96 Data columns (total 8 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 lcavol 97 non-null float64 1 lweight 97 non-null float64 2 age 97 non-null int64 3 lbph 97 non-null float64 4 svi 97 non-null int64 5 lcp 97 non-null float64 6 gleason 97 non-null int64 7 pgg45 97 non-null int64 dtypes: float64(4), int64(4) memory usage: 6.2 KB 15. Check the presence of missing values in the `data` variable Since all the columns are numerical variables, we have not to distinguish variables between numerical and categorical kind. How many missing values are there? In which columns? We have no missing values.\nWhich types do the variable have? lcavol,lweight,lbph andlcp are float64, while age,svi,gleason and pgg45 are int64\nprint(\u0026#34;For each variable in data, we have no missing values:\\n\\n\u0026#34;,data.isna().sum()) For each variable in data, we have no missing values: lcavol 0 lweight 0 age 0 lbph 0 svi 0 lcp 0 gleason 0 pgg45 0 dtype: int64 16. Show histograms of all variables in a single figure Use argument figsize to enlarge the figure if needed fig = plt.figure() predictors.hist(grid=True, figsize=(20,8), layout = (2,4)) fig.tight_layout() plt.suptitle(\u0026#34;Not Standardized data\u0026#34;, fontsize=25) fig.show() \u0026lt;Figure size 432x288 with 0 Axes\u0026gt; 17. Show the basic statistics (min, max, mean, quartiles, etc. for each variable) in data data.describe() lcavol lweight age lbph svi lcp gleason pgg45 count 97.000000 97.000000 97.000000 97.000000 97.000000 97.000000 97.000000 97.000000 mean 1.350010 3.628943 63.865979 0.100356 0.216495 -0.179366 6.752577 24.381443 std 1.178625 0.428411 7.445117 1.450807 0.413995 1.398250 0.722134 28.204035 min -1.347074 2.374906 41.000000 -1.386294 0.000000 -1.386294 6.000000 0.000000 25% 0.512824 3.375880 60.000000 -1.386294 0.000000 -1.386294 6.000000 0.000000 50% 1.446919 3.623007 65.000000 0.300105 0.000000 -0.798508 7.000000 15.000000 75% 2.127041 3.876396 68.000000 1.558145 0.000000 1.178655 7.000000 40.000000 max 3.821004 4.780383 79.000000 2.326302 1.000000 2.904165 9.000000 100.000000 18. Generate a new DataFrame called dataTrain and containing only the rows of data in which the train variable has value “T” Hint: use the loc attribute of DataFrame to access a groups of rows and columns by label(s) or boolean arrays How many rows and columns does dataTrain have? dataTrain = data.loc[train == \u0026#39;T\u0026#39;] # Obviously, len(idx)==len(dataTrain) is True! # # Alternative way: # # 1. Get the indexes corresponding to train == \u0026#39;T\u0026#39; # idxTrain = train.loc[train == \u0026#39;T\u0026#39;].index.tolist() # # 2. Access to interesting rows with .iloc() # dataTrain = data.iloc[idxTrain] print(f\u0026#34;dataTrain contains {dataTrain.shape[0]} rows and {dataTrain.shape[1]} columns.\u0026#34;) dataTrain contains 67 rows and 8 columns. 19. Generate a new DataFrame called dataTest and containing only the rows of data in which the train variable has value “F” How many rows and columns does dataTest have? dataTest = data.loc[train == \u0026#39;F\u0026#39;] 20. Generate a new Series called lpsaTrain and containing only the values of variable lpsa in which the train variable has value “T” How many valuses does lpsaTrain have? # Create a new Series variable lpsaTrain = lpsa.loc[train == \u0026#39;T\u0026#39;] # # Another way to define it # data_all = pd.read_csv(\u0026#39;../input/prostate-data/prostate.data\u0026#39;,sep=\u0026#39;\\t\u0026#39;) # lpsaTrain = data_all.loc[train == \u0026#39;T\u0026#39;][\u0026#39;lpsa\u0026#39;] # # To check if it is correct: # idxTrain = train.loc[train == \u0026#39;T\u0026#39;].index.tolist() # lpsaTrain == lpsa.iloc[idxTrain] print(f\u0026#34;lpsaTrain has {lpsaTrain.shape[0]} values.\u0026#34;) lpsaTrain has 67 values. 21. Generate a new Series called lpsaTest and containing only the values of variable lpsa in which the train variable has value “F” How many valuses does lpsaTest have? lpsaTest = lpsa.loc[train == \u0026#39;F\u0026#39;] # # To check if it is correct: # len(lpsaTest) == len(data)-len(lpsaTrain) print(f\u0026#34;lpsaTrain has {lpsaTest.shape[0]} values.\u0026#34;) lpsaTrain has 30 values. 22. Show the correlation matrix among all the variables in dataTrain Hint: use the correct method in DataFrame Hint: check if the values in the matrix correspond to those in Table 3.1 of the book # Create correlation matrix corrM = dataTrain.corr().round(decimals = 3) # As in the book, I plot values up to 3 decimals # Display only the lower diagonal correlation matrix lowerM = np.tril(np.ones(corrM.shape), k=-1) # Lower matrix of ones. (for k=0 I include also the main diagonal) cond = lowerM.astype(bool) # Create a matrix of false, except in lowerM corrM = corrM.where(cond, other=\u0026#39;\u0026#39;) # .where() replaces values with other where the condition is False. corrM lcavol lweight age lbph svi lcp gleason pgg45 lcavol lweight 0.3 age 0.286 0.317 lbph 0.063 0.437 0.287 svi 0.593 0.181 0.129 -0.139 lcp 0.692 0.157 0.173 -0.089 0.671 gleason 0.426 0.024 0.366 0.033 0.307 0.476 pgg45 0.483 0.074 0.276 -0.03 0.481 0.663 0.757 # We can compare the above correlation matrix with the one in the book: Image(\u0026#34;../input/prostate-data/tab.png\u0026#34;) 23. Drop the column lpsa from the `dataTrain` DataFrame and save the result in a new DataFrame called `predictorsTrain` Warning: I can not drop lpsa from dataTrain, because I have already done it! In fact:\nat step 14. I dropped lpsa from data at step 18. I created dataTrain from data, by selecting certain rows. So at this step dataTrain does not contain lpsa. # predictorsTrain = dataTrain.drop[\u0026#39;lpsa\u0026#39;] predictorsTrain = dataTrain 24. Drop the column `lpsa` from the dataTest DataFrame and save the result in a new DataFrame called `predictorsTest` dataTest.columns.tolist() predictorsTest = dataTest Warning: I can not drop lpsa from dataTest, because I have already done it! In fact:\nat step 14. I dropped lpsa from data at step 19. I created dataTest from data, by selecting certain rows. So at this step dataTest does not contain lpsa. 25. Generate a new DataFrame called `predictorsTrain_std` and containing the standardized variables of DataFrame `predictorsTrain` Hint: compute the mean of each column and save them in variable predictorsTrainMeans Hint: compute the standard deviation of each column and save them in variable predictorsTrainStds Hint: compute the standardization of each variable by the formula: \\\\[\\\\frac{predictorsTrain - predictorsTrainMeans}{predictorsTrainStd}\\\\] predictorsTrainMeans = predictorsTrain.mean() predictorsTrainStds = predictorsTrain.std() predictorsTrain_std = (predictorsTrain - predictorsTrainMeans)/predictorsTrainStds # standardized cariables of predictorTrain predictorsTrain_std # Standardizing makes it easier to compare scores, even if those scores were measured on different scales. # It also makes it easier to read results from regression analysis and ensures that all variables contribute to a scale when added together. lcavol lweight age lbph svi lcp gleason pgg45 0 -1.523680 -1.797414 -1.965590 -0.995955 -0.533063 -0.836769 -1.031712 -0.896487 1 -1.857204 -0.643057 -0.899238 -0.995955 -0.533063 -0.836769 -1.031712 -0.896487 2 -1.468157 -1.961526 1.233468 -0.995955 -0.533063 -0.836769 0.378996 -0.213934 3 -2.025981 -0.720349 -0.899238 -0.995955 -0.533063 -0.836769 -1.031712 -0.896487 4 -0.452342 -0.406493 -0.366061 -0.995955 -0.533063 -0.836769 -1.031712 -0.896487 ... ... ... ... ... ... ... ... ... 90 1.555621 0.998130 0.433703 -0.995955 -0.533063 -0.836769 -1.031712 -0.896487 91 0.981346 0.107969 -0.499355 0.872223 1.847952 -0.836769 0.378996 -0.384573 92 1.220657 0.525153 0.433703 -0.995955 1.847952 1.096538 0.378996 1.151171 93 2.017972 0.568193 -2.765355 -0.995955 1.847952 1.701433 0.378996 0.468618 95 1.262743 0.310118 0.433703 1.015748 1.847952 1.265298 0.378996 1.833724 67 rows × 8 columns\n26. Show the histogram of each variables of predictorsTrain_std in a single figure Use argument figsize to enlarge the figure if needed Hint: which kind of difference can you see in the histograms? print(\u0026#34;Now all the variables are centered at 0 and they variance equal to 1. So we can compare them in a better way.\u0026#34;) fig = plt.figure() predictorsTrain_std.hist(grid=True, figsize=(20,8), layout = (2,4)) plt.suptitle(\u0026#34;Standardized data\u0026#34;, fontsize=25) fig.tight_layout() plt.show() Now all the variables are centered at 0 and they variance equal to 1. So we can compare them in a better way. \u0026lt;Figure size 432x288 with 0 Axes\u0026gt; Linear Regression #27. Generate a linear regression model using `predictorsTrain_std` as dependent variables and `lpsaTrain` as independent variable Hint: find a function for linear regression model learning in sklearn (fit)\nHow do you set parameter fit_intercept? Why?\nThe parameter fit_intercept specifies whether to calculate the intercept for this model:\nIf False, then the y-intercept to 0 (it is forced to 0); if True, then the y-intercept will be determined by the line of best fit (it\u0026rsquo;s allowed to \u0026ldquo;fit\u0026rdquo; the y-axis). As default fit_intercept is True and it is good for us.\nHow do you set parameter normalize? Why? Can this parameter be used to simplify the generation of the predictor matrix?\nIf fit_intercept = False, then the parameter normalize is ignored. If normalize = True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm.\nBy default normalize = False.\nWe have already standardized our variables, so we set it as False.\n# Create X_test #predictorsTestMean = predictorsTest.mean() predictorsTestStds = predictorsTest.std() #predictorsTest_std = (predictorsTest - predictorsTestMeans)/predictorsTestStds # standardized cariables of predictorTrain predictorsTest_std = (predictorsTest - predictorsTrainMeans)/predictorsTrainStds # standardized cariables of predictorTrain (BETTER WAY TO DO IT) # Prepare the independent and dependent variables for the model # Independent variables X_train = predictorsTrain_std X_test = predictorsTest_std # Dependent variable y_train = lpsaTrain y_test = lpsaTest linreg = LinearRegression() # we don\u0026#39;t need to specify args, because the default ones are already good for us linreg.fit(X_train,y_train) # by default: fit_intercept = True and normalize = False # This setting is good because we want to compute the intercept and we don\u0026#39;t need to normalize X because we have already done it LinearRegression()\nDifference in setting up fit_intercept in LinearRegression() ## Difference in setting up fit_intercept lr_fi_true = LinearRegression(fit_intercept=True) lr_fi_false = LinearRegression(fit_intercept=False) lr_fi_true.fit(X_train, y_train) lr_fi_false.fit(X_train, y_train) print(\u0026#39;Intercept when fit_intercept=True : {:.5f}\u0026#39;.format(lr_fi_true.intercept_)) print(\u0026#39;Intercept when fit_intercept=False : {:.5f}\u0026#39;.format(lr_fi_false.intercept_)) # FIGURE # SOURCE: https://stackoverflow.com/questions/46779605/in-the-linearregression-method-in-sklearn-what-exactly-is-the-fit-intercept-par # fig properties row = 2 col = 4 width = 20 height = 8 # initialize the figure fig, axes = plt.subplots(row, col,figsize=(width,height)) for ax,variable in zip(axes.flatten(),X_train.columns.tolist()): ax.scatter(X_train[variable],y_train, label=\u0026#39;Actual points\u0026#39;) ax.grid(color=\u0026#39;grey\u0026#39;, linestyle=\u0026#39;-\u0026#39;, linewidth=0.5) idx = X_train.columns.get_loc(variable) # get corresponding column index to access the right coeff lr_fi_true_yhat = np.dot(X_train[variable], lr_fi_true.coef_[idx]) + lr_fi_true.intercept_ lr_fi_false_yhat = np.dot(X_train[variable], lr_fi_false.coef_[idx]) + lr_fi_false.intercept_ ax.plot(X_train[variable], lr_fi_true_yhat, \u0026#39;g--\u0026#39;, label=\u0026#39;fit_intercept=True\u0026#39;) ax.plot(X_train[variable], lr_fi_false_yhat, \u0026#39;r-\u0026#39;, label=\u0026#39;fit_intercept=False\u0026#39;) fig.tight_layout() plt.show(fig) # force to show the plot after the print Intercept when fit_intercept=True : 2.45235 Intercept when fit_intercept=False : 0.00000 lr_fi_true = LinearRegression(fit_intercept=True) lr_fi_true.fit(X_train, y_train) print(lr_fi_true.coef_,\u0026#34;\\n\u0026#34;,lr_fi_true.intercept_,\u0026#34;\\n\\n\u0026#34;) lr_fi_false = LinearRegression(fit_intercept=False) lr_fi_false.fit(X_train, y_train) print(lr_fi_true.coef_,\u0026#34;\\n\u0026#34;,lr_fi_false.intercept_) [ 0.71640701 0.2926424 -0.14254963 0.2120076 0.30961953 -0.28900562 -0.02091352 0.27734595] 2.4523450850746262 [ 0.71640701 0.2926424 -0.14254963 0.2120076 0.30961953 -0.28900562 -0.02091352 0.27734595] 0.0 28. Show the parameters of the linear regression model computed above. Compare the parameters with those shown in Table 3.2 of the book (page 50) col = [\u0026#39;Term\u0026#39;,\u0026#39;Coefficient\u0026#39;] # headers intercept_val = np.array([linreg.intercept_]).round(2) coeff_val = linreg.coef_.round(2) intercept_label = np.array([\u0026#39;Intercept\u0026#39;]) coeff_label = X_train.columns.tolist() terms = np.concatenate((intercept_val,coeff_val), axis=0) coeffs = np.concatenate((intercept_label,coeff_label),axis=0) table = np.column_stack((coeffs,terms)) print(tabulate(table, headers=col, tablefmt=\u0026#39;fancy_grid\u0026#39;)) ╒═══════════╤═══════════════╕ │ Term │ Coefficient │ ╞═══════════╪═══════════════╡ │ Intercept │ 2.45 │ ├───────────┼───────────────┤ │ lcavol │ 0.72 │ ├───────────┼───────────────┤ │ lweight │ 0.29 │ ├───────────┼───────────────┤ │ age │ -0.14 │ ├───────────┼───────────────┤ │ lbph │ 0.21 │ ├───────────┼───────────────┤ │ svi │ 0.31 │ ├───────────┼───────────────┤ │ lcp │ -0.29 │ ├───────────┼───────────────┤ │ gleason │ -0.02 │ ├───────────┼───────────────┤ │ pgg45 │ 0.28 │ ╘═══════════╧═══════════════╛ # We can compare the above correlation matrix with the one in the book: Image(\u0026#34;../input/prostate-data/tab2.png\u0026#34;) 29. Compute the coefficient of determination of the prediction For coefficient of determination we mean \\(R^{2}\\).\ny_predicted = linreg.predict(X_test) y_predicted array([1.96903844, 1.16995577, 1.26117929, 1.88375914, 2.54431886, 1.93275402, 2.04233571, 1.83091625, 1.99115929, 1.32347076, 2.93843111, 2.20314404, 2.166421 , 2.79456237, 2.67466879, 2.18057291, 2.40211068, 3.02351576, 3.21122283, 1.38441459, 3.41751878, 3.70741749, 2.54118337, 2.72969658, 2.64055575, 3.48060024, 3.17136269, 3.2923494 , 3.11889686, 3.76383999]) score = r2_score(y_test,y_predicted) # goodness of fit measure for linreg score2 = mean_squared_error(y_test,y_predicted) print(f\u0026#34;The coefficient of determination (i.e. R^2) of the prediction is {round(score,3)}\\n\\ The mean squared error is: {round(score2,3)}.\\n\\ The root of the mean squared error is {round(np.sqrt(score2),3)}\u0026#34;) The coefficient of determination (i.e. R^2) of the prediction is 0.503 The mean squared error is: 0.521. The root of the mean squared error is 0.722 plt.figure(figsize=[7,5]) plt.scatter(X_test[\u0026#39;lcavol\u0026#39;],y_test, marker=\u0026#39;o\u0026#39;, s = 50) plt.scatter(X_test[\u0026#39;lcavol\u0026#39;],y_predicted, marker=\u0026#39;^\u0026#39;, s = 50) plt.title(\u0026#39;Predicted and Real values of Y by using `lcavol`\u0026#39;,fontsize=16) plt.legend(labels = [\u0026#39;Test\u0026#39;,\u0026#39;Predicted\u0026#39;],loc = \u0026#39;upper right\u0026#39;) plt.xlabel(\u0026#39;lcavol\u0026#39;,fontsize=12) plt.ylabel(\u0026#39;lpsa\u0026#39;,fontsize=12) plt.show() data_all = pd.read_csv(\u0026#39;../input/prostate-data/prostate.data\u0026#39;,sep=\u0026#39;\\t\u0026#39;) data_all = data_all.drop(labels = [\u0026#39;Unnamed: 0\u0026#39;],axis=1) featuresToPlot = data_all.columns.tolist() dataToPlot = data_all[featuresToPlot] pd.plotting.scatter_matrix(dataToPlot, alpha = 0.7, diagonal = \u0026#39;kde\u0026#39;, figsize=(10,10)) plt.show() plt.figure(figsize=[7,5]) plt.scatter(predictorsTrain_std[\u0026#39;lweight\u0026#39;],lpsaTrain, marker=\u0026#39;o\u0026#39;, s = 50) plt.title(\u0026#39;Linear relationship btw `lweight` and `lpsa`\u0026#39;,fontsize=16) plt.xlabel(\u0026#39;lweight\u0026#39;,fontsize=12) plt.ylabel(\u0026#39;lpsa\u0026#39;,fontsize=12) plt.show() # in this case linear regression is nice! We can see how these relationships are linear. We can see linearity from the plot.\nr2test = round(linreg.score(X_test,lpsaTest),2) r2train = round(linreg.score(X_train,lpsaTrain),2) print(f\u0026#34;Coefficient of determination for Test set is {r2test}\\n\\ Coefficient of determination for Test set is {r2train}\u0026#34;) # it\u0026#39;s higher because the model is created by using train set Coefficient of determination for Test set is 0.5 Coefficient of determination for Test set is 0.69 30. Compute the standard errors, the Z scores (Student’s t statistics) and the related p-values Hint: use library `statsmodels instead of sklearn Hint: compare the results with those in Table 3.2 of the book (page 50) y_predicted = linreg.predict(X_test) X_trainC = sm.add_constant(X_train) # We need this in order to have an intercept # otherwise we will no have the costant (it would be 0) model = sm.OLS(y_train,X_trainC) # define the OLS (Ordinary Least Square - Linear Regression model) results = model.fit() # fit the model results.params # Coefficients of the model results.summary(slim=True) # Adjusted R^2 is the r^2 scaled by the number of parameters in the model # F-statistics tells if the value of R^2 is significant or not. OLS Regression Results Dep. Variable: lpsa R-squared: 0.694 Model: OLS Adj. R-squared: 0.652 No. Observations: 67 F-statistic: 16.47 Covariance Type: nonrobust Prob (F-statistic): 2.04e-12 coef std err t P\u003e|t| [0.025 0.975] const 2.4523 0.087 28.182 0.000 2.278 2.627 lcavol 0.7164 0.134 5.366 0.000 0.449 0.984 lweight 0.2926 0.106 2.751 0.008 0.080 0.506 age -0.1425 0.102 -1.396 0.168 -0.347 0.062 lbph 0.2120 0.103 2.056 0.044 0.006 0.418 svi 0.3096 0.125 2.469 0.017 0.059 0.561 lcp -0.2890 0.155 -1.867 0.067 -0.599 0.021 gleason -0.0209 0.143 -0.147 0.884 -0.306 0.264 pgg45 0.2773 0.160 1.738 0.088 -0.042 0.597 Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n# We can compare the above correlation matrix with the one in the book: Image(\u0026#34;../input/prostate-data/tab2.png\u0026#34;) ","date":null,"permalink":"/posts/some-exercise-about-statistical-learning/sl-ex2-prostatecancer/","section":"Posts","summary":"","title":"SL2: Analysis of Prostate Cancer dataset - linear regression model"},{"content":" Aim of the analysisThe aim of this analysis is to predict behavior to retain customers. # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # data analysis and wrangling import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import random as rnd # visualization import seaborn as sns import matplotlib.pyplot as plt # %matplotlib inline # machine learning from sklearn.linear_model import LogisticRegression # from sklearn.metrics import r2_score # from sklearn.metrics import mean_squared_error # Input data files are available in the read-only \u0026#34;../input/\u0026#34; directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk(\u0026#39;/kaggle/input\u0026#39;): for filename in filenames: print(os.path.join(dirname, filename)) /kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv # Automatically Wrap Graph Labels in Matplotlib and Seaborn # source: https://medium.com/dunder-data/automatically-wrap-graph-labels-in-matplotlib-and-seaborn-a48740bc9ce import textwrap def wrap_labels(ax, width, break_long_words=False): labels = [] for label in ax.get_xticklabels(): text = label.get_text() labels.append(textwrap.fill(text, width=width, break_long_words=break_long_words)) ax.set_xticklabels(labels, rotation=0) Analysis of the dataset Telcom_Customer_Churn #1. Open the Telco Customer Churn dataset page in Kaggle. 2. Check the main properties of this dataset in the “Data” tab. How many samples (rows) does it have?\nThe number of samples (rows) is: 7043\nHow many variables (columns)?\nThe number of variables (columns) is: 21\nWhat does each row/column represent?\nEach row represents a customer, each column contains customer’s attributes. The data set includes information about:\nCustomers who left within the last month:\nthe column is called \u0026ldquo;Churn\u0026rdquo; Services that each customer has signed up for:\nphone, multiple lines, internet, online security, online backup,device protection, tech support, and streaming TV and movies Customer account information:\nhow long they’ve been a customer, contract, payment method,paperless billing, monthly charges, and total charges Demographic info about customers:\ngender, age range, and if they have partners and dependents Which is the “target” column? What does it represent?\nThe target column is the \u0026ldquo;Churn\u0026rdquo;-column, because we want to predict this kind of behaviour.\n3. Download the dataset into your computer. Which is the extension of the downloaded file? .zip\n4. Uncompress the file Which is the extension of the uncompressed file? .csv\n5. Open the uncompressed file by both a text editor and a spreadsheet software Which symbol is used to separate columns?\nThe comma!\nWhich symbol is used to separate rows?\nNew line \u0026lsquo;\\n\u0026rsquo;!\nWhich values can you find for variable SeniorCitizen? And for variable Partner?\nFor SeniorCitizen the possible value is a boolean variable, where 1 means that the person is a senior citizen and 0 means no.\nFor Partner the possibile value is string variable, with `Yes\u0026rsquo; or \u0026lsquo;No\u0026rsquo;.\n6. Generate a new notebook for analyzing this dataset Hint: click on “New Kernel”, then choose the Notebook kernel type, on the right Assign the following title to the notebook:\n\u0026ldquo;SL_L1_TelcoCustomerChurn_Surname\u0026rdquo; Then click on the “Commit” button on top-right to make the notebook ready to be started 7. Open the notebook documentation page to get help if needed Hint: click the “Docs” link on the right-bottom of your notebook page 8. Select the first cell (we will call it “Library import cell” in the following), run it What is the output of this action?\nIt loads some python packages\nWhat does the code “import numpy as np” do? Can you provide a reference website for this library?\nRunning import numpy as np I load numpy package, which is useful for linear algebra. You can find a reference here.\nWhat does the code “import pandas as pd” do? Can you provide a reference website for this library?\nRunning import pandas as pd I load pandas package,m which is useful for data processing, and to handle CSV file I/O (e.g. pd.read_csv). You can find a reference here.\nWhat does the code “import os” do? Can you provide a reference website for this library?\nRunning import os I load os package, which is useful for operating system functionality. There is a reference here.\nHow many data files are available? Please provide their names.\nWith os module I can list all the files available, and in our case there\u0026rsquo;s only the file named \u0026ldquo;/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\u0026rdquo;.\n9. Add to the first cell new lines to load the following libraries: seaborn, matplotlib.pyplot, sklearn.linear_model (only LogisticRegression) Hint: find similar code in the Titanic notebook if needed 10. Select the first cell and add a new cell on top of it Hint: use the button on top-right of the cell 11. Select the new cell and transform it in a “Markdown” cell, then copy all the text in this pdf file and paste it in the new Markdown cell 12. Please write your answers to the questions above in the new Markdown cell. From now on you can use the same cell to write your answers as well 13. Select the “Library input cell” and add a new cell below it 14. Use the new cell to load the Telco Customer Churn dataset into a Pandas DataFrame variable called data Hint: find similar code in the Titanic notebook if needed Remind to run the cell after writing the code-box 15. Add the following `comment` before data loading line: “Data acquisition” 16. Add also a `Markdown cell` before the data loading cell and write in bold the text “Data acquisition” Markdown cells should be used to give a structure to the report, hence they should be added before each new section Data acquisition ### Data acquisition data = pd.read_csv(\u0026#39;../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\u0026#39;) 17. In a new cell show the number of rows, the number of columns, and the total number of cells in the dataset Hint: display the related parameters of the Pandas DataFrame Hint: use the print function to print the results You should print, in particular, the following strings: “The number of customers is XXXX” “The number of variables is YYYY” “The total number of cells is ZZZZ” Other hints: How can you select a single element from the shape tuple? How can you convert a number to string? How can you concatenate two strings? How can you print the final string? Data analysis ##Dataset dimension [num_rows,num_columns]=data.shape; num_elements=data.size; print(f\u0026#34;The number of costumers (rows) is {num_rows}.\\n\\ The number of variables (columns) is {num_columns}.\\n\\ The number of elements is {num_elements}.\\n\u0026#34;) print(\u0026#34;To select a single element from the shape tuple it\u0026#39;s enough to assing the return of `data.shape` to two variables.\\n\\ So I get a single element by calling one of them.\\n\\n\\ I can convert a number to string by call the method `str`.\\n\\ I can concatenate two strings by using `+` between the two strings and I print the final string by calling `print` with the argument `\\\u0026#34;string1\\\u0026#34;+\\\u0026#34;string2\\\u0026#34;\u0026#34;) The number of costumers (rows) is 7043. The number of variables (columns) is 21. The number of elements is 147903. To select a single element from the shape tuple it's enough to assing the return of `data.shape` to two variables. So I get a single element by calling one of them. I can convert a number to string by call the method `str`. I can concatenate two strings by using `+` between the two strings and I print the final string by calling `print` with the argument `\u0026quot;string1\u0026quot;+\u0026quot;string2\u0026quot; 18. Add the following `comment` at the beginning of the cell: “Dataset dimension” 19. Add a new `markdown cell` before this cell and write in it the title “Data Analysis” 20. In a new cell show the names of the variables in the dataset Hint: print the column’s names of variable data print(\u0026#34;The names of the variables in the dataset are:\u0026#34;) for num, column in zip(range(1,len(data.columns)+1),data.columns): print(f\u0026#34;{num}. {column}\u0026#34;) The names of the variables in the dataset are: 1. customerID 2. gender 3. SeniorCitizen 4. Partner 5. Dependents 6. tenure 7. PhoneService 8. MultipleLines 9. InternetService 10. OnlineSecurity 11. OnlineBackup 12. DeviceProtection 13. TechSupport 14. StreamingTV 15. StreamingMovies 16. Contract 17. PaperlessBilling 18. PaymentMethod 19. MonthlyCharges 20. TotalCharges 21. Churn 21. In a new cell show the first and last 10 rows in the dataset Hint: find the correct DataFrame methods in the Pandas’ documentation print(\u0026#34;The first 10 rows in the dataset are:\u0026#34;) data.head(10) The first 10 rows in the dataset are: customerID gender SeniorCitizen Partner Dependents tenure PhoneService MultipleLines InternetService OnlineSecurity ... DeviceProtection TechSupport StreamingTV StreamingMovies Contract PaperlessBilling PaymentMethod MonthlyCharges TotalCharges Churn 0 7590-VHVEG Female 0 Yes No 1 No No phone service DSL No ... No No No No Month-to-month Yes Electronic check 29.85 29.85 No 1 5575-GNVDE Male 0 No No 34 Yes No DSL Yes ... Yes No No No One year No Mailed check 56.95 1889.5 No 2 3668-QPYBK Male 0 No No 2 Yes No DSL Yes ... No No No No Month-to-month Yes Mailed check 53.85 108.15 Yes 3 7795-CFOCW Male 0 No No 45 No No phone service DSL Yes ... Yes Yes No No One year No Bank transfer (automatic) 42.30 1840.75 No 4 9237-HQITU Female 0 No No 2 Yes No Fiber optic No ... No No No No Month-to-month Yes Electronic check 70.70 151.65 Yes 5 9305-CDSKC Female 0 No No 8 Yes Yes Fiber optic No ... Yes No Yes Yes Month-to-month Yes Electronic check 99.65 820.5 Yes 6 1452-KIOVK Male 0 No Yes 22 Yes Yes Fiber optic No ... No No Yes No Month-to-month Yes Credit card (automatic) 89.10 1949.4 No 7 6713-OKOMC Female 0 No No 10 No No phone service DSL Yes ... No No No No Month-to-month No Mailed check 29.75 301.9 No 8 7892-POOKP Female 0 Yes No 28 Yes Yes Fiber optic No ... Yes Yes Yes Yes Month-to-month Yes Electronic check 104.80 3046.05 Yes 9 6388-TABGU Male 0 No Yes 62 Yes No DSL Yes ... No No No No One year No Bank transfer (automatic) 56.15 3487.95 No 10 rows × 21 columns\nprint(\u0026#34;The last 10 rows in the dataset are:\u0026#34;) data.tail(10) The last 10 rows in the dataset are: customerID gender SeniorCitizen Partner Dependents tenure PhoneService MultipleLines InternetService OnlineSecurity ... DeviceProtection TechSupport StreamingTV StreamingMovies Contract PaperlessBilling PaymentMethod MonthlyCharges TotalCharges Churn 7033 9767-FFLEM Male 0 No No 38 Yes No Fiber optic No ... No No No No Month-to-month Yes Credit card (automatic) 69.50 2625.25 No 7034 0639-TSIQW Female 0 No No 67 Yes Yes Fiber optic Yes ... Yes No Yes No Month-to-month Yes Credit card (automatic) 102.95 6886.25 Yes 7035 8456-QDAVC Male 0 No No 19 Yes No Fiber optic No ... No No Yes No Month-to-month Yes Bank transfer (automatic) 78.70 1495.1 No 7036 7750-EYXWZ Female 0 No No 12 No No phone service DSL No ... Yes Yes Yes Yes One year No Electronic check 60.65 743.3 No 7037 2569-WGERO Female 0 No No 72 Yes No No No internet service ... No internet service No internet service No internet service No internet service Two year Yes Bank transfer (automatic) 21.15 1419.4 No 7038 6840-RESVB Male 0 Yes Yes 24 Yes Yes DSL Yes ... Yes Yes Yes Yes One year Yes Mailed check 84.80 1990.5 No 7039 2234-XADUH Female 0 Yes Yes 72 Yes Yes Fiber optic No ... Yes No Yes Yes One year Yes Credit card (automatic) 103.20 7362.9 No 7040 4801-JZAZL Female 0 Yes Yes 11 No No phone service DSL Yes ... No No No No Month-to-month Yes Electronic check 29.60 346.45 No 7041 8361-LTMKD Male 1 Yes No 4 Yes Yes Fiber optic No ... No No No No Month-to-month Yes Mailed check 74.40 306.6 Yes 7042 3186-AJIEK Male 0 No No 66 Yes No Fiber optic Yes ... Yes Yes Yes Yes Two year Yes Bank transfer (automatic) 105.65 6844.5 No 10 rows × 21 columns\n22. In a new cell show i) the type of variable data, ii) the number of missing values for each variable, iii) the type of each variable, iv) the total memory used to store variable data Hint: all this information can be provided by a single method of DataFrame With the method .info we can observe all the info asked. data.info(verbose=True,memory_usage=\u0026#39;deep\u0026#39;) \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 7043 entries, 0 to 7042 Data columns (total 21 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 customerID 7043 non-null object 1 gender 7043 non-null object 2 SeniorCitizen 7043 non-null int64 3 Partner 7043 non-null object 4 Dependents 7043 non-null object 5 tenure 7043 non-null int64 6 PhoneService 7043 non-null object 7 MultipleLines 7043 non-null object 8 InternetService 7043 non-null object 9 OnlineSecurity 7043 non-null object 10 OnlineBackup 7043 non-null object 11 DeviceProtection 7043 non-null object 12 TechSupport 7043 non-null object 13 StreamingTV 7043 non-null object 14 StreamingMovies 7043 non-null object 15 Contract 7043 non-null object 16 PaperlessBilling 7043 non-null object 17 PaymentMethod 7043 non-null object 18 MonthlyCharges 7043 non-null float64 19 TotalCharges 7043 non-null object 20 Churn 7043 non-null object dtypes: float64(1), int64(2), object(18) memory usage: 7.8 MB print(\u0026#34;\\n\u0026#39;TotalCharges\u0026#39; contains also empty value and it will be difficult to handle,\\ for instance data.iloc[488][\u0026#39;TotalCharges\u0026#39;] contains: \u0026#34;,data.iloc[488][\u0026#39;TotalCharges\u0026#39;],\u0026#34;.\\n\\ A possible solution is to cast these two variables.\u0026#34;) 'TotalCharges' contains also empty value and it will be difficult to handle,for instance data.iloc[488]['TotalCharges'] contains: . A possible solution is to cast these two variables. WARNING: Pay attention to \u0026lsquo;customerID\u0026rsquo; and \u0026lsquo;TotalCharges\u0026rsquo; variables because they are object, but they contains numerical values! How many missing values are there in total? To detect missing values I use the method .isnull():\n# # Count total NaN at each column in a DataFrame # print(\u0026#34; \\nCount total NaN at each column in a DataFrame:\\n\u0026#34;,\\ # data.isnull().sum(),\u0026#34;\\n\u0026#34;) # # Count total NaN at each row in a DataFrame # for i in range(len(data.index)) : # print(\u0026#34; Total NaN in row\u0026#34;, i + 1, \u0026#34;:\u0026#34;, # data.iloc[i].isnull().sum()) print(\u0026#34;There are \u0026#34;,data.isnull().sum().sum(),\u0026#34; missing values in total.\u0026#34;) There are 0 missing values in total. Which variables are categorical? print(\u0026#34;\\nThe categorical variables are:\u0026#34;) objVar = data.select_dtypes(include=[\u0026#39;object\u0026#39;]).columns.tolist() for num, col in zip(range(1,len(objVar)+1),objVar): print(f\u0026#34;{num}. {col}\u0026#34;) The categorical variables are: 1. customerID 2. gender 3. Partner 4. Dependents 5. PhoneService 6. MultipleLines 7. InternetService 8. OnlineSecurity 9. OnlineBackup 10. DeviceProtection 11. TechSupport 12. StreamingTV 13. StreamingMovies 14. Contract 15. PaperlessBilling 16. PaymentMethod 17. TotalCharges 18. Churn Which variables are numerical? print(\u0026#34;\\nThe numerical variables are:\u0026#34;) numVar = data.select_dtypes(exclude=[\u0026#39;object\u0026#39;]).columns.tolist() for num, col in zip(range(1,len(objVar)+1),numVar): print(f\u0026#34;{num}. {col}\u0026#34;) The numerical variables are: 1. SeniorCitizen 2. tenure 3. MonthlyCharges 23. In a new cell show the following basic statistics for all `numerical variables`: number of non-missing values, mean, standard deviation, minimum, maximum, median, 1 st and 3 rd quartiles * Hint: all this information can be provided by a single method of DataFrame data.describe(percentiles=[.25, .5, .75]) SeniorCitizen tenure MonthlyCharges count 7043.000000 7043.000000 7043.000000 mean 0.162147 32.371149 64.761692 std 0.368612 24.559481 30.090047 min 0.000000 0.000000 18.250000 25% 0.000000 9.000000 35.500000 50% 0.000000 29.000000 70.350000 75% 0.000000 55.000000 89.850000 max 1.000000 72.000000 118.750000 24. In a new cell show the following basic information for all `categorical variables`: number of non-missing values, number of unique values, most frequent value and frequency of the most frequent value. * Hint: all this information can be provided by the DataFrame method used in question 22, using specific arguments * Can you see any strange value in this result? data.describe(include=[\u0026#39;object\u0026#39;]) customerID gender Partner Dependents PhoneService MultipleLines InternetService OnlineSecurity OnlineBackup DeviceProtection TechSupport StreamingTV StreamingMovies Contract PaperlessBilling PaymentMethod TotalCharges Churn count 7043 7043 7043 7043 7043 7043 7043 7043 7043 7043 7043 7043 7043 7043 7043 7043 7043 7043 unique 7043 2 2 2 2 3 3 3 3 3 3 3 3 3 2 4 6531 2 top 7590-VHVEG Male No No Yes No Fiber optic No No No No No No Month-to-month Yes Electronic check No freq 1 3555 3641 4933 6361 3390 3096 3498 3088 3095 3473 2810 2785 3875 4171 2365 11 5174 Visualization of data #25. In a new cell show the histograms of each numeric variable (i.e., column) in the dataset Hint: try to find a specific method in the DataFrame API documentation # numdata = data.select_dtypes(include=[\u0026#39;number\u0026#39;]) # cols = numdata.columns.values numdata = data._get_numeric_data() # numdata.hist(bins=20) # fig properties row = 1 col = 3 width = 20 height = 4 # initialize the figure fig, axes = plt.subplots(row, col,figsize=(width,height)) for ax,numcol in zip(axes.flatten(),numdata.columns.tolist()): numdata.hist(column=numcol,ax=ax) plt.show(fig) # force to show the plot after the print 26. In a new cell show the box-plots of each numeric variable (i.e., column) in the dataset Hint: try to find a specific method in the DataFrame API documentation Does this chart provide a good visualization? Why? Try to generate one box-plot for each numerical variable Try to put all three charts in the same figure using the subplot function print(\u0026#34;Not a good visualization because it groups together all the numerical variables:\\n\u0026#34;) fig1 = numdata.boxplot() plt.show(fig1) # force to show the plot after the print print(\u0026#34;\\nMoreover for \u0026#39;SeniorCitizen\u0026#39; variable we can not distinguish all the info from the plot.\u0026#34;) Not a good visualization because it groups together all the numerical variables: Moreover for 'SeniorCitizen' variable we can not distinguish all the info from the plot. print(\u0026#34;To have a good visualization, we can plot each numerical variable in a single boxplot as follows:\u0026#34;) # fig properties row = 1 col = 3 width = 20 height = 4 # initialize the figure fig2, axes = plt.subplots(row, col,figsize=(width,height)) # fig.tight_layout() # #fig.subplots_adjust(wspace=0.2) for ax,numcol in zip(np.ravel(axes),numdata.columns.tolist()): numdata.boxplot(column=numcol,ax=ax) plt.show(fig2) # force to show the plot after the print To have a good visualization, we can plot each numerical variable in a single boxplot as follows: 27. In a new cell show the histograms of the categorical variables in the dataset Hint: try to use a function from the Seaborn library which counts the number of time each element appears and makes a related bar plot Hint: use the subplot function to put all the charts in the same figure Hint: resize the figure so that to avoid overlapping and enable a clear visualization of all charts catdata = data.select_dtypes(include=[\u0026#34;object\u0026#34;]); # fig properties row = 4 col = 4 width = 18 height = 10 fig, axes = plt.subplots(row, col,figsize=(width,height)) # Dropping \u0026#39;costumerID\u0026#39; and \u0026#39;TotalCharges\u0026#39; variables because makes too hard to plot! categorical_variables = catdata[catdata.columns.difference([\u0026#39;customerID\u0026#39;,\u0026#39;TotalCharges\u0026#39;])].columns.tolist() for ax,col in zip(axes.flatten(),categorical_variables): sns.countplot(data=data, x=col,ax=ax) # Since for \u0026#39;PaymentMethod\u0026#39; we have xticklabels which are overlapping if col == \u0026#39;PaymentMethod\u0026#39;: #ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\u0026#34;right\u0026#34;) # rotate x-tick-labels wrap_labels(ax, 10) # wrap the words at or before the 10th character fig.tight_layout() plt.show(fig) # force to show the plot after the print 28. In a new cell generate a new DataFrame called data1 and containing only variables gender, Partner, MonthlyCharges, Churn * Hint: you could try also other selections #data1 = data[[\u0026#39;gender\u0026#39;,\u0026#39;Partner\u0026#39;,\u0026#39;MonthlyCharges\u0026#39;,\u0026#39;Churn\u0026#39;]] data1 = pd.DataFrame(data,columns=[\u0026#39;gender\u0026#39;,\u0026#39;Partner\u0026#39;,\u0026#39;MonthlyCharges\u0026#39;,\u0026#39;Churn\u0026#39;]) 29. In a new cell show the first 5 rows of the new dataset data1.head(5) gender Partner MonthlyCharges Churn 0 Female Yes 29.85 No 1 Male No 56.95 No 2 Male No 53.85 Yes 3 Male No 42.30 No 4 Female No 70.70 Yes Prepare the data #30. Convert categorical values in data1 to numeric as follows: gender: Male=0, Female=1\nPartner: No=0, Yes=1\nChurn: No=0, Yes=1\nHint: find similar code in the Titanic notebook if needed\ngender_mapping = {\u0026#39;Male\u0026#39;: 0, \u0026#39;Female\u0026#39;: 1} mapping = {\u0026#39;No\u0026#39;: 0, \u0026#39;Yes\u0026#39;: 1} data1[\u0026#39;gender\u0026#39;] = data1[\u0026#39;gender\u0026#39;].map(gender_mapping) # data1[\u0026#39;gender\u0026#39;] = data1[\u0026#39;gender\u0026#39;].astype(int) # it doesn\u0026#39;t need to cast data1[\u0026#39;Partner\u0026#39;] = data1[\u0026#39;Partner\u0026#39;].map(mapping) data1[\u0026#39;Churn\u0026#39;] = data1[\u0026#39;Churn\u0026#39;].map(mapping) display(data.loc[:,[\u0026#39;gender\u0026#39;, \u0026#39;Partner\u0026#39;,\u0026#39;Churn\u0026#39;]].head()) display(data1.loc[:,[\u0026#39;gender\u0026#39;, \u0026#39;Partner\u0026#39;,\u0026#39;Churn\u0026#39;]].head()) gender Partner Churn 0 Female Yes No 1 Male No No 2 Male No Yes 3 Male No No 4 Female No Yes gender Partner Churn 0 1 1 0 1 0 0 0 2 0 0 1 3 0 0 0 4 1 0 1 31. Generate a separate Series variable called data1Churn for the dependent (churn) variable and drop it from DataFrame data1 Hint: Series is a data structure defined in Pandas, try to find its documentation page Hint: each column of a DataFrame is a Series Hint: learn how to drop columns from a dataset in the Titanoc notebook What is the difference between data1[[‘Churn’]] and data1[‘Churn’]? When single square brackets are used with Pandas DataFrame? When double brackets are used instead? ## Genereate a series variable: data1Churn = data1[\u0026#39;Churn\u0026#39;] ## Dropping \u0026#34;Churn\u0026#34; variable from data1 # data1 = data1.drop(\u0026#34;Churn\u0026#34;,axis=1) # don\u0026#39;t run this twice! data1 = data1[[\u0026#39;gender\u0026#39;,\u0026#39;Partner\u0026#39;,\u0026#39;MonthlyCharges\u0026#39;]] The difference between data1[[‘Churn’]] and data1[‘Churn’] is that:\ndata1[[‘Churn’]] return a Pandas DataFrame, while data1[‘Churn’] returns a Pandas Series.\nSo according to our needs we choose the more appropriate class to use.\nLinear Logistic Regression #32. Generate a linear logistic model using data1 as independent variables and data1Churn as dependent variable, then show the model “score” Hint: try to find a function for linear logistic model learning in the sklearn library Hint: find similar code in the Titanic notebook if needed # Logistic Regression logreg = LogisticRegression() logreg.fit(data1, data1Churn) Y_pred = logreg.predict(data1)# test on data1 acc_log = round(logreg.score(data1,data1Churn), 2) # R2 print(\u0026#34;The score of the Logistic Regression model is: \u0026#34;,acc_log) The score of the Logistic Regression model is: 0.72 33. Show the parameters of the linear logistic model computed above. Which variable seems to be more related to customer churn? Hint: find similar code in the Titanic notebook if needed logreg.get_params(deep=True) # penalty: Specify the norm of the penalty # for other info see here: # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.get_params print(\u0026#34;The intercept is: \u0026#34;,logreg.intercept_[0],\u0026#34;\\n\\ The coefficients of our model are: \u0026#34;,logreg.coef_[0][:]) The intercept is: -1.8510896831170833 The coefficients of our model are: [ 0.01730586 -0.83873431 0.01754114] # Correlation coeff_data1 = pd.DataFrame(data1.columns) coeff_data1.columns = [\u0026#39;Feature\u0026#39;] coeff_data1[\u0026#34;Correlation\u0026#34;] = pd.Series(logreg.coef_[0]) coeff_data1.sort_values(by=\u0026#39;Correlation\u0026#39;, ascending=False) Feature Correlation 2 MonthlyCharges 0.017541 0 gender 0.017306 1 Partner -0.838734 ","date":null,"permalink":"/posts/some-exercise-about-statistical-learning/sl-ex1-telcocustomerchurn/","section":"Posts","summary":"","title":"SL1: Telco Customer Churn first data analysis using Python"},{"content":"","date":null,"permalink":"/tags/2024/","section":"Tags","summary":"","title":"2024"},{"content":"","date":null,"permalink":"/tags/albums/","section":"Tags","summary":"","title":"Albums"},{"content":"","date":null,"permalink":"/tags/music/","section":"Tags","summary":"","title":"Music"},{"content":"As we reflect on the musical landscape of 2024, I\u0026rsquo;m excited to share the albums and tracks that have profoundly impacted my year. Let’s dive into the music that defined my year.\nMy 2024 Playlist #This playlist is a musical journey that blends vibrant beats, heartfelt ballads, and eclectic tracks—my essential soundtrack for embracing every moment of 2024.\nTop 5 Albums of 2024 #The Smile - Wall of Eyes \u0026amp; Cutouts #The second album from The Smile (Thom Yorke, Jonny Greenwood, Tom Skinner), Wall of Eyes, stands out for its cinematic style and existential themes. The title track is a suggestive bossa nova and \u0026ldquo;Teleharmonic\u0026rdquo; is a real gem, even though I especially loved the drum and bass version in Peaky Blinders. The album overall feels cohesive and thoughtfully crafted, with each listen revealing more depth.\nCutouts explores a darker, more experimental side. The ambient \u0026ldquo;Foreign Spies\u0026rdquo; and the eerie bossa nova of \u0026ldquo;Bodies Laughing\u0026rdquo; stand out, while \u0026ldquo;Instant Psalm\u0026rdquo; channels a Radiohead-esque ballad that adds emotional weight to this shadowy album.\nNathy Peluso - GRASA #Nathy Peluso’s GRASA is a bold, genre-blending album that mixes Latin rhythms, hip-hop, and jazz. With her commanding vocals and dynamic presence, Peluso delivers a standout project that defines 2024.\nTyler, The Creator - CHROMAKOPIA #\u0026ldquo;Chromakopia\u0026rdquo; by Tyler, the Creator marks a mature and introspective turn, exploring themes of identity and family. Musically, it blends jazz, funk, and hip-hop, alternating between lighter tracks like \u0026ldquo;Sticky\u0026rdquo; and deeper moments like \u0026ldquo;Take Your Mask Off,\u0026rdquo; which is my favorite.\nA reflective yet experimental album, it balances personal insights with creative flair, making it a compelling listen.\nFloating Points - Cascade #Cascade marks a return to Floating Points\u0026rsquo; dance roots, blending jazz, house, and techno. The album has been praised for its blend of electronic precision and live depth, solidifying Shepherd’s status as a leading dance music producer.\nBadBadNotGood \u0026amp; Baby Rose - Slow Burn (EP) #Slow Burn is a hypnotic collaboration that intertwines Baby Rose’s raw, soulful voice with BadBadNotGood\u0026rsquo;s moody, intricate jazz. The EP effortlessly blends R\u0026amp;B and jazz into a mesmerizing soundscape, making it one of the most captivating listens of 2024.\nOther Notable Mentions #Beth Gibbons - Lives Outgrown \u0026amp; Beak - \u0026raquo;\u0026raquo; #Beth Gibbons, the iconic voice of Portishead, delivers Lives Outgrown, a haunting live album filled with raw emotion and timeless vocal beauty.\nMeanwhile, Beak\u0026rsquo;s \u0026raquo;\u0026raquo; dives deep into experimental krautrock, blending hypnotic rhythms with dark, minimalist soundscapes.\nWhile both projects highlight the artists\u0026rsquo; unique talents, the synergy of Gibbons and Barrow within Portishead remains unmatched, delivering genre-defining music that transcends their individual works.\nPorridge Radio - Clouds in the Sky They Will Always Be There for Me #Porridge Radio’s latest album blends raw indie rock with lush orchestration. Highlights like A Hole in the Ground and Wednesday showcase their knack for balancing intensity with introspection.\nCindy Lee - Diamond Jubilee #Cindy Lee\u0026rsquo;s Diamond Jubilee is an evocative double album blending haunting melodies with lo-fi experimentation. Highlights like the instrumental gem Realistik Heaven showcase its unique atmosphere, but the 32 tracks can feel overwhelming.\nNot on Spotify.\nPhoto by Oleg Ivanov on Unsplash.\n","date":"13 December 2024","permalink":"/posts/my-favorite-music-of-2024/","section":"Posts","summary":"Albums and tracks that shaped my 2024.","title":"My Favorite Music of 2024"},{"content":"","date":null,"permalink":"/tags/playlist/","section":"Tags","summary":"","title":"Playlist"},{"content":"","date":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":" ","date":null,"permalink":"/","section":"Welcome to Congo! 🎉","summary":"","title":"Welcome to Congo! 🎉"},{"content":"Hey there! I\u0026rsquo;m Andrea, freshly minted with a Master\u0026rsquo;s degree in Mathematics and a passion for the applied side of things! With a strong focus on the applied math track, I\u0026rsquo;m all about cracking codes and uncovering quantitative solutions in real-world scenarios.\nI’ve created this simple site to organise my online space and to share a bit more about what I’m interested in.\n","date":"3 April 2024","permalink":"/about/","section":"Welcome to Congo! 🎉","summary":"","title":"About"},{"content":"","date":null,"permalink":"/tags/math/","section":"Tags","summary":"","title":"Math"},{"content":" In 2024 I graduated with a Master\u0026rsquo;s degree in Mathematics from the University of Verona. My Master\u0026rsquo;s thesis, titled \u0026ldquo;Data-Driven Modelling and Calibration of Multi-Agent Systems,\u0026rdquo; focuses on two main targets:\nInvestigating the Discrete-to-Continuum transition in traffic dynamics, underlying the relationship between macroscopic and microscopic models. Employing neural network-driven models to simulate particle interaction dynamics, particularly in a traffic scenario. To address the parameter estimation challenge, I contextualized the problem within optimal control framework and utilized data provided by the Esimas Project.\nFor further insights, please explore my presentation created with reveal.js, and examine the codes available on my GitHub repository TrafficModelling.\nPhoto by Sajjad Ahmadi on Unsplash.\n","date":"11 March 2024","permalink":"/posts/modelling-traffic-with-lwr-models/","section":"Posts","summary":"My MSc thesis about traffic modelling dynamics.","title":"Modelling Traffic with LWR models"},{"content":"","date":null,"permalink":"/tags/2023/","section":"Tags","summary":"","title":"2023"},{"content":"Hey there! As we wrap up 2023, I’m excited to share my favorite albums that defined this year for me, along with a playlist filled with tracks that have accompanied me through every twist and turn.\nMy 2023 Playlist #This playlist is a sonic adventure that blends electronic grooves, mellow indie tunes, and introspective anthems. It has been my trusty soundtrack for navigating the highs and lows of the year.\nTop 5 Albums of 2023 #Daniela Pes - SPIRA #Italian artist Daniela Pes captivates with her debut album SPIRA, which fuses folk, jazz, and electronic elements. She invents a unique language blending ancient Gallura dialects with new words, creating an immersive experience. Pes\u0026rsquo;s powerful vocals transition from ethereal whispers to dynamic peaks, guiding listeners on a personal journey. Produced by IOSONOUNCANE, SPIRA solidifies her as a rising star, earning accolades such as the Targa Tenco.\nFlavien Berger - Dans Cent Ans #Flavien Berger concludes his thematic trilogy with Dans Cent Ans, completing the journey that began with Léviathan and Contre-Temps. This album looks into the future, \u0026ldquo;in a hundred years,\u0026rdquo; and immerses listeners in rich, evocative landscapes while exploring the passage of time.\nBellboy - Opéra partie I (EP) #What a debut! Bellboy’s Opéra partie I captures the essence of dreamy synths and lush arrangements reminiscent of Sebastian Tellier’s style. This EP is a captivating introduction to an exciting new artist.\nEzra Collective - Where I\u0026rsquo;m Meant To Be #Though released in late 2022, Ezra Collective\u0026rsquo;s Mercury Prize-winning album bursts with energy and vibrant jazz-infused melodies.\nbdrmm - I Don\u0026rsquo;t Know #bdrmm’s I Don’t Know offers a compelling shoegaze experience, with raw atmospheres and introspective lyrics that resonate deeply, perfect for those quiet moments.\nOther Notable Mentions #Puma Blue - Holy Waters #Puma Blue’s Holy Waters is a melancholic blend of soul and jazz, creating an intimate atmosphere perfect for late-night listening.\nKamaal Williams - Stings #With Stings, Kamaal Williams fuses jazz, funk, and electronic elements, delivering a refreshing and genre-blending experience.\nUnknown Mortal Orchestra - V #V by Unknown Mortal Orchestra is a psychedelic journey, showcasing their signature lo-fi warmth and experimental soundscapes.\nPhoto by Florencia Viadana on Unsplash.\n","date":"13 December 2023","permalink":"/posts/my-favorite-music-of-2023/","section":"Posts","summary":"A selection of albums and songs that defined my 2023.","title":"My Favorite Music of 2023"},{"content":" Introduction #Finding \u0026ldquo;the One\u0026rdquo; isn\u0026rsquo;t a straightforward task. To increase your chances, you might consider a simple mathematical rule: the 37% Rule. This rule gives you a sense of how long you should keep looking and when it might be a good time to stop and settle down.\nBalancing when to settle down is crucial. Settling down too early might mean missing better matches later, while waiting too long risks missing out on great partners. Therefore, the central question is: how many options do we need to consider before to make a choice? This question comes up in many real-life decision-making processes, from renting an apartment to the search for a parking space. Mathematicians refer to these as \u0026ldquo;Optimal Stopping problems\u0026rdquo;, with the most well-known in literature being the \u0026ldquo;Secretary problem\u0026rdquo;, where the aim is to select the best candidate for a job position.\nThe \u0026ldquo;Secretary problem\u0026rdquo; reflects past-century gender biases, while nowadays dating is a prevalent setting for Optimal Stopping problems. In this context we wonder how many frogs do you need to kiss to enhance your chances of finding a prince or princess.\nStrategy #In the dating scenario we consider simple rules:\nWhen you reject someone they are gone forever. You can not go back and change your mind. When you choose someone your dating process is over, and you can not see next potential partners in future dates. The strategy proposed by the 37% rule involves a two-phase approach:\nExploring phase: Explore the initial 37% of the applicants and gather data, without make commitments. This serves as a calibration period, allowing you to form an idea of the market and develop a realistic expectation for a potential life partner.\nLeap phase: Commit to someone better than all those you\u0026rsquo;ve encountered thus far in the process.\nRoughly speaking, you should reject everyone in the the first third of your decision-making process. Then, after the exploration, selecting the next great option you encounter is optimal. It doesn\u0026rsquo;t guarantee that it\u0026rsquo;s the best one, but it\u0026rsquo;s optimal because you have the highest chance that it\u0026rsquo;s.\nIn fact, employing this strategy yields surprisingly a probability of 37% (more precisely, the probability is \\(\\dfrac{1}{e} \\approx 0.369\\) ) for selecting the best option from the available pool. 1 Due to its two-phased nature, this rule is often referred to as the Look-Then-Leap Rule 2.\nYou can appreciate the 37% rule when you compare it with a random strategy. When you choose over \\(n\\) potential partners and you settle down with a partner at random, then you’d only have a \\(\\dfrac{1}{n}\\) chance of finding your true love. For \\(n \\geq 3\\), it becomes evident that this random approach is less advantageous when compared to the look-then-leap strategy.\nTake a look at the Kepler experience, it\u0026rsquo;s an interesting application of this rule. 3\nAlternative setting #Since you don\u0026rsquo;t know how many people you will encounter in your lifetime, you can set a time frame for your search, based on how long you expect your dating life to be. And also in this setting the 37% rule works. In fact, the 37% rule can be applied to either the number of candidates or the time over which one is searching.\nFor example, consider to start dating when you are 18 years old and you would like to settle down at 35 years old. You should reject everyone until just after your 24th birthday, which corresponds to the exploring phase. Then you choose the next person you date, who is better than everyone you have met before.\nThere are several variations of this problem, discussed in the book \u0026ldquo;Algorithms to Live By: The Computer Science of Human Decisions\u0026rdquo; by Brian Christian and Tom Griffiths.2\nConclusion # Life is undeniably complex and cannot be confined to a purely mathematical analysis. This rule does not consider the nuances of feelings, \u0026ldquo;gut instincts\u0026rdquo;, and instant chemistry, as it only focuses on maximizing probabilities. It\u0026rsquo;s essential to acknowledge that matters of the heart often defy logic, and this heuristic rule, while helpful, doesn\u0026rsquo;t guarantee success.\nMoreover, this strategy isn\u0026rsquo;t limited to selecting a life partner; it\u0026rsquo;s applicable to numerous scenarios where people seek something and want to determine the optimal moment to conclude their search. The essence of sound decision-making lies in striking a balance between exploration and decisiveness, and that\u0026rsquo;s precisely what the 37 percent rule aids in achieving.\nSources # algorithmstoliveby.com bigthink.com washingtonpost.com ted.com cantorsparadise.com sneaky-potato.github.io Photo by Shaira Dela Peña on Unsplash.\nFor more details, you can find a proof here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFor more details take a look at this book: https://algorithmstoliveby.com.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYou can find how Kepler chose his wife here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"16 October 2023","permalink":"/posts/the-37-rule-a-game-changer-in-dating/","section":"Posts","summary":"How many frogs must you kiss to get a prince or a princess?","title":"The 37% Rule: A Game-Changer in Dating"},{"content":"","date":null,"permalink":"/tags/til/","section":"Tags","summary":"","title":"TIL"},{"content":"","date":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python"},{"content":"I share my solution to exercises proposed in the class of Statistical Learning of the master\u0026rsquo;s degree in Mathematics of the University of Verona. Photo by Naser Tamimi o Unsplash.\n","date":null,"permalink":"/posts/some-exercise-about-statistical-learning/","section":"Posts","summary":"Analysis of some real dataset using Python.","title":"Some exercise about Statistical Learning"},{"content":"Why use Hugo? #In this post I want to explain how I setted up my static website using Hugo, GitHub and Netlify. I decided to use these tools because:\nEasy to use. To write contents you will use Markdown; Keep focus on writing. All your website contents will be built in Hugo; Fast build times. By connecting Netlify with your GitHub repo you have a continuous deployment, which makes automatic the content publishing. Let\u0026rsquo;s get started. As a host operating system, I will be using Mint 20.03.\nStep 0: Install Hugo 1 #From the official Hugo realeases page download a .deb package of the latest version and install it with dpkg command:\n$ sudo dpkg -i hugo_\u0026lt;latest_version\u0026gt;_Linux-64bit.deb Step 1: Start with Hugo #Create a new website #Use the hugo new site command:\n$ hugo new site test-site In this way the new site has a structure, but it has no content.\nAdd a theme #From official Hugo Themes choose a theme and then add the theme repository as a Git submodule as follows:\n$ git submodule add -b stable https://github.com/jpanther/congo.git themes/congo Warning! In this tutorial we choose Congo theme. Further configuration at congo docs. Then modify config.toml by specifying the theme to use:\n1 2 3 4 baseURL = \u0026#34;/\u0026#34; languageCode = \u0026#34;en-us\u0026#34; title = \u0026#34;My New Hugo Site\u0026#34; theme = \u0026#34;congo\u0026#34; Pay attention to this step, otherwise there could be some problems. 2\nTo check our site we can start the built-in server to build and serve the site.\nRun the hugo server command from the site’s root directory:\n$ hugo server and go to http://localhost:1313/ in your browser.\nWrite a post #Run the hugo new command:\n$ hugo new post/new-post.md In this way Hugo creates a new file new-post.md into content/post. It should looks like:\n--- title: \u0026#34;New Post\u0026#34; date: 2022-05-02T00:05:45+02:00 draft: true --- as the default.md in /archetypes folder.\nThe new file has automatically setted the flag draft: true. With this flag you can control the visibility of the file.\nRunning locally hugo server we consider only the visible contents (i.e. the posts with draft: false flag), while hugo server -D allows us to consider all the contents (also the ones with draft: true flag).\nGenerate static output for publishing #One way to publish our website is:\n$ hugo -D In this way Hugo generates your static HTML pages into the ./public. Then we can copy the content of the folder to some server by yourself.\nAnyway it\u0026rsquo;s more convenient to use Netlify for this step. Before to pass to Netlify we want to setup a new repository on Github.\nStep 2: Taking track with Git #Create a local repo #To create a local repository:\n$ git init $ git add . $ git commit -m \u0026#39;start my website\u0026#39; Then we have to setup a remote repository on GitHub.\nSync your changes to a remote GitHub repository #Since it\u0026rsquo;s not necessary to track also the generated HTML in /public, so we specify it in .gitignore.\n$ git remote add origin https://github.com/\u0026lt;username\u0026gt;/\u0026lt;GitHub_repository_name\u0026gt;.git $ echo \u0026#39;public/\u0026#39; \u0026gt;\u0026gt; .gitignore $ git branch -M main $ git add . $ git commit -m \u0026#39;after the first post\u0026#39; $ git push -u origin main Now we pass to set up Netlify.\nStep 3: Setting up Netlify #First sign up to Netlify. Click on New site from Git button and then it will open a page titled \u0026ldquo;Create a new site\u0026rdquo;.\nThe procedure is divided in three steps:\n\u0026ldquo;Connect to Git provider\u0026rdquo;:\nClick on GitHub button to authorize Netlify to access your repositories, where your site\u0026rsquo;s source code is hosted.\n\u0026ldquo;Pick a repository\u0026rdquo;:\nChoose the repository where you store your website content (i.e the one you have created in the previous step.)\n\u0026ldquo;Build options, and deploy!\u0026rdquo;.\nSome deployment configurations as:\nOwner (the owner of the website) Branch deploy (which branch of the GitHub repo to deploy) build command (which command to use to build the site) publish directory (the directory to deploy from, public is where Hugo generates the output by default). Then click on the Deploy site and it will be presented the site overview.\nIn the Change site name pop-up window, set the new site name and save it.\nTroubleshooting #The first deploy may fail. Possible reasons are:\nSpecify to Netlify which Hugo version to use:\nAs in this answer on stackoverflow, if you do not setup the right version of Hugo, then the default one may be different and work differently as you see in local. A smooth solution is to specify the version you use in a config file netlify.toml at the site’s root directory.\nFor congo theme I use this netlify.toml, which is a little bit different. Sources #To start my site and so for this tutorial I followed the steps described in:\ndolugen.com kiroule.com Conclusion #Now we have built our static website! Looking at the docs of the theme we can customize our site.\nPhoto by Ilya Pavlov on Unsplash.\nFollow the guide to install hugo in the proper way.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFor more info take a look at the offical docs.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"8 May 2022","permalink":"/posts/getting-started-with-hugo-github-and-netlify/","section":"Posts","summary":"A quick guide to start blogging with these three tools.","title":"Getting started with Hugo, GitHub and Netlify"},{"content":"","date":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo"},{"content":"","date":null,"permalink":"/tags/netlify/","section":"Tags","summary":"","title":"Netlify"},{"content":" In 2020 I graduated with a bachelor\u0026rsquo;s degree in Applied Mathematics from the University of Verona. In my thesis \u0026ldquo;Strategie di controllo e stima dei parametri per modelli epidemiologici applicati al COVID19\u0026rdquo; I analyzed some control policies to cope an epidemic and I estimated the parameters for some epidemiological models applied to Covid-19.\nTo describe the impact of the lockdown policy in Italy during the the first part of 2020 I developed a SIR-type model called \\(\\kappa\\)-SEIR. To estimate the parameters I took into account the data provided by the GitHub account of Dipartimento della Protezione Civile.\nTake a look at my presentation here. It\u0026rsquo;s not another beamer with Latex, I used reveal.js.\nTo see the codes you can look at my github repo EpidModCovid19.\nPhoto by Hakan Nural on Unsplash.\n","date":"1 April 2022","permalink":"/posts/modelling-the-covid-19-epidemic-in-italy/","section":"Posts","summary":"My BCs thesis. Not another beamer with Latex, I used \u003ca href=\"https://revealjs.com/\" target=\"_blank\" rel=\"noreferrer\"\u003ereveal.js\u003c/a\u003e.","title":"Modelling the COVID-19 epidemic in Italy"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"}]